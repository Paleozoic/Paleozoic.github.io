<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Dubbo之注册中心扩展：Eureka]]></title>
    <url>%2F2017%2F06%2F23%2FDubbo%E4%B9%8B%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E6%89%A9%E5%B1%95%EF%BC%9AEureka%2F</url>
    <content type="text"><![CDATA[架构的推进往往不是一蹴而就，尝试将dubbo的注册中心由Zookeeper修改为Eureka，以便可以和Spring Cloud Config更好地集合起来。 dubbo扩展方式首先dubbo是通过JDK SPI的方式进行扩展的，具体操作可以浏览Dubbo之基于“版本”的服务调度(路由规则)，有所参考。 待续待到明年花开时…… 引用dubbo开发者指南：注册中心扩展]]></content>
      <categories>
        <category>RPC</category>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>RPC</tag>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo之基于“版本”的服务调度(路由规则)]]></title>
    <url>%2F2017%2F06%2F14%2FDubbo%E4%B9%8B%E5%9F%BA%E4%BA%8E%E2%80%9C%E7%89%88%E6%9C%AC%E2%80%9D%E7%9A%84%E6%9C%8D%E5%8A%A1%E8%B0%83%E5%BA%A6(%E8%B7%AF%E7%94%B1%E8%A7%84%E5%88%99)%2F</url>
    <content type="text"><![CDATA[通过扩展dubbo的路由规则，实现通过不同入参调用不同的服务实例。(注意：需修改dubbo源码) 写在前面dubbo.xsd完全没有router的配置，what the fuck！注解也没关相关配置！！官方文档如下：12&lt;dubbo:protocol router=&quot;xxx&quot; /&gt;&lt;dubbo:provider router=&quot;xxx&quot; /&gt; &lt;!-- 缺省值设置，当&lt;dubbo:protocol&gt;没有配置loadbalance时，使用此配置 --&gt; 走读源码：首先dubbo APP启动时，便会在RegistryDirectory初始化时，针对每个消费者加载路由规则（不过看源码routers传入的都是null）。并且每当注册中心相关数据有改变时会调用public synchronized void notify(List&lt;URL&gt; urls)方法，从注册中心同步消费者路由规则。内部会调用protected void setRouters(List&lt;Router&gt; routers)来设置路由规则，且会append MockInvokersSelector路由器：routers.add(new MockInvokersSelector());即dubbo目前的路由规则是通过注册中心将表达式等推送到客户端（其实还有脚本、文件等方式，此处不展开讲） 遇到问题 不修改消费者代码可以用新的提供者 不修改原来的提供者 通过增加数据库配置：key-rest url的KV对实现指定调度 基于REST的解决方案这种方案本质上和dubbo没啥关系了。我大致画个架构图出来，就可以很明显看到其实现。 DB里面提供类似如下的配置（结合nginx）： Key Nginx Real IP P1 192.168.1.1:8080/P1_ng/ 192.168.2.1:8080 192.168.2.2:8080 P2 192.168.1.1:8080/P2_ng/ 192.168.2.3:8080 192.168.2.4:8080 代码里面根据Key的不同，通过HTTP REST去调用nginx，然后nginx分发到不同IP下的实例。 架构图（图中并未画出多个nginx分流的情况，自行脑补之）： 缺点 需要指定IP，而IP并不是恒定不变的（虽然生产环境很少改变，但是dev/st/uat就不一定了） 负载均衡：需要通过nginx实现服务端的负载均衡 nginx HA：需要引入nginx HA方案 分流：通过多个nginx实例分流减少IO压力 基于扩展dubbo路由规则的解决方案dubbo通过group，interface，version，三者决定是不是同一个服务。group暂不作考虑（目前没用到，那么整个服务注册便是一颗无根树）。配置version=”*”可以获取所有版本的提供者实例针对上面的需求，有3个方案： 方案一：使用一个版本号，然后dubbo拿到该version的所有Invoker，然后通过提供者、消费者的IP进行匹配。这样子便解决了上面除IP之外的所有问题。并且不需要修改dubbo源码。 方案二：修改dubbo原生路由规则，让其支持基于版本号的路由设定。 方案三：通过配置version=”*”，让dubbo可以通过某个注解获取所有的版本的所有Invoker。然后再扩展路由过滤。 方案一只需要研究dubbo的表达式怎么写就可以了； 方案二需要扩展路由，而具体可以参考方案三的实现； 方案三我会具体介绍，基本上包含了方案二的全部实现，但是注意方案二是通过注册中心推送规则。而方案三是通过注解注入相关规则。从而导致方案三的缺点：更新配置时需要更新所有实例的内存数据 解决方案如下：（这些只是顺便一提，不展开讲了） 配置所有实例共享，显然需要跨进程缓存：Redis、Zookeeper之类的。还可以利用watch实时更新 更新配置时，调用所有节点的方法更新配置数据 dubbo容错调度 Invoker：这里的Invoker是Provider的一个可调用Service的抽象，Invoker封装了Provider地址及Service接口信息。 Directory：(SPI, Prototype, ThreadSafe)集群目录服务，Directory service。Directory代表多个Invoker，可以把它看成List，但与List不同的是，它的值可能是动态变化的，比如注册中心推送变更。 Cluster：Cluster将Directory中的多个Invoker伪装成一个Invoker，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个。 Router：Router负责从多个Invoker中按路由规则选出子集，比如读写分离，应用隔离等。 LoadBalance：LoadBalance负责从多个Invoker中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选。 关于Router，有以下应用： 服务路由是治理的核心功能，它可以动态调整集群间的访问关系： 排除预发布机：=&gt; host != 172.22.3.91 白名单：(注意：一个服务只能有一条白名单规则，否则两条规则交叉，就都被筛选掉了)，host != 10.20.153.10,10.20.153.11 =&gt; 黑名单：host = 10.20.153.10,10.20.153.11 =&gt; 服务寄宿在应用上，只暴露一部分的机器，防止整个集群挂掉：=&gt; host = 172.22.3.1*,172.22.3.2* 为重要应用提供额外的机器：application != kylin =&gt; host != 172.22.3.95,172.22.3.96 读写分离：method != find*,list*,get*,is* =&gt; host = 172.22.3.97,172.22.3.98 or method = find*,list*,get*,is* =&gt; host = 172.22.3.94,172.22.3.95,172.22.3.96 前后台分离：application = bops =&gt; host = 172.22.3.91,172.22.3.92,172.22.3.93 or application != bops =&gt; host = 172.22.3.94,172.22.3.95,172.22.3.96 隔离不同机房网段：host != 172.22.3.* =&gt; host != 172.22.3.* 提供者与消费者部署在同集群内，本机只访问本机的服务：=&gt; host = $host 而路由规则的配置通常是通过监控中心or治理中心的页面完成，也可以通过RegistryFactory写入。 dubbo源码解析（可以不看）以下可以概括为源码乱读= =|| ReferenceConfig1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798//注意urls.add(url.addParameterAndEncoded(Constants.REFER_KEY, StringUtils.toQueryString(map)));这行代码，根据map的参数生成字符串。isInjvm同进程调用。//所以在map里面添加注解配置的routers参数private T createProxy(Map&lt;String, String&gt; map) &#123; URL tmpUrl = new URL("temp", "localhost", 0, map); final boolean isJvmRefer; if (isInjvm() == null) &#123; if (url != null &amp;&amp; url.length() &gt; 0) &#123; //指定URL的情况下，不做本地引用 isJvmRefer = false; &#125; else if (InjvmProtocol.getInjvmProtocol().isInjvmRefer(tmpUrl)) &#123; //默认情况下如果本地有服务暴露，则引用本地服务. isJvmRefer = true; &#125; else &#123; isJvmRefer = false; &#125; &#125; else &#123; isJvmRefer = isInjvm().booleanValue(); &#125; if (isJvmRefer) &#123; URL url = new URL(Constants.LOCAL_PROTOCOL, NetUtils.LOCALHOST, 0, interfaceClass.getName()).addParameters(map); invoker = refprotocol.refer(interfaceClass, url); if (logger.isInfoEnabled()) &#123; logger.info("Using injvm service " + interfaceClass.getName()); &#125; &#125; else &#123; if (url != null &amp;&amp; url.length() &gt; 0) &#123; // 用户指定URL，指定的URL可能是对点对直连地址，也可能是注册中心URL String[] us = Constants.SEMICOLON_SPLIT_PATTERN.split(url); if (us != null &amp;&amp; us.length &gt; 0) &#123; for (String u : us) &#123; URL url = URL.valueOf(u); if (url.getPath() == null || url.getPath().length() == 0) &#123; url = url.setPath(interfaceName); &#125; if (Constants.REGISTRY_PROTOCOL.equals(url.getProtocol())) &#123; urls.add(url.addParameterAndEncoded(Constants.REFER_KEY, StringUtils.toQueryString(map))); &#125; else &#123; urls.add(ClusterUtils.mergeUrl(url, map)); &#125; &#125; &#125; &#125; else &#123; // 通过注册中心配置拼装URL List&lt;URL&gt; us = loadRegistries(false); if (us != null &amp;&amp; us.size() &gt; 0) &#123; for (URL u : us) &#123; URL monitorUrl = loadMonitor(u); if (monitorUrl != null) &#123; map.put(Constants.MONITOR_KEY, URL.encode(monitorUrl.toFullString())); &#125; urls.add(u.addParameterAndEncoded(Constants.REFER_KEY, StringUtils.toQueryString(map))); &#125; &#125; if (urls == null || urls.size() == 0) &#123; throw new IllegalStateException("No such any registry to reference " + interfaceName + " on the consumer " + NetUtils.getLocalHost() + " use dubbo version " + Version.getVersion() + ", please config &lt;dubbo:registry address=\"...\" /&gt; to your spring config."); &#125; &#125; if (urls.size() == 1) &#123; invoker = refprotocol.refer(interfaceClass, urls.get(0)); &#125; else &#123; List&lt;Invoker&lt;?&gt;&gt; invokers = new ArrayList&lt;Invoker&lt;?&gt;&gt;(); URL registryURL = null; for (URL url : urls) &#123; invokers.add(refprotocol.refer(interfaceClass, url)); if (Constants.REGISTRY_PROTOCOL.equals(url.getProtocol())) &#123; registryURL = url; // 用了最后一个registry url &#125; &#125; if (registryURL != null) &#123; // 有 注册中心协议的URL // 对有注册中心的Cluster 只用 AvailableCluster URL u = registryURL.addParameter(Constants.CLUSTER_KEY, AvailableCluster.NAME); invoker = cluster.join(new StaticDirectory(u, invokers)); &#125; else &#123; // 不是 注册中心的URL invoker = cluster.join(new StaticDirectory(invokers)); &#125; &#125; &#125; Boolean c = check; if (c == null &amp;&amp; consumer != null) &#123; c = consumer.isCheck(); &#125; if (c == null) &#123; c = true; // default true &#125; if (c &amp;&amp; ! invoker.isAvailable()) &#123; throw new IllegalStateException("Failed to check the status of the service " + interfaceName + ". No provider available for the service " + (group == null ? "" : group + "/") + interfaceName + (version == null ? "" : ":" + version) + " from the url " + invoker.getUrl() + " to the consumer " + NetUtils.getLocalHost() + " use dubbo version " + Version.getVersion()); &#125; if (logger.isInfoEnabled()) &#123; logger.info("Refer dubbo service " + interfaceClass.getName() + " from url " + invoker.getUrl()); &#125; // 创建服务代理 return (T) proxyFactory.getProxy(invoker); &#125;``` 留意：init()的以下代码：```javacheckDefault(); //初始化默认的ConsumerConfigappendProperties(this);//追加当前配置到ConsumerConfig 但是protected static void appendProperties(AbstractConfig config)是AbstractConfig的方法，入参也是AbstractConfig里面，所以appendProperties不能追加ReferenceConfig的参数。1234567private void checkDefault() &#123; if (consumer == null) &#123; consumer = new ConsumerConfig(); &#125; //追加AbstractConfig的参数 appendProperties(consumer);&#125; AnnotationBeanAnnotationBean extends AbstractConfig用于包装处理com.alibaba.dubbo.config.annotation.Reference和com.alibaba.dubbo.config.annotation.Service注解。 通过AnnotationBean的方法private Object refer(Reference reference, Class&lt;?&gt; referenceClass)解析AnnotationBean成private final ConcurrentMap&lt;String, ReferenceBean&lt;?&gt;&gt; referenceConfigs = new ConcurrentHashMap&lt;String, ReferenceBean&lt;?&gt;&gt;(); 在refer方法中，对consumer的解析执行了2次（此处甚是迷惑）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private Object refer(Reference reference, Class&lt;?&gt; referenceClass) &#123; //method.getParameterTypes()[0] String interfaceName; if (! "".equals(reference.interfaceName())) &#123; interfaceName = reference.interfaceName(); &#125; else if (! void.class.equals(reference.interfaceClass())) &#123; interfaceName = reference.interfaceClass().getName(); &#125; else if (referenceClass.isInterface()) &#123; interfaceName = referenceClass.getName(); &#125; else &#123; throw new IllegalStateException("The @Reference undefined interfaceClass or interfaceName, and the property type " + referenceClass.getName() + " is not a interface."); &#125; String key = reference.group() + "/" + interfaceName + ":" + reference.version(); ReferenceBean&lt;?&gt; referenceConfig = referenceConfigs.get(key); if (referenceConfig == null) &#123; referenceConfig = new ReferenceBean&lt;Object&gt;(reference); if (void.class.equals(reference.interfaceClass()) &amp;&amp; "".equals(reference.interfaceName()) &amp;&amp; referenceClass.isInterface()) &#123; referenceConfig.setInterface(referenceClass); &#125; if (applicationContext != null) &#123; referenceConfig.setApplicationContext(applicationContext); if (reference.registry() != null &amp;&amp; reference.registry().length &gt; 0) &#123; List&lt;RegistryConfig&gt; registryConfigs = new ArrayList&lt;RegistryConfig&gt;(); for (String registryId : reference.registry()) &#123; if (registryId != null &amp;&amp; registryId.length() &gt; 0) &#123; registryConfigs.add((RegistryConfig)applicationContext.getBean(registryId, RegistryConfig.class)); &#125; &#125; referenceConfig.setRegistries(registryConfigs); &#125; if (reference.consumer() != null &amp;&amp; reference.consumer().length() &gt; 0) &#123; // referenceConfig.setConsumer((ConsumerConfig)applicationContext.getBean(reference.consumer(), ConsumerConfig.class)); &#125; if (reference.monitor() != null &amp;&amp; reference.monitor().length() &gt; 0) &#123; referenceConfig.setMonitor((MonitorConfig)applicationContext.getBean(reference.monitor(), MonitorConfig.class)); &#125; if (reference.application() != null &amp;&amp; reference.application().length() &gt; 0) &#123; referenceConfig.setApplication((ApplicationConfig)applicationContext.getBean(reference.application(), ApplicationConfig.class)); &#125; if (reference.module() != null &amp;&amp; reference.module().length() &gt; 0) &#123; referenceConfig.setModule((ModuleConfig)applicationContext.getBean(reference.module(), ModuleConfig.class)); &#125; if (reference.consumer() != null &amp;&amp; reference.consumer().length() &gt; 0) &#123; referenceConfig.setConsumer((ConsumerConfig)applicationContext.getBean(reference.consumer(), ConsumerConfig.class)); &#125; try &#123; referenceConfig.afterPropertiesSet(); &#125; catch (RuntimeException e) &#123; throw (RuntimeException) e; &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125; &#125; referenceConfigs.putIfAbsent(key, referenceConfig); referenceConfig = referenceConfigs.get(key); &#125; return referenceConfig.get();&#125; AbstractDirectoryAbstractDirectory是增加router的Directory。com.alibaba.dubbo.rpc.cluster.directory.AbstractDirectory1234567891011121314151617181920//获取所有的Invoker，并遍历路由规则过滤求Invokers子集public List&lt;Invoker，并根据路由规则遍历过滤&lt;T&gt;&gt; list(Invocation invocation) throws RpcException &#123; if (destroyed)&#123; throw new RpcException("Directory already destroyed .url: "+ getUrl()); &#125; List&lt;Invoker&lt;T&gt;&gt; invokers = doList(invocation); List&lt;Router&gt; localRouters = this.routers; // local reference if (localRouters != null &amp;&amp; localRouters.size() &gt; 0) &#123; for (Router router: localRouters)&#123; try &#123; if (router.getUrl() == null || router.getUrl().getParameter(Constants.RUNTIME_KEY, true)) &#123; invokers = router.route(invokers, getConsumerUrl(), invocation); &#125; &#125; catch (Throwable t) &#123; logger.error("Failed to execute router: " + getUrl() + ", cause: " + t.getMessage(), t); &#125; &#125; &#125; return invokers;&#125; RegistryDirectoryRegistryDirectory: 注册目录服务，通过此类可以获取消费者消费的提供者的所有实例Invokerscom.alibaba.dubbo.registry.integration.RegistryDirectory 1234567891011121314151617181920212223242526272829//从注册中心获取消费者消费的提供者的所有实例Invokerspublic List&lt;Invoker&lt;T&gt;&gt; doList(Invocation invocation) &#123; if (forbidden) &#123; throw new RpcException(RpcException.FORBIDDEN_EXCEPTION, "Forbid consumer " + NetUtils.getLocalHost() + " access service " + getInterface().getName() + " from registry " + getUrl().getAddress() + " use dubbo version " + Version.getVersion() + ", Please check registry access list (whitelist/blacklist)."); &#125; List&lt;Invoker&lt;T&gt;&gt; invokers = null; Map&lt;String, List&lt;Invoker&lt;T&gt;&gt;&gt; localMethodInvokerMap = this.methodInvokerMap; // local reference if (localMethodInvokerMap != null &amp;&amp; localMethodInvokerMap.size() &gt; 0) &#123; String methodName = RpcUtils.getMethodName(invocation); Object[] args = RpcUtils.getArguments(invocation); if(args != null &amp;&amp; args.length &gt; 0 &amp;&amp; args[0] != null &amp;&amp; (args[0] instanceof String || args[0].getClass().isEnum())) &#123; invokers = localMethodInvokerMap.get(methodName + "." + args[0]); // 可根据第一个参数枚举路由 &#125; if(invokers == null) &#123; invokers = localMethodInvokerMap.get(methodName); &#125; if(invokers == null) &#123; invokers = localMethodInvokerMap.get(Constants.ANY_VALUE); &#125; if(invokers == null) &#123; Iterator&lt;List&lt;Invoker&lt;T&gt;&gt;&gt; iterator = localMethodInvokerMap.values().iterator(); if (iterator.hasNext()) &#123; invokers = iterator.next(); &#125; &#125; &#125; return invokers == null ? new ArrayList&lt;Invoker&lt;T&gt;&gt;(0) : invokers;&#125; ConditionRouterConditionRouter：条件路由，关于条件路由的定义以及正则匹配，具体见这里，源码是：Map&lt;String, MatchPair&gt; parseRule(String rule)com.alibaba.dubbo.rpc.cluster.router.condition.ConditionRouter 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 路由规则逻辑 * 1. 必须同时配置 消费端匹配 和 提供端过滤 * 2. 消费端不匹配 返回所有 invoker * * @param invokers * @param url refer url * @param invocation * @param &lt;T&gt; * @return * @throws RpcException */ public &lt;T&gt; List&lt;Invoker&lt;T&gt;&gt; route(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) throws RpcException &#123; if (invokers == null || invokers.size() == 0) &#123; return invokers; &#125; try &#123; //消费端匹配 如果不匹配 直接返回所有invoker if (!matchWhen(url)) &#123; return invokers; &#125; List&lt;Invoker&lt;T&gt;&gt; result = new ArrayList&lt;Invoker&lt;T&gt;&gt;(); if (!hasThenCondition()) &#123; logger.warn("The current consumer in the service blacklist. consumer: " + NetUtils.getLocalHost() + ", service: " + url.getServiceKey()); return result; &#125; for (Invoker&lt;T&gt; invoker : invokers) &#123; if (matchThen(invoker.getUrl(), url)) &#123; result.add(invoker); &#125; &#125; if (result.size() &gt; 0) &#123; return result; &#125; else if (force) &#123; logger.warn("The route result is empty and force execute. consumer: " + NetUtils.getLocalHost() + ", service: " + url.getServiceKey() + ", router: " + url.getParameterAndDecoded(Constants.RULE_KEY)); return result; &#125; &#125; catch (Throwable t) &#123; logger.error("Failed to execute condition router rule: " + getUrl() + ", invokers: " + invokers + ", cause: " + t.getMessage(), t); &#125; return invokers; &#125; 修改源码（此步才是关键）dubbo运行流程先理清一下dubbo的运行流程： Spring扫描注解得到所有的AnnotationBean。即@Refernce和@Service注解的引用 对每一个AnnotationBean执行：private Object refer(Reference reference, Class&lt;?&gt; referenceClass)，得到ConcurrentMap&lt;String, ReferenceBean&lt;?&gt;&gt; referenceConfigs和Set&lt;ServiceConfig&lt;?&gt;&gt; serviceConfigs = new ConcurrentHashSet&lt;ServiceConfig&lt;?&gt;&gt;()。这两个都是AnnotationBean的属性。（在这里查找对注解的解析代码，层层引用，可以定位到：com.alibaba.dubbo.config.AbstractConfig#appendAnnotation） 从ReferenceConfig&lt;T&gt;调用get()方法，会进行init()操作。ReferenceConfig会被缓存，所以消费者代理也会被缓存（消费者代理是ReferenceConfig的属性）。里面有下面一段源码（留意我写的注释）： 1234//attributes是？？？StaticContext.getSystemContext().putAll(attributes);//attributes通过系统context进行存储.//map是时间戳、PID、revision、methods等参数，这些参数编码之后作为url的refer参数的值。所以不能将router放在map里面。后面会说routers是如何识别的ref = createProxy(map); //创建代理 通过带来调用服务提供者 注解配置解析首先，路由器应该是可配置的，那么就要添加解析路由器的相关代码。dubbo是客户端负载均衡，理论上路由的判断应放在客户端执行，所以这里我本来打算修改消费者相关的代码。com.alibaba.dubbo.config.ReferenceConfig 的private void init()方法初始化并解析消费者的配置。但注意：如果在com.alibaba.dubbo.config.AbstractReferenceConfig 抽象类添加router属性，会有点问题：（AbstractInterfaceConfig属于提供者和消费者共同的配置接口，而且从注册中心加载URL的方法protected List&lt;URL&gt; loadRegistries(boolean provider)由它提供，所以扩展路由代码写在AbstractInterfaceConfig里面）： 12345678910// 路由protected String router;public String getRouter() &#123; return router;&#125;public void setRouter(String router) &#123; this.router = router;&#125; 先修改注解：增加routers的配置参数：com.alibaba.dubbo.config.annotation.Reference1String[] routers() default &#123;&#125;; 注意到：在ReferenceConfig的构造方法里会调用appendAnnotation来解析注解（注：对于数组合会并成’,’分隔的字符串）。123public ReferenceConfig(Reference reference) &#123; appendAnnotation(Reference.class, reference);&#125; 修改com.alibaba.dubbo.config.AbstractConfig#appendAnnotation:123456789101112131415161718Object value = method.invoke(annotation, new Object[0]);if (value != null &amp;&amp; ! value.equals(method.getDefaultValue())) &#123; Class&lt;?&gt; parameterType = ReflectUtils.getBoxedClass(method.getReturnType()); if ("filter".equals(property) || "listener".equals(property) || "router".equals(property)) &#123; //增加路由处理 parameterType = String.class; value = StringUtils.join((String[]) value, ","); //将数组处理成字符串 &#125; else if ("parameters".equals(property)) &#123; parameterType = Map.class; value = CollectionUtils.toStringMap((String[]) value); &#125; try &#123; //此处的setter方法属于AbstractReferenceConfig Method setterMethod = getClass().getMethod(setter, new Class&lt;?&gt;[] &#123; parameterType &#125;); //根据参数类型和参数，通过反射调用set方法设置指。 setterMethod.invoke(this, new Object[] &#123; value &#125;); //调用set方法 &#125; catch (NoSuchMethodException e) &#123; // ignore &#125;&#125; 经过了上面的一步，注解的参数已经被解析到AbstractConfig里面了。 router设置注意：dubbo的router扩展如下(这种方式是JDK SPI)： 写2个类，分别实现Router和RouterFactory。RouterFactory假设是com.alibaba.dubbo.rpc.cluster.router.xxxRouterFactory 然后让dubbo可以识别routerName是xxx的路由。 1234567891011src |-main |-java |-com |-xxx |-xxxRouterFactory.java (实现RouterFactory接口) |-xxxRouter.java (实现Router接口) |-resources |-META-INF |-dubbo |-com.alibaba.dubbo.rpc.cluster.RouterFactory (纯文本文件，内容为：xxx=com.alibaba.dubbo.rpc.cluster.router.xxxRouterFactory) 设置路由，通常在初始化AbstractDirectory时设置，或由RegistryDirectory的notify通知更新。 1234567891011121314protected void setRouters(List&lt;Router&gt; routers)&#123; // copy list routers = routers == null ? new ArrayList&lt;Router&gt;() : new ArrayList&lt;Router&gt;(routers); // append url router String routerkey = url.getParameter(Constants.ROUTER_KEY); //看是否存在routerName，URL会通过category=routers&amp;router=xxxRouterName&amp;dynamic=false传入router相关配置 注：Constants.ROUTER_KEY="router" if (routerkey != null &amp;&amp; routerkey.length() &gt; 0) &#123; RouterFactory routerFactory = ExtensionLoader.getExtensionLoader(RouterFactory.class).getExtension(routerkey);//getExtensionLoader获取ExtensionLoader然后根据routerName获取RouterFactory实例 routers.add(routerFactory.getRouter(url)); &#125; // append mock invoker selector routers.add(new MockInvokersSelector()); Collections.sort(routers); this.routers = routers;&#125; 那么问题来了，URL是怎么生成的呢？ 123456789101112131415else &#123; // 通过注册中心配置拼装URL，其他两种情况是本地调用和指定的点对点调用。可不做考虑 List&lt;URL&gt; us = loadRegistries(false);//URL从注册中心来 if (us != null &amp;&amp; us.size() &gt; 0) &#123; for (URL u : us) &#123; URL monitorUrl = loadMonitor(u); if (monitorUrl != null) &#123; map.put(Constants.MONITOR_KEY, URL.encode(monitorUrl.toFullString())); &#125; urls.add(u.addParameterAndEncoded(Constants.REFER_KEY, StringUtils.toQueryString(map))); &#125; &#125; if (urls == null || urls.size() == 0) &#123; throw new IllegalStateException("No such any registry to reference " + interfaceName + " on the consumer " + NetUtils.getLocalHost() + " use dubbo version " + Version.getVersion() + ", please config &lt;dubbo:registry address=\"...\" /&gt; to your spring config."); &#125;&#125; 到此为止，在URL添加上router即可，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748protected List&lt;URL&gt; loadRegistries(boolean provider) &#123; checkRegistry(); List&lt;URL&gt; registryList = new ArrayList&lt;URL&gt;(); if (registries != null &amp;&amp; registries.size() &gt; 0) &#123; for (RegistryConfig config : registries) &#123; String address = config.getAddress(); if (address == null || address.length() == 0) &#123; address = Constants.ANYHOST_VALUE; &#125; String sysaddress = System.getProperty("dubbo.registry.address"); if (sysaddress != null &amp;&amp; sysaddress.length() &gt; 0) &#123; address = sysaddress; &#125; if (address != null &amp;&amp; address.length() &gt; 0 &amp;&amp; ! RegistryConfig.NO_AVAILABLE.equalsIgnoreCase(address)) &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); appendParameters(map, application); appendParameters(map, config); map.put("path", RegistryService.class.getName()); map.put("dubbo", Version.getVersion()); map.put(Constants.TIMESTAMP_KEY, String.valueOf(System.currentTimeMillis())); if (ConfigUtils.getPid() &gt; 0) &#123; map.put(Constants.PID_KEY, String.valueOf(ConfigUtils.getPid())); &#125; if (! map.containsKey("protocol")) &#123; if (ExtensionLoader.getExtensionLoader(RegistryFactory.class).hasExtension("remote")) &#123; map.put("protocol", "remote"); &#125; else &#123; map.put("protocol", "dubbo"); &#125; &#125; List&lt;URL&gt; urls = UrlUtils.parseURLs(address, map); for (URL url : urls) &#123; url = url.addParameter(Constants.REGISTRY_KEY, url.getProtocol()); if(this.router!=null&amp;&amp;this.router.length()&gt;0)&#123;//router存在 url = url.addParameter(Constants.ROUTER_KEY, this.router);//添加路由参数 &#125; url = url.setProtocol(Constants.REGISTRY_PROTOCOL); if ((provider &amp;&amp; url.getParameter(Constants.REGISTER_KEY, true)) || (! provider &amp;&amp; url.getParameter(Constants.SUBSCRIBE_KEY, true))) &#123; registryList.add(url); &#125; &#125; &#125; &#125; &#125; return registryList;&#125; 修改的具体代码Github查看PS：一看提交记录，其实才改了几行代码，但是其中需要做的准备工作感觉很多。这也算是Coding的乐趣之一了:) 使用示例项目示例见Github关键代码 KeyRouter：包括工厂类和实例类 JDK SPI：注意文件名和位置 注解引入router：12@Reference(version = "*",router = "keyRouter")private HelloRest helloRest3; 关于XML配置文件这里并没有扩展XML配置文件，有兴趣可以自行扩展。或许有一天我会补上这儿的代码。 其实原理上大体差不多。区别应该是初始化的时候读的是XML。 那么关注的类是： NamespaceHandler BeandefinitionParser DubboBeanDefinitionParser DubboNamespaceHandler TODO从注册中心推送的routers是否会影响现有的routers？覆盖/被覆盖/叠加/抛异常？待研究。 写在最后dubbo的确是优秀的框架，一开始我的设计是重写负载均衡器，然后走读代码+官方文档，才发现dubbo早已考虑到了这些需求。dubbo提供了router层，稍作变动问题便可以迎刃而解。很感谢阿里的开源。：) 引用Dubbo用户指南Dubbo开发者指南]]></content>
      <categories>
        <category>RPC</category>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>RPC</tag>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud搭建分布式服务治理架构]]></title>
    <url>%2F2017%2F06%2F08%2FSpring%20Cloud%E6%90%AD%E5%BB%BA%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[文章介绍了如何通过Spring Cloud搭建分布式服务治理架构，相关源码开源于Github。 spring-cloud-demoGithub地址单机演示版本在single-node分支，对应配置中心仓库是：config-repo。完全分布式演示版本在multi-nodes分支，对应配置中心仓库是：config-repo-nodes。主分支属于伪分布式版本。目的实现高可用的微服务架构。对应配置中心仓库是：config-repo-cluster。主分支是较为完整的分布式多实例演示（主要体现在高可用、负载均衡、横向扩展）。包括配置中心，监控，服务注册与发现，网关，微服务调用，还差熔断器的验证。 集群部署准备(单个节点模拟多节点)网关zuul 实现动态路由zuul 解决zuul的单点问题（未完成） 服务注册中心 Eureka HA 3个实例两两相互注册 分布式配置中心 配置中心注册至Eureka，由Eureka实现HA。 压测调优配置各个组件的最优实践参数。 DevOps在我的设想中，通过java -jar xxx.jar --spring.profiles.active=dev指定激活的配置文件，通过--server.port=20001指定端口号(方便创建多实例)。然后通过简单的脚本时间多服务器，多实例的自动化部署。还可以做成一个DevOps的可视化系统。这个会在集群模式里添加详细的设计和演示。当然，还有更加好的DevOps实践，这里不展开讲了（事实上我也不懂~~ :) ）。 配置中心配置中心仓库位于该项目config-repo-cluster分支。application.yml则是公共配置文件。ms-admin-dev.yml 表示spring.application.name是ms-admin的应用（其实是serviceId，如果不配置，默认是applicationName），spring.profiles.active是dev的配置文件。配置文件位于仓库根目录，每个应用的配置通过文件名来区分。也可以新建仓库或者分支实现区分。 单机模拟分布式集群环境 导入IDE 执行父项目Spring-cloud-demo的maven命令，分别是clean和package 之后执行命令：（务必顺序执行）12345678910111213141516171819202122# 其实在win下命令不好使，就不写win下批处理脚本了。还是一条条执行吧……cd D:\xxx\xxx\spring-cloud-demo #项目所在目录# 启动服务注册与发现中心java -jar 00-ms-registry-discovery/target/ms-registry-discovery-1.0-SNAPSHOT.jar --spring.profiles.active=devMaster --server.port=20000java -jar 00-ms-registry-discovery/target/ms-registry-discovery-1.0-SNAPSHOT.jar --spring.profiles.active=devBackup1 --server.port=20001java -jar 00-ms-registry-discovery/target/ms-registry-discovery-1.0-SNAPSHOT.jar --spring.profiles.active=devBackup2 --server.port=20002# 启动配置中心实例java -jar 10-ms-config-center/target/ms-config-center-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20003java -jar 10-ms-config-center/target/ms-config-center-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20004# 启动监控管理java -jar 20-ms-admin/target/ms-admin-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20005# 启动网关(多活分流)java -jar 30-ms-gateway/target/ms-gateway-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20006java -jar 30-ms-gateway/target/ms-gateway-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20007# 启动网关管理（动态路由）java -jar 31-ms-gateway-admin/target/ms-gateway-admin-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20008# POST调用 http://localhost:20008/refreshRoute 刷新路由(ms-gateway会有log提示更新成功)# 启动微服务java -jar 40-ms-hello/target/ms-hello-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20009java -jar 40-ms-hello/target/ms-hello-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20010java -jar 41-ms-world/target/ms-world-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20011java -jar 41-ms-world/target/ms-world-1.0-SNAPSHOT.jar --spring.profiles.active=dev --server.port=20012 访问地址 Spring Boot Admin http://localhost:20005/ Eureka http://localhost:20000/ http://localhost:20001/ http://localhost:20002/ 直接访问配置中心 http://localhost:20003/{applicationName}/{profile}/{lable/branch} http://localhost:20003/ms-admin/dev/config-repo-cluster http://localhost:20004/ms-admin/dev/config-repo-cluster 直接访问服务 http://localhost:20009/hello http://localhost:20010/hello http://localhost:20011/world http://localhost:20012/world 代理访问 http://localhost:20006/ms-hello/hello http://localhost:20007/ms-hello/hello http://localhost:20006/ms-world/world http://localhost:20007/ms-world/world 配置之坑 如果是下划线ms-hello ,则host = new URI(url).getHost();会返回null，从而导致org.springframework.cloud.netflix.feign.FeignClientsRegistrar#validate 抛出校验异常 FeignClients不可用 诡异：将spring.cloud.config的配置写在application.yml不生效，只能写在bootstrap.yml，让我觉得好坑。（原来貌似是可以的） 分布式高可用理论上可以实现系统强大的横向扩展能力，增加集群的负载和吞吐。但是，注意所有涉及到网关层面的设计，只是实现了分流，但是还是存在单点问题的。而关于zuul/nginx的HA，暂行搁置。 方案一：此架构图首要启动配置中心，之后其他应用从配置中心读取配置（但是配置中心需要对反向代理服务器实现HA。PS：此方案已被抛弃。） 方案二：此架构图首要启动服务注册中心，配置中心随后注册到Eureka。（通过Eureka实现配置中心的HA。） PS 网关用于控制对外（但不限制此要求）提供的路由。可用nginx替代（服务端负载均衡） 内部服务治理则直接基于Eureka实现服务注册发现，Ribbon（Feign）实现负载均衡（客户端负载均衡） TODO 整合更多组件 调优]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>分布式架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式一致性算法：二段提交和三段提交]]></title>
    <url>%2F2017%2F05%2F31%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%EF%BC%9A%E4%BA%8C%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%92%8C%E4%B8%89%E6%AE%B5%E6%8F%90%E4%BA%A4%2F</url>
    <content type="text"><![CDATA[分布式一致性算法：二段提交和三段提交，常用于分布式系统的事务操作。 概念定义 协调者：coordinator,事务管理器，每个节点（参与者）都知道自己的事务状态，但无法知道自己之外的事务状态。协调者的作用是管理所有参与者的事务状态。决定整个事务的commit/rollback。 参与者：participant,资源管理器，真正控制锁定事务资源的节点。可以理解为提供RPC服务的节点。 前提 节点可恢复：宕机后节点必须是能恢复提供正常服务的。 日志持久化：节点写入的undo log和redo log是被认为持久化且不可改变的。 单点问题：单点问题属于HA问题，不在讨论范围内。 二段提交2PC(Two-phase commit)过程 准备阶段(Voting phase)：协调者询问参与者开启事务，参与者写入undo log和redo log，并告知协调者已就绪 事务提交阶段(Commit phase)：协调者收到所有参与者的就绪确认，则向参与者发送提交事务请求。参与者commit事务并释放资源。并告知协调者事务已提交。 异常分析新的协调者也就无法告知其他参与者执行commit还是rollback了。 参与者宕机：协调者会告知所有参与者回滚事务，执行rollback。参与者如果恢复，对于未提交的事务也采取rollback操作。 协调者宕机：协调者在提交阶段宕机，如果此前没有任何参与者收到协调者的commit or abort命令，则整个事务的状态不可知。只能等待协调者恢复后，重新发送命令告诉参与者。（如果引入协调者响应超时机制，超时后去询问其他参与者，参与者都未提交事务，则事务回滚；若有一个参与者提交了事务，则所有参与者提交事务。） 协调者和参与者同时宕机：而参与者执行了rollback/commit操作，却来不及返回给协调者。此时即使选举了新的协调者，也不知道宕掉的参与者的事务状态。 缺点 最终一致性：参与者宕机，等待恢复期间数据节点间的数据不一致。实现的是最终一致性。 资源阻塞问题：准备阶段事务已经start，各个参与者锁定所有的相关资源，等待事务结束后才释放资源。通过Lease机制，超时后自动释放事务资源，缓解阻塞问题。特别是协调者在提交阶段没有发送commit/abort命令时宕机，所有参与者都不知道事务状态，只能一直阻塞等待协调者恢复服务。 伪代码来自《Distributed Systems: Principles and Paradigms》一书。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859//协调者的代码实现write("START_2PC tolocal log");multicast("VOTE_REQUESTto all participants"); //协调者广播消息给参与者并写入WALwhile(not all votes have been collected) //协调者等待所有参与者返回ack消息&#123; waitfor("any incoming vote");//有一个参与者响应了 if(timeout)//参与者响应超时 &#123; write("GLOBAL_ABORT to local host"); //协调者放弃事务 multicast("GLOBAL_ABORT to all participants"); //广播所有参与者放弃事务 exit(); &#125; record(vote);//记录参与者的响应消息&#125;if(all participants send VOTE_COMMIT and coordinatorvotes COMMIT)//所有参与者同意提交事务，并且协调者确定应提交事务&#123; write("GLOBAL_COMMIT to local log"); //协调者提交本地WAL multicast("GLOBAL_COMMIT to all participants"); //广播所有参与者提交WAL&#125;else //如果投票是放弃事务&#123; write("GLOBAL_ABORT to local log"); multicast("GLOBAL_ABORT to all participants");&#125;//参与者的代码实现write("INIT to locallog");waitfor("VOTE_REQUEST from coordinator");//等待协调者的开启事务请求if(timeout) //如果超时，放弃事务&#123; write("VOTE_ABORT to local log"); exit();&#125;if("participantvotes COMMIT") //如果当前参与者同意commit&#123; write("VOTE_COMMIT to local log"); //写入WAL send("VOTE_COMMIT to coordinator"); //告诉协调者已经就绪 waitfor("DESCISION from coordinator"); //等待协调者收集所有参与者的投票，确认是abort or commit if(timeout) //如果协调者响应超时 &#123; multicast("DECISION_REQUEST to other participants"); //询问其他参与者的事务状态 waituntil("DECISION is received"); /// remain blocked 等待确认：是abort or commit write("DECISION to local log"); //写本地日志记录 &#125; if(DECISION == "GLOBAL_COMMIT") //如果确定提交事务 &#123; write("GLOBAL_COMMIT to local log"); &#125; else if(DECISION== "GLOBAL_ABORT") //如果确定放弃事务 &#123; write("GLOBAL_ABORT to local log"); &#125;&#125;else //如果参与者不同意事务，告诉协调者&#123; write("GLOBAL_ABORT to local log"); send("GLOBAL_ABORT to coordinator");&#125; 三段提交3PC(Three-phase commit)过程 canCommit：协调者询问参与者是否可以开启事务？（此时事务没有打开，资源没有被锁定阻塞） preCommit：参与者确认可以提交，协调者请求参与者开启事务，写入redo log和undo log。 doCommit：协调者收到参与者preCommit的确认后，发送请求让参与者提交事务。 异常分析略。大致类似于2PC，区别在于3PC解决的问题和代价。见下文。 解决了2PC的问题和代价3PC将2PC的第一阶段拆分为canCommit和preCommit两个阶段。 资源阻塞问题：canCommit阶段不开启事务，只是询问状态。降低阻塞的可能性。代价：多了一次RPC请求，存在数据不一致的情况。PS：但是之后的preCommit和doCommit不正是2PC吗？2PC会出现的问题理论上也会出现在3PC才对。其解决方法便是增加canCommit阶段，canCommit通过后，参与者宕机是默认commit的，通过这个来减少阻塞。canCommit用来投票，但是preCommit如果反馈的结果是abort事务，不是数据直接不一致了？ 缺点如上，解决2PC的代价便是缺点。只是减少了阻塞，带来更大的数据不一致可能。不得不联想到CAP啊，三者不能共存。 其他分布式事务解决方案 TCC(Try-Confirm-Cancel)模型(2PC的变种)，手动事务补偿 事务性消息队列 普通消息队列+定时补偿机制 引用分布式系统的一致性探讨深入理解分布式系统的2PC和3PC关于分布式事务、两阶段提交协议、三阶提交协议两阶段提交协议的异常处理分布式事务－二阶段提交与三阶段提交Introduction to 3PC 三阶段提交协议简介]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性算法</category>
      </categories>
      <tags>
        <tag>分布式一致性算法</tag>
        <tag>二段提交</tag>
        <tag>三段提交</tag>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法]]></title>
    <url>%2F2017%2F05%2F15%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[基础算法，算法有时候好玩，有时候不好玩。看得懂和写得出差距很大很大。 写在前面文章算法正确性均未验证，只是谈谈思路。 常用算法分治 将大问题分解为小问题 合并小问题的解即为大问题的解，小问题之间相互独立。在大数据中，基于MR思想的分布式计算框架，比如MapReduce和Spark。都是典型的分治法的利用。数据分区，分区内的管道式计算。属于“分”，分区内的数据互不干扰，相互独立。分区结果多路归并，比如Reduce操作。得到最优解。 动态规划贪心回溯分支限界快速排序算法 算法思想：分治法 算法思路： 从数组取一个数作为基准元（固定基准元、随机基准元、三数取中，目的是使分隔成的子数组长度尽可能相等） 将数组分割成左右2个子数组，左数组小于基准元，右数组大于基准元。并返回新的基准元下标。（注意边界条件是low&lt;high） 对左右数组分别递归调用，直到low&lt;high12345678910111213141516171819202122232425262728293031//定义分区函数，将数组分为左右数组并返回大小分界处的元素下标。public int Partition(int[] arr,int low,int high)&#123; //取low下标元素作为基准元 int keyNum = arr[low]; int i = low,j = high; while(i&lt;j)&#123; while(arr[i]&gt;keyNum&amp;&amp;i&lt;j)&#123;//找到左边大于基准元的元素下标 i++; &#125; while(arr[j]&lt;keyNum&amp;&amp;i&lt;j)&#123;//找到右边小于基准元的元素下标 j--; &#125; if(i&lt;j)&#123;//交换i，j元素 swap(arr,i,j); &#125; &#125; /** * 循环结束时，左数组小于基准元，右数组大于基准元， * 但是基准元还是位于low处（等于被忽略），所以应将基准元放回中间位置 **/ swap(arr,low,i); return i; //此时的i就是i==j(low==high)的时候&#125;public void qsort(int[] arr,int low,int high)&#123; if(low&lt;high)&#123; int division = Partition(arr,low,high); //得到左右分区边界下标 qsort(arr,low,division-1); //division可以认为有序，递归时应跳过division项 qsort(arr,division+1,high); &#125;&#125;`]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络协议之HTTP、TCP、UDP]]></title>
    <url>%2F2017%2F05%2F15%2F%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E4%B9%8BHTTP%E3%80%81TCP%E3%80%81UDP%2F</url>
    <content type="text"><![CDATA[介绍网络协议：HTTP、TCP、UDP 写在前面此篇blog仅作了解计算机网络的一些基础知识。如果对于网络协议有更深入的追求。强烈推荐TCP/IP详解：卷I：协议 OSI模型OSI即Open System Interconnection，开放系统互联。OSI定义了7层协议栈。 层 描述 例子 应用层 这层协议是为了满足特定应用的通信需求而设计的，通常定义一个服务接口 HTTP、FTP、STMP 表示层 这层协议将以一种网络表示传输数据，这种表示与计算机使用的表示无关，两种表示可能完全不相同。如果需要，可以在这一层对数据进行加密 TLS安全、CORBA数据表示 会话层 在这层要实现可靠性和适应性，例如故障检测和自动恢复 SIP 传输层 这是处理消息（而不是数据包）的最低一层。消息被定位到与进程相连的通信端口上。这层协议是可以面向连接的，也可以是无连接的。 TCP、UDP 网络层 在特定网络的计算机间传输数据包，在一个WAN或一个互连网络中，这一层负责生成一个通过路由器的路径。在单一的LAN中不需要路由 IP、ATM虚电路 数据链路层 负责再有直接物理连接的结点间传输数据包。在WAN中，传输是在路由器间或路由器或主机间进行的。在LAN中，传输是在任意一对的主机间进行的 Ethernet MAC、ATM信元传送、PPP 物理层 指驱动网络的电路和硬件。它通过发送模拟信号传输二进制数据序列，用电信号的振幅或频率调制信号（在电缆电路上），光信号（在光纤电路上），或其他电磁信号（在无线电和微波电路上） Ethernet基带信号、ISDN 两台主机在网络中通信的示意图： OSI模型详情图： UDP(User Datagram Protocol,用户数据报协议) 特点 面向无连接。即发送数据前不需要建立连接 不可靠传输，可能发生丢包，并且传输可能乱序 UDP占用较少系统资源，协议信息简单常见运用：网络视频、语音聊天、直播等 UDP封装IP是网络层协议，UDP是网络层上一层的传输层协议。所以UDP数据报与IP首部会组成一个IP数据报： UDP首部 源端口号：发送进程 目的端口号：接收进程 UDP长度：指UDP首部和UDP数据的字节长度 UDP校验和：可选，UDP报文可没有UDP校验和。覆盖UDP首部和UDP数据 TCP(Transmission Control Protocol,传输控制协议) 特点 TCP为应用层提供全双工服务。这意味数据能在两个方向上独立地进行传输。 面向连接，可靠传输，保证传输顺序、完整 TCP封装IP是网络层协议，TCP是网络层上一层的传输层协议。所以TCP数据报与IP首部会组成一个IP数据报： TCP首部 源端口号：发送进程 目的端口号：接收进程 TCP校验和：必选，TCP报文必须有TCP校验和。覆盖TCP首部和TCP数据 同步序号：Synchronize Sequence Numbers，SYN，用来标识TCP发送端向接收端发送的数据字节流，它表示在这个报文段中的第一个数据字节。一共32位，SYN&gt;232－1后从0开始计数。 确认序号：Acknowledgement Number，ACK，上次已成功收到数据字节序号加1 TCP状态位 SYN：SYNchronous，同步标识 ACK：ACKnowledgement，确认标识 URG：URGent，紧急指针（urgent pointer）有效 PSH：PuSH，接收方应尽快将这个报文段交给应用层 RST：ReSeT，重建连接 FIN：FINish，发送端完成发送任务 TCP三次握手与四次挥手看到一篇blog通过FSM(Finite State Machine,有限状态机)伪代码描述TCP传输过程。 PS：此blog的FSM图和算法都是中文描述，非常清晰。 先看FSM图： TCB：Transmit Control Block，传输控制模块，它用于记录TCP协议运行过程中的变量。对于有多个连接的TCP，每个连接都有一个TCB。TCB结构的定义包括这个连接使用的源端口、目的端口、序号、应答序号、对方窗口大小、己方窗口大小、TCP状态、TCP的重传有关变量。 RTS：Request-to-send CTS：Clear-to-send Segment：传输的数据报文 State Description CLOSED No connection exists LISTEN Passive open received; waiting for SYN SYN-SENT SYN sent; waiting for ACK SYN-RCVD SYN+ACK sent; waiting for ACK ESTABLISHED Connection established; data transfer in progress FIN-WAIT-1 First FIN sent; waiting for ACK FIN-WAIT-2 ACK to first FIN received; waiting for second FIN CLOSED-WAIT First FIN received, ACK sent; waiting for application to close TIME-WAIT Second FIN received, ACK sent; waiting for 2MSL time-out LAST-ACK Second FIN sent; waiting for ACK CLOSING Both sides decided to close simultaneously 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157TCP_Main_Module (Segment)&#123; Search the TCB Table //搜索TCB Table if (corresponding TCB is not found)&#123;//TCB不存在 Create a TCB with the state CLOSED //创建TCB，状态为CLOSED &#125; Find the state of the entry in the TCB table //获取TCB的state switch (state)&#123; case CLOSED state:&#123; //CLOSED状态 if ("passive open" message received)&#123; //收到“被动打开”报文，进入LISTEN状态 go to LISTEN state. &#125; if ("active open" message received)&#123;//收到“主动打开”报文 send a SYN segment //发送SYN信号 go to SYN-SENT state //进入SYN-SENT状态 &#125; if (any segment received)&#123;//收到报文 send an RST segment //发送RST(Reset)信号 &#125; if (any other message received)&#123;//收到其他报文 issue an error message //报告错误响应 &#125; break; &#125; case LISTEN state:&#123; if ("send data" message received) &#123; Send a SYN segment //发送SYN信号 Go to SYN-SENT state //进入SYN-SENT状态 &#125; if (any SYN segment received)&#123; Send a SYN + ACK segment Go to SYN-RCVD state &#125; if (any other segment or message received)&#123; Issue an error message &#125; break; &#125; case SYN-SENT state:&#123; if (time-out)&#123; Go to CLOSED state &#125; if (SYN segment received)&#123; Send a SYN + ACK segment Go to SYN-RCVD state &#125; if (SYN + ACK segment received)&#123; Send an ACK segment Go to ESTABLISHED state &#125; if (any other segment or message received)&#123; Issue an error message &#125; break; &#125; case SYN-RCVD state:&#123; if (an ACK segment received)&#123; Go to ESTABLISHID state &#125; if (time-out)&#123; Send an RTS segment Go to CLOSED state &#125; if ("close" message received)&#123; Send a FIN segment Go to FIN-WAIT-I state &#125; if (RTS segment received)&#123; Go to LISTEN state &#125; if (any other segment or message received)&#123; Issue an error message &#125; break; &#125; case ESTABLISHED state:&#123; if (a FIN segment received)&#123; Send an ACK segment Go to CLOSED-WAIT state &#125; if ("close" message received)&#123; Send a FIN segment Go to FIN-WAIT-I &#125; if (a RTS or an SYN segment received)&#123; Issue an error message &#125; if (data or ACK segment received)&#123; call the input module &#125; if ("send" message received)&#123; call the output module &#125; break; &#125; case FIN-WAIT-1 state:&#123; if (a FIN segment received)&#123; Send an ACK segment Go to CLOSING state &#125; if (a FIN + ACK segment received)&#123; Send an ACK segment Go to FIN-WAIT state &#125; if (an ACK segment received)&#123; Go to FIN-WAIT-2 state &#125; if (any other segment or message received)&#123; Issue an error message &#125; break; &#125; case FIN-WAIT-2 state:&#123; if (a FIN segment received)&#123; Send an ACK segment Go to TIME-WAIT state &#125; break; &#125; case CLOSING state:&#123; if (an ACK segment received)&#123; Go to TIME-WAIT state &#125; if (any other message or segment received)&#123; Issue an error message &#125; break; &#125; case TIME-WAIT state:&#123; if (time-out)&#123; Go to CLOSED state &#125; if (any other message or segment received)&#123; Issue an error message &#125; break; &#125; case CLOSED-WAIT state:&#123; if ("close" message received)&#123; Send a FIN segment Go to LAST-ACK state &#125; if (any other message or segment received)&#123; Issue an error message &#125; break; &#125; case LAST-ACK state:&#123; if (an ACK segment received)&#123; Go to CLOSED state &#125; if (any other message or segment received)&#123; Issue an error message &#125; break; &#125; &#125;&#125; // end module 初始化Server监听指定端口，等待Clinet的申请建立TCP连接 三次握手（建立连接）握手过程 第一次握手：Client发送SYN seq=x给Server请求确认。Client状态：SYN_SEND 第二次握手：Server回复ACK=x+1确认收到Client的请求。Client-&gt;Server连接建立。并发送SYN seq=x，请求Client建立连接。Server状态：SYN_RECV 第三次握手：Client回复ACK=y+1确认收到Server的请求。Server-&gt;Client连接建立。至此双方连接建立完毕，可以开始传输数据由于TCP是全双工的，需要至少三次握手来同步双方的SYN序号，如此才能保证Client&lt;-&gt;Server的数据是有序、准确的。(Client顺序发送，Server顺序接收) 所以说，三次握手是保证TCP可靠传输的前提。 网络异常分析可通过上面的FSM伪代码分析握手失败的处理。 第[1,2]次握手失败，Client收不到Server的SYN+ACK，超时后关闭连接 第[2,3]次握手失败，Server收不到Client的ACK，超时后发送RTS报文段，进入CLOSED状态 超时CLOSED：Lease机制的运用，超时释放资源，防止死锁。或者说资源被占用时间过长。 SYN Flood攻击：攻击者恶意申请大量的TCP连接，使Server存在大量等待SYN-ACK的TCP连接，占用系统资源。这些资源都只能等待超时后释放。 数据传输传输过程如图所示： 首先执行三次握手：1-3步，初始化了Client的seq(SYN)=1，Server的ack(SYN)=1 Client发送数据包，seq=1，ACK=1，数据包Len=1440 Client发送数据包，seq=1441，ACK=1，数据包Len=1440 Server接收数据包，返回seq=1，ACK=1441 Client发送数据包，seq=1441+1440=2881，ACK=1，数据包Len=1440 Client发送数据包，seq=2881+1440=4321，ACK=1，数据包Len=1440 Server接收数据包，返回seq=1，ACK=2881 ……(重复) 注意：Client顺序发送数据包（seq number），Server必然顺序ACK数据包（ack number）。但不是一应一答的阻塞。（猜想应存在缓冲区，先缓存一个时间窗口的数据包，Server再顺序ACK） 由上图可以看出：Seq和Ack共同保证了数据的可靠性（顺序传输和顺序确认）。而三次握手分别初始化了Seq和Ack，这便是可靠传输的前提。 流量控制（接收方流量控制）接收方通过接收窗口，让发送方的发送速率不要太快，要让接收方来得及接收。突出的是端到端的流量控制。 接收窗口：rwnd，receiver window/advertised window 假设Client(Sender)-&gt;Server(Receiver)发送数据，Server会告诉Client一个接收窗口，比如rwnd=400字节，那Client会向Server发送400字节的数据。 假设Client每次向Server发送100字节的数据，第一次Server收到100字节后会返回rwnd=300字节…一直循环至rwnd=0。 此时Client会等待Server重新发送一个rwnd。 如果Server-&gt;Client发送的rwnd=200丢失了呢？Client会不会一直等待Server发送非0 rwnd？而Server也在等待Client发送数据？（死锁出现）如何解决死锁？ TCP维护了一个计时器。TCP接到rwnd=0，就启动一个Timer。Timer到期会向Server发送请求非0 rwnd。若Server返回的rwnd仍为0，则重置Timer，直至rwnd&gt;0。 如果是Client-&gt;Server发送的数据丢失了呢？ Server会重新发送rwnd给Client。 拥塞控制（发送方流量控制）在某段时间，若对网络中某资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏——产生拥塞(congestion)。 出现资源拥塞的条件： 对资源需求的总和 &gt; 可用资源。若网络中有许多资源同时产生拥塞，网络的性能就要明显变坏，整个网络的吞吐量将随输入负荷的增大而下降。目的是防止网络过载，强调的是整个网络环境的资源控制。 拥塞窗口：cwnd，congestion window RTT：Round-Trip Time，往返时延。在计算机网络中它是一个重要的性能指标，表示从发送端发送数据开始，到发送端收到来自接收端的确认（接收端收到数据后便立即发送确认），总共经历的时延。 MSS：maximum segment size，TCP协议会将大于MSS的数据包拆分成多个小的数据包进行传输。 慢启动（slow start，exponential increase） 发送方维持一个拥塞窗口，慢启动开始时，发送方使cwnd=MSS。每次接收方ACK后，cwnd会递增一个MSS大小。总体会呈指数增长。 具体如下图（Slow start, exponential increase），随着RTT的递增，ACK和cwnd都会指数增长。 当然，指数增长只在开始阶段存在，当cwnd增长到慢启动的阈值（ssthresh，slow start threshold）后，进入到拥塞避免阶段，cwnd加法增长（Additive Increase）。 拥塞避免（congestion avoidance，additive increase） 拥塞避免阶段，cwnd呈线性（加法）增长（additive increase）。直至超时（丢包，收到3次重复的ACK），或者达到最大窗口。会有一下2种快速恢复处理方式： TCP Tahoe：设置新的ssthresh为当前cwnd的一半，设置cwnd=1。之后进入慢启动阶段。 TCP Reno：设置新的ssthresh为当前cwnd的一半，设置cwnd=ssthresh_new。之后进入拥塞避免阶段。 快速重传（fast retransmission） 重传在TCP中本应等待超时后重复发送数据包，快重传则是在收到3次重复ACK后，则直接立刻重发而不用等待超时。 快速恢复（fast recovery） ​ 四次挥手（断开连接） 第一次挥手：Client发送seq=x+2，ACK=y+1。请求Server关闭连接。Client状态：FIN-WAIT-1 第二次挥手：Server确认ACK x+3。Server状态：CLOSED-WAIT 第三次挥手：Server发送seq=y+1，请求Client关闭连接。Server状态：LAST-ACK 第四次挥手：Client收到Server的关闭连接请求。进入状态TIME-WAIT。并发送ACK=y+2确认关闭连接。 HTTP(HyperText Transfer Protocol,超文本传输协议) HTTP/1.1首先，我们知道HTTP协议是基于TCP进行传输的。 持久连接（persistent connection）HTTP/1.1对比HTTP/1.0引入了持久连接（persistent connection），所谓持久连接，即默认TCP连接不关闭，TCP连接可以被多个HTTP请求复用。避免重复的三次握手建立连接。 在HTTP/1.0需要设置Connection: keep-alive来开启持久连接。 TCP连接在超时keep-alive timeout或者服务器收到Connection: close命令后，连接被关闭。 注意：HTTP的持久连接不等价于HTTP的长连接，持久连接维持的是传输层的TCP连接 管道机制（pipelining）管道机制：即同一个TCP连接里面，可以将多个HTTP请求（request）整批提交，而在发送过程中不需先等待服务端的回应。 HTTP头部 HTTP/2.0HTTPS(Hyper Text Transfer Protocol over Secure Socket Layer) HTTPS头部 基于Java NIO的封装 Mina Netty 引用计算机网络11–OSI参考模型第11章 UDP:用户数据报协议第17章 TCP：传输控制协议《分布式系统：概念与设计》《TCP/IP Protocol Suite Fourth Edition》TCP 的那些事儿（上）TCP 的那些事儿（下）&lt;再看TCP/IP第一卷&gt;TCP/IP协议族中的最压轴戏—-TCP协议及细节传输控制协议TCP流量控制TCP拥塞控制]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>TCP</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】Spark性能优化指南——高级篇]]></title>
    <url>%2F2017%2F05%2F14%2F%5B%E8%BD%AC%5DSpark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94%E9%AB%98%E7%BA%A7%E7%AF%87%2F</url>
    <content type="text"><![CDATA[转自美团点评技术团队：Spark性能优化指南——高级篇 原文连接Spark性能优化指南——高级篇 前言继基础篇讲解了每个Spark开发人员都必须熟知的开发调优与资源调优之后，本文作为《Spark性能优化指南》的高级篇，将深入分析数据倾斜调优与shuffle调优，以解决更加棘手的性能问题。 数据倾斜调优调优概述有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。 数据倾斜发生时的现象 绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 数据倾斜发生的原理数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。 因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。 下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。 如何定位导致数据倾斜的代码数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。 某个task执行特别慢的情况首先要看的，就是数据倾斜发生在第几个stage中。 如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。 比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。 这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。123456789val conf = new SparkConf()val sc = new SparkContext(conf)val lines = sc.textFile("hdfs://...")val words = lines.flatMap(_.split(" "))val pairs = words.map((_, 1))val wordCounts = pairs.reduceByKey(_ + _)wordCounts.collect().foreach(println(_)) 通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。 某个task莫名其妙内存溢出的情况这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。 但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。 查看导致数据倾斜的key的数据分布情况知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。 此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。 举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。123val sampledPairs = pairs.sample(false, 0.1)val sampledWordCounts = sampledPairs.countByKey()sampledWordCounts.foreach(println(_)) 数据倾斜的解决方案解决方案一：使用Hive ETL预处理数据 方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。 方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。 方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。 方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 项目实践经验：在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。 解决方案二：过滤少数导致倾斜的key 方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。 方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 解决方案三：提高shuffle操作的并行度 方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。 方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。 解决方案四：两阶段聚合（局部聚合+全局聚合） 方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。 **方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。 方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 第一步，给RDD中的每个key都打上一个随机前缀。JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(10); return new Tuple2&lt;String, Long&gt;(prefix + "_" + tuple._1, tuple._2); &#125; &#125;);// 第二步，对打上随机前缀的key进行局部聚合。JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;);// 第三步，去除RDD中每个key的随机前缀。JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair( new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple) throws Exception &#123; long originalKey = Long.valueOf(tuple._1.split("_")[1]); return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2); &#125; &#125;);// 第四步，对去除了随机前缀的RDD进行全局聚合。JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); 解决方案五：将reduce join转为map join 方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。 方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。 123456789101112131415161718192021222324252627282930313233// 首先将数据量比较小的RDD的数据，collect到Driver中来。List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。// 可以尽可能节省内存空间，并且减少网络传输性能开销。final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);// 对另外一个RDD执行map类操作，而不再是join类操作。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。 List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value(); // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。 Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;(); for(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123; rdd1DataMap.put(data._1, data._2); &#125; // 获取当前RDD数据的key以及value。 String key = tuple._1; String value = tuple._2; // 从rdd1数据Map中，根据key获取到可以join到的数据。 Row rdd1Value = rdd1DataMap.get(key); return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value)); &#125; &#125;);// 这里得提示一下。// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。// rdd2中每条数据都可能会返回多条join后的数据。 解决方案六：采样倾斜key并分拆join操作 方案适用场景：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。 方案实现思路：对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。而另外两个普通的RDD就照常join即可。最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。方案实现原理：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。 方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。 方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(false, 0.1);// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._1, 1L); &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1); &#125; &#125;);final Long skewedUserid = reversedSampledRDD.sortByKey(false).take(1).get(0)._2;// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;);// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return !tuple._1.equals(skewedUserid); &#125; &#125;);// rdd2，就是那个所有key的分布相对较为均匀的rdd。// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。// 对扩容的每条数据，都打上0～100的前缀。JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter( new Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;).flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; Random random = new Random(); List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(i + "_" + tuple._1, tuple._2)); &#125; return list; &#125; &#125;);// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + "_" + tuple._1, tuple._2); &#125; &#125;) .join(skewedUserid2infoRDD) .mapToPair(new PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple) throws Exception &#123; long key = Long.valueOf(tuple._1.split("_")[1]); return new Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2); &#125; &#125;);// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);// 将倾斜key join后的结果与普通key join后的结果，uinon起来。// 就是最终的join结果。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2); 解决方案七：使用随机前缀和扩容RDD进行join 方案适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。 方案实现思路：该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。然后将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 方案实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 123456789101112131415161718192021222324252627282930// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair( new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(0 + "_" + tuple._1, tuple._2)); &#125; return list; &#125; &#125;);// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + "_" + tuple._1, tuple._2); &#125; &#125;);// 将两个处理后的RDD进行join即可。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD); 解决方案八：多种方案组合使用在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。 shuffle调优调优概述大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。 ShuffleManager发展概述在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。 HashShuffleManager运行原理未经优化的HashShuffleManager下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。 我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。 那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。 接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。 shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。 优化后的HashShuffleManager下图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。 开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。 当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。 假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。 SortShuffleManager运行原理SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。 普通运行机制下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。 在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。 一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。 SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。 bypass运行机制下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下： shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。 不是聚合类的shuffle算子（比如reduceByKey）。此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。 该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。 而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 shuffle相关参数调优以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。 spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。spark.shuffle.memoryFraction 默认值：0.2 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。spark.shuffle.manager 默认值：sort 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。spark.shuffle.consolidateFiles 默认值：false 参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。 调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。写在最后的话本文分别讲解了开发过程中的优化原则、运行前的资源参数设置调优、运行中的数据倾斜的解决方案、为了精益求精的shuffle调优。希望大家能够在阅读本文之后，记住这些性能调优的原则以及方案，在Spark作业开发、测试以及运行的过程中多尝试，只有这样，我们才能开发出更优的Spark作业，不断提升其性能。]]></content>
      <categories>
        <category>转载</category>
        <category>Spark优化</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】Spark性能优化指南——基础篇]]></title>
    <url>%2F2017%2F05%2F14%2F%5B%E8%BD%AC%5DSpark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[转自美团点评技术团队：Spark性能优化指南——基础篇 原文连接Spark性能优化指南——基础篇 前言在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。 开发调优调优概述Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。 原则一：避免创建重复的RDD通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。一个简单的例子1234567891011121314151617// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)val rdd2 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd2.reduce(...)// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)rdd1.reduce(...) 原则二：尽可能复用同一个RDD除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。一个简单的例子1234567891011121314151617181920212223242526// 错误的做法。// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。JavaPairRDD&lt;Long, String&gt; rdd1 = ...JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)// 分别对rdd1和rdd2执行了不同的算子操作。rdd1.reduceByKey(...)rdd2.map(...)// 正确的做法。// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。// 其实在这种情况下完全可以复用同一个RDD。// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。JavaPairRDD&lt;Long, String&gt; rdd1 = ...rdd1.reduceByKey(...)rdd1.map(tuple._2...)// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。 原则三：对多次使用的RDD进行持久化当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。 Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。对多次使用的RDD进行持久化的代码示例1234567891011121314151617// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。// 正确的做法。// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").cache()rdd1.map(...)rdd1.reduce(...)// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").persist(StorageLevel.MEMORY_AND_DISK_SER)rdd1.map(...)rdd1.reduce(...) 对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。Spark的持久化级别 持久化级别 含义解释 MEMORY_ONLY 使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。 MEMORY_AND_DISK 使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。 MEMORY_ONLY_SER 基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 MEMORY_AND_DISK_SER 基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 DISK_ONLY 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。 MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等. 对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。 如何选择一种最合适的持久化策略 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 原则四：尽量避免使用shuffle类算子如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。Broadcast与map进行join代码示例12345678910111213141516// 传统的join操作会导致shuffle操作。// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。val rdd3 = rdd1.join(rdd2)// Broadcast+map的join操作，不会导致shuffle操作。// 使用Broadcast将一个数据量较小的RDD作为广播变量。val rdd2Data = rdd2.collect()val rdd2DataBroadcast = sc.broadcast(rdd2Data)// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。val rdd3 = rdd1.map(rdd2DataBroadcast...)// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。 原则五：使用map-side预聚合的shuffle操作如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。 所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。 比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。 原则六：使用高性能的算子除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。 使用reduceByKey/aggregateByKey替代groupByKey详情见“原则五：使用map-side预聚合的shuffle操作”。 使用mapPartitions替代普通mapmapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！ 使用foreachPartitions替代foreach原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。 使用filter之后进行coalesce操作通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 使用repartitionAndSortWithinPartitions替代repartition与sort类操作repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 原则七：广播大变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。 因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。广播大变量的代码示例123456789101112// 以下代码在算子函数中，使用了外部的变量。// 此时没有做任何特殊操作，每个task都会有一份list1的副本。val list1 = ...rdd1.map(list1...)// 以下代码将list1封装成了Broadcast类型的广播变量。// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。// 每个Executor内存中，就只会驻留一份广播变量副本。val list1 = ...val list1Broadcast = sc.broadcast(list1)rdd1.map(list1Broadcast...) 原则八：使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。 对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。 以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：123456// 创建SparkConf对象。val conf = new SparkConf().setMaster(...).setAppName(...)// 设置序列化器为KryoSerializer。conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")// 注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 原则九：优化数据结构Java中，有三种类型比较耗费内存： 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 资源调优调优概述在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。 Spark作业基本运行原理详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。 当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。 因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。 task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。 以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。 资源参数调优了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。 num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。 driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。 资源参数参考示例以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节： ./bin/spark-submit \ --master yarn-cluster \ --num-executors 100 \ --executor-memory 6G \ --executor-cores 4 \ --driver-memory 1G \ --conf spark.default.parallelism=1000 \ --conf spark.storage.memoryFraction=0.5 \ --conf spark.shuffle.memoryFraction=0.3 \ 写在最后的话根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。在后续的《Spark性能优化指南——高级篇》中，我们会详细讲解数据倾斜调优以及Shuffle调优。]]></content>
      <categories>
        <category>转载</category>
        <category>Spark优化</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yarn基础：概念与原理]]></title>
    <url>%2F2017%2F05%2F14%2FYarn%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[简单介绍Yarn里面的一些基础知识和原理实现。 基本概念Yarn也是Master-Slaves的设计模式，Master是ResourceManager，Slaves便是NodeManager。 ResourceManager负责集群资源统一管理和计算框架管理，主要包括调度与应用程序管理，RM是系统中将资源分配给各个应用的最终决策者 调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序 应用程序管理器负责管理整个系统中的所有应用程序，包括应用程序提交，与调度器协商资源，启动并监控AppMaster运行状态 NodeManager负责监控可用资源、报告故障情况和管理容器的生命周期。 ApplicationMasterAM理解为一个作业的“头”，它管理这个作业的声明周期，包括动态增加和减少资源的使用；管理执行流程（例如针对不同的Map输出调用Reducers）；处理故障和计算偏差(computation skew)；以及执行其它的本地优化。由于RM和NM的通信协议是可扩展的通信协议，所以AM支持任意语言的代码。 ContainerYarn中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。 Yarn资源分配流程给一个Spark通过Yarn Cluster提交作业的流程原理图，看图即可。 引用Apache Hadoop YARN: Yet another resource negotiator]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Yarn</category>
      </categories>
      <tags>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm API]]></title>
    <url>%2F2017%2F05%2F14%2FStorm_API%2F</url>
    <content type="text"><![CDATA[介绍Storm API的含义与使用。 【介绍G1 GC的】http://blog.csdn.net/renfufei/article/details/41897113 http://workman666.iteye.com/blog/2348863 http://www.flyne.org/article/204http://www.flyne.org/article/190/2http://www.flyne.org/article/222http://www.flyne.org/article/199http://www.flyne.org/article/216http://storm.apache.org/releases/current/Trident-API-Overview.htmlhttp://storm.apache.org/releases/current/]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过Dubbo学习RPC]]></title>
    <url>%2F2017%2F05%2F09%2F%E9%80%9A%E8%BF%87Dubbo%E5%AD%A6%E4%B9%A0RPC%2F</url>
    <content type="text"><![CDATA[RPC基础，通过Dubbo的设计学习RPC框架的基本组成。 Dubbo依赖关系由dubbo可以看到一个基本的RPC框架设计和依赖。 Provider向注册中心注册服务(pub) Consumer订阅注册中心消息(sub) Provider向注册中心注册服务时会被注册中心推送至Consumer Consumer通过注册中心获取到服务的注册信息，比如调用地址等。Consumer通过调用地址列表做负载均衡（客户端负载均衡），然后调用Provider。数据之间需要序列化后通过网络传输 监控 注册中心（服务注册与发现）序列化协议 协议 优点 缺点 数据格式 可读性 Kyro Avro Xstream Hessian Jackson JDK 网络传输 框架 JDK底层 传输协议 连接方式 优点 缺点 Netty NIO Mina NIO Grizzly NIO Twisted REST类 负载均衡服务端负载均衡(nginx/zuul)由网关统一管理应用请求的分发，好处是服务请求入口统一管理，方便做限流、权限控制等；缺点是所有负载均衡的分发压力（CPU和IO）全部归于网关。 客户端负载均衡(dubbo loadbalance/ribbon)有客户端从配置中心获取服务实例列表，然后客户端根据服务列表做负载均衡的处理。好处是负载均衡的分发压力分摊给客户端，缺点是不方便做请求的统一管理。 负载均衡策略一般来说，负载均衡的算法有3大类：轮询、哈希以及随机。 监控Dubbo支持协议(序列化以及网络传输)dubbo将对象序列化，包括header(codec)（序列化编码方式，可选）和body(serialization)（对象序列化后的内容，二进制或者字符串）。Client通过网络传输，将序列化内容发送给服务端。 协议 访问地址 dubbo协议 dubbo协议 rmi协议 rmi协议 hessian协议 hessian协议 http协议 http协议 webservice协议 webservice协议 thrift协议 thrift协议 memcached协议 memcached协议 redis协议 redis协议 Dubbo服务集群容错Failover 失败转移失败转移，当出现失败，重试其它服务器，通常用于读操作，但重试会带来更长延迟。以下是源码，只保留关键部分。com.alibaba.dubbo.rpc.cluster.support.FailoverClusterInvoker1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @param invocation Invocation. (API, Prototype, NonThreadSafe) 此对象保存了方法名、参数类型、参数值、Attachment以及当前上下文的Invoker * @param invokers Invoker. (API/SPI, Prototype, ThreadSafe) 当前上下文的Invoker。Invoker用于执行Invocation。此处的List&lt;Invoker&lt;T&gt;&gt; invokers是注册中心获取到的所有实例 * @param loadbalance 负载均衡器，提供select方法选择客户端调用的实例 * @return * @throws RpcException */public Result doInvoke(Invocation invocation, final List&lt;Invoker&lt;T&gt;&gt; invokers, LoadBalance loadbalance) throws RpcException &#123; List&lt;Invoker&lt;T&gt;&gt; copyinvokers = invokers; checkInvokers(copyinvokers, invocation); //此方法用于检查invokers的大小是否为0，为0抛出异常 // 通过方法名、RETRIES_KEY、DEFAULT_RETRIES获取重试次数 int len = getUrl().getMethodParameter(invocation.getMethodName(), Constants.RETRIES_KEY, Constants.DEFAULT_RETRIES) + 1; if (len &lt;= 0) &#123; //如果配置了负数，那是不行的。至少也得重试1次。 len = 1; &#125; // retry loop. RpcException le = null; // last exception. List&lt;Invoker&lt;T&gt;&gt; invoked = new ArrayList&lt;Invoker&lt;T&gt;&gt;(copyinvokers.size()); // invoked invokers. 存储已经执行过的Invoker Set&lt;String&gt; providers = new HashSet&lt;String&gt;(len); // 存储提供者，其实就是提供者的实例地址，URL for (int i = 0; i &lt; len; i++) &#123; //重试时，进行重新选择，避免重试时invoker列表已发生变化. //注意：如果列表发生了变化，那么invoked判断会失效，因为invoker实例已经改变 if (i &gt; 0) &#123; checkWhetherDestroyed(); //检查AbstractClusterInvoker的destroyed是否为true copyinvokers = list(invocation); //根据invocation拿到它自己的提供者列表 //重新检查一下 checkInvokers(copyinvokers, invocation); //检查一下提供者是否存在 &#125; Invoker&lt;T&gt; invoker = select(loadbalance, invocation, copyinvokers, invoked); //通过负载均衡器选择实例 invoked.add(invoker); RpcContext.getContext().setInvokers((List)invoked); //更新上下文已经被执行过的实例 try &#123; Result result = invoker.invoke(invocation); if (le != null &amp;&amp; logger.isWarnEnabled()) &#123; // le是上次调用失败保留的异常信息 &#125; return result; &#125; catch (RpcException e) &#123; if (e.isBiz()) &#123; // biz exception. throw e; &#125; le = e; &#125; catch (Throwable e) &#123; le = new RpcException(e.getMessage(), e); &#125; finally &#123; providers.add(invoker.getUrl().getAddress()); &#125; &#125; throw new RpcException(...); &#125; Failfast 快速失败快速失败，只发起一次调用，失败立即报错，通常用于非幂等性的写操作。以下是源码，只保留关键部分。com.alibaba.dubbo.rpc.cluster.support.FailfastClusterInvoker123456789101112public Result doInvoke(Invocation invocation, List&lt;Invoker&lt;T&gt;&gt; invokers, LoadBalance loadbalance) throws RpcException &#123; checkInvokers(invokers, invocation); Invoker&lt;T&gt; invoker = select(loadbalance, invocation, invokers, null); try &#123; return invoker.invoke(invocation); &#125; catch (Throwable e) &#123; if (e instanceof RpcException &amp;&amp; ((RpcException)e).isBiz()) &#123; // biz exception. throw (RpcException) e; &#125; throw new RpcException(...); //调用失败，直接抛出RpcException &#125;&#125; Failsafe 失败安全失败安全，出现异常时，直接忽略，通常用于写入审计日志等操作。以下是源码，只保留关键部分。com.alibaba.dubbo.rpc.cluster.support.FailfastClusterInvoker12345678910public Result doInvoke(Invocation invocation, List&lt;Invoker&lt;T&gt;&gt; invokers, LoadBalance loadbalance) throws RpcException &#123; try &#123; checkInvokers(invokers, invocation); Invoker&lt;T&gt; invoker = select(loadbalance, invocation, invokers, null); return invoker.invoke(invocation); &#125; catch (Throwable e) &#123; logger.error("Failsafe ignore exception: " + e.getMessage(), e); return new RpcResult(); // ignore &#125;&#125; Failback 失败自动恢复失败自动恢复，后台记录失败请求，定时重发，通常用于消息通知操作。以下是源码，只保留关键部分。com.alibaba.dubbo.rpc.cluster.support.FailbackClusterInvoker12345678910111213141516171819202122232425262728293031protected Result doInvoke(Invocation invocation, List&lt;Invoker&lt;T&gt;&gt; invokers, LoadBalance loadbalance) throws RpcException &#123; try &#123; checkInvokers(invokers, invocation); Invoker&lt;T&gt; invoker = select(loadbalance, invocation, invokers, null); return invoker.invoke(invocation); &#125; catch (Throwable e) &#123; addFailed(invocation, this); //调用失败，后台线程定时重发调用 return new RpcResult(); // ignore &#125;&#125;private void addFailed(Invocation invocation, AbstractClusterInvoker&lt;?&gt; router) &#123; if (retryFuture == null) &#123; synchronized (this) &#123; if (retryFuture == null) &#123; retryFuture = scheduledExecutorService.scheduleWithFixedDelay(new Runnable() &#123; public void run() &#123; // 收集统计信息 try &#123; retryFailed(); &#125; catch (Throwable t) &#123; // 防御性容错 logger.error("Unexpected error occur at collect statistic", t); &#125; &#125; &#125;, RETRY_FAILED_PERIOD, RETRY_FAILED_PERIOD, TimeUnit.MILLISECONDS); &#125; &#125; &#125; failed.put(invocation, router);&#125; Forking 并行调用并行调用，只要一个成功即返回，全部异常则返回最后一个异常。通常用于实时性要求较高的操作，但需要浪费更多服务资源。以下是源码，只保留关键部分。com.alibaba.dubbo.rpc.cluster.support.ForkingClusterInvoker12345678910111213141516171819202122232425262728293031323334353637383940414243444546public Result doInvoke(final Invocation invocation, List&lt;Invoker&lt;T&gt;&gt; invokers, LoadBalance loadbalance) throws RpcException &#123; checkInvokers(invokers, invocation); final List&lt;Invoker&lt;T&gt;&gt; selected; final int forks = getUrl().getParameter(Constants.FORKS_KEY, Constants.DEFAULT_FORKS); final int timeout = getUrl().getParameter(Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT); if (forks &lt;= 0 || forks &gt;= invokers.size()) &#123; //如果配置的并行调用数量小于0或者大于最大提供者实例数量，则设置为最大提供者实例数量 selected = invokers; &#125; else &#123; selected = new ArrayList&lt;Invoker&lt;T&gt;&gt;(); for (int i = 0; i &lt; forks; i++) &#123; //轮询forks次 //在invoker列表(排除selected)后,如果没有选够,则存在重复循环问题.见select实现. Invoker&lt;T&gt; invoker = select(loadbalance, invocation, invokers, selected); if(!selected.contains(invoker))&#123;//防止重复添加invoker，因为负载均衡器可能选择到相同的实例。如果选择到相同的实例，会导致并行数量减少 selected.add(invoker); &#125; &#125; &#125; RpcContext.getContext().setInvokers((List)selected); final AtomicInteger count = new AtomicInteger(); // 异常数量计数器 final BlockingQueue&lt;Object&gt; ref = new LinkedBlockingQueue&lt;Object&gt;(); //用来存储提供者返回的结果或者是调用后抛出的异常 for (final Invoker&lt;T&gt; invoker : selected) &#123; executor.execute(new Runnable() &#123; //Executors.newCachedThreadPool线程池调度线程，异步调用提供者 public void run() &#123; try &#123; Result result = invoker.invoke(invocation); ref.offer(result); // &#125; catch(Throwable e) &#123; int value = count.incrementAndGet(); if (value &gt;= selected.size()) &#123;//如果异常次数已经等于提供者数量，证明所有提供者调用失败，将异常放入队列 ref.offer(e); &#125; &#125; &#125; &#125;); &#125; try &#123; Object ret = ref.poll(timeout, TimeUnit.MILLISECONDS); //阻塞获取BlockingQueue的结果（可能是正确结果或者异常） if (ret instanceof Throwable) &#123; Throwable e = (Throwable) ret; throw new RpcException(e instanceof RpcException ? ((RpcException)e).getCode() : 0, "Failed to forking invoke provider " + selected + ", but no luck to perform the invocation. Last error is: " + e.getMessage(), e.getCause() != null ? e.getCause() : e); &#125; return (Result) ret; &#125; catch (InterruptedException e) &#123; throw new RpcException("Failed to forking invoke provider " + selected + ", but no luck to perform the invocation. Last error is: " + e.getMessage(), e); &#125;&#125; Broadcast 广播调用轮询所有提供者实例，只返回最后一个提供者实例的结果。任意一个实例抛出异常则整个RPC过程异常。通常用于通知所有提供者更新缓存或日志等本地资源信息。以下是源码，只保留关键部分。com.alibaba.dubbo.rpc.cluster.support.BroadcastClusterInvoker123456789101112131415161718192021public Result doInvoke(final Invocation invocation, List&lt;Invoker&lt;T&gt;&gt; invokers, LoadBalance loadbalance) throws RpcException &#123; checkInvokers(invokers, invocation); RpcContext.getContext().setInvokers((List)invokers); RpcException exception = null; Result result = null; for (Invoker&lt;T&gt; invoker: invokers) &#123; try &#123; result = invoker.invoke(invocation); &#125; catch (RpcException e) &#123; exception = e; logger.warn(e.getMessage(), e); &#125; catch (Throwable e) &#123; exception = new RpcException(e.getMessage(), e); logger.warn(e.getMessage(), e); &#125; &#125; if (exception != null) &#123; throw exception; &#125; return result;&#125; Mergeable 合并调用调用多个实例，并调用合并器Merger合并所有的返回结果这个代码比较长，略。com.alibaba.dubbo.rpc.cluster.support.MergeableClusterInvoker Dubbo负载均衡算法Random LoadBalance(随机均衡算法)com.alibaba.dubbo.rpc.cluster.loadbalance.RandomLoadBalance根据权重进行实例的随机选择，即每个实例的随机选中的概率是根据权重的决定的。1234567891011121314151617181920212223242526protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123; //URL是消费者的相关信息 int length = invokers.size(); // 总个数 int totalWeight = 0; // 总权重 boolean sameWeight = true; // 权重是否都一样 for (int i = 0; i &lt; length; i++) &#123; int weight = getWeight(invokers.get(i), invocation); totalWeight += weight; // 累计总权重 if (sameWeight &amp;&amp; i &gt; 0 &amp;&amp; weight != getWeight(invokers.get(i - 1), invocation)) &#123; sameWeight = false; // 计算所有权重是否一样 &#125; &#125; if (totalWeight &gt; 0 &amp;&amp; ! sameWeight) &#123; // 如果权重不相同且权重大于0则按总权重数随机 int offset = random.nextInt(totalWeight); // 并确定随机值落在哪个片断上 for (int i = 0; i &lt; length; i++) &#123; offset -= getWeight(invokers.get(i), invocation); if (offset &lt; 0) &#123; return invokers.get(i); &#125; &#125; &#125; // 如果权重相同或权重为0则均等随机 return invokers.get(random.nextInt(length));&#125; RoundRobin LoadBalance(权重轮循均衡算法)com.alibaba.dubbo.rpc.cluster.loadbalance.RoundRobinLoadBalance1234567891011121314151617181920212223242526272829303132333435363738394041protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123; String key = invokers.get(0).getUrl().getServiceKey() + "." + invocation.getMethodName(); int length = invokers.size(); // 总个数 int maxWeight = 0; // 最大权重 int minWeight = Integer.MAX_VALUE; // 最小权重 final LinkedHashMap&lt;Invoker&lt;T&gt;, IntegerWrapper&gt; invokerToWeightMap = new LinkedHashMap&lt;Invoker&lt;T&gt;, IntegerWrapper&gt;(); int weightSum = 0; for (int i = 0; i &lt; length; i++) &#123; int weight = getWeight(invokers.get(i), invocation); maxWeight = Math.max(maxWeight, weight); // 累计最大权重 minWeight = Math.min(minWeight, weight); // 累计最小权重 if (weight &gt; 0) &#123; invokerToWeightMap.put(invokers.get(i), new IntegerWrapper(weight)); weightSum += weight; &#125; &#125; AtomicPositiveInteger sequence = sequences.get(key); if (sequence == null) &#123; sequences.putIfAbsent(key, new AtomicPositiveInteger()); sequence = sequences.get(key); &#125; int currentSequence = sequence.getAndIncrement(); if (maxWeight &gt; 0 &amp;&amp; minWeight &lt; maxWeight) &#123; // 权重不一样 int mod = currentSequence % weightSum; for (int i = 0; i &lt; maxWeight; i++) &#123; for (Map.Entry&lt;Invoker&lt;T&gt;, IntegerWrapper&gt; each : invokerToWeightMap.entrySet()) &#123; final Invoker&lt;T&gt; k = each.getKey(); final IntegerWrapper v = each.getValue(); if (mod == 0 &amp;&amp; v.getValue() &gt; 0) &#123; return k; &#125; if (v.getValue() &gt; 0) &#123; v.decrement(); mod--; &#125; &#125; &#125; &#125; // 取模轮循 return invokers.get(currentSequence % length);&#125; LeastAction LoadBalance(最少活跃调用数均衡算法)com.alibaba.dubbo.rpc.cluster.loadbalance.LeastActiveLoadBalance如果一个实例被调用的次数较少，则会优先调用该实例。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123; int length = invokers.size(); // 总个数 int leastActive = -1; // 最小的活跃数 int leastCount = 0; // 相同最小活跃数的个数 int[] leastIndexs = new int[length]; // 相同最小活跃数的下标 int totalWeight = 0; // 总权重 int firstWeight = 0; // 第一个权重，用于于计算是否相同 boolean sameWeight = true; // 是否所有权重相同 for (int i = 0; i &lt; length; i++) &#123; Invoker&lt;T&gt; invoker = invokers.get(i); int active = RpcStatus.getStatus(invoker.getUrl(), invocation.getMethodName()).getActive(); // 活跃数 int weight = invoker.getUrl().getMethodParameter(invocation.getMethodName(), Constants.WEIGHT_KEY, Constants.DEFAULT_WEIGHT); // 权重 if (leastActive == -1 || active &lt; leastActive) &#123; // 发现更小的活跃数，重新开始 leastActive = active; // 记录最小活跃数 leastCount = 1; // 重新统计相同最小活跃数的个数 leastIndexs[0] = i; // 重新记录最小活跃数下标 totalWeight = weight; // 重新累计总权重 firstWeight = weight; // 记录第一个权重 sameWeight = true; // 还原权重相同标识 &#125; else if (active == leastActive) &#123; // 累计相同最小的活跃数 leastIndexs[leastCount ++] = i; // 累计相同最小活跃数下标 totalWeight += weight; // 累计总权重 // 判断所有权重是否一样 if (sameWeight &amp;&amp; i &gt; 0 &amp;&amp; weight != firstWeight) &#123; sameWeight = false; &#125; &#125; &#125; // assert(leastCount &gt; 0) if (leastCount == 1) &#123; // 如果只有一个最小则直接返回 return invokers.get(leastIndexs[0]); &#125; if (! sameWeight &amp;&amp; totalWeight &gt; 0) &#123; // 如果权重不相同且权重大于0则按总权重数随机 int offsetWeight = random.nextInt(totalWeight); // 并确定随机值落在哪个片断上 for (int i = 0; i &lt; leastCount; i++) &#123; int leastIndex = leastIndexs[i]; offsetWeight -= getWeight(invokers.get(leastIndex), invocation); if (offsetWeight &lt;= 0) return invokers.get(leastIndex); &#125; &#125; // 如果权重相同或权重为0则均等随机 return invokers.get(leastIndexs[random.nextInt(leastCount)]);&#125; ConsistentHash LoadBalance(一致性Hash均衡算法)com.alibaba.dubbo.rpc.cluster.loadbalance.ConsistentHashLoadBalance根据一致性哈希算法，一致性哈希选择器依赖的参数是： virtualInvokers：每个哈希槽对应的Invoker replicaNumber：理解为哈希槽的数量，由hash.nodes指定，默认值是160 identityHashCode：哈希码，根据invokers生成 argumentIndex：参数索引数组，int[]类型。由hash.arguments指定，默认值是0。会根据该参数来确定选择那些输入参数作为key生成的依据。然后MD5之后做Hash。综上，可以认为，同一个方法的调用中，如果参数的哈希值相同则会调用同一个实例。源码解析12345678910protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123; String key = invokers.get(0).getUrl().getServiceKey() + "." + invocation.getMethodName(); int identityHashCode = System.identityHashCode(invokers); ConsistentHashSelector&lt;T&gt; selector = (ConsistentHashSelector&lt;T&gt;) selectors.get(key); if (selector == null || selector.getIdentityHashCode() != identityHashCode) &#123; selectors.put(key, new ConsistentHashSelector&lt;T&gt;(invokers, invocation.getMethodName(), identityHashCode)); selector = (ConsistentHashSelector&lt;T&gt;) selectors.get(key); //ConsistentHashSelector 一致性哈希选择器 &#125; return selector.select(invocation);&#125; 引用Dubbo开发者指南Dubbo架构设计详解]]></content>
      <categories>
        <category>RPC</category>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>RPC</tag>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis Cluster与一致性哈希]]></title>
    <url>%2F2017%2F05%2F09%2FRedis%20Cluster%E4%B8%8E%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%2F</url>
    <content type="text"><![CDATA[Redis Cluster与一致性哈希 写在前面当初由果推因，竟暗合一致性哈希的原理。感觉很有意思~详见分布式令牌桶设计实现（流控）文章下：猜测一下Redis Cluster执行涉及多个key的原子操作的设计原理。后来某一天突然看到集群伸缩与一致性哈希的文章，不谋而合。]]></content>
      <categories>
        <category>一致性哈希</category>
      </categories>
      <tags>
        <tag>Redis Cluster</tag>
        <tag>一致性哈希</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2017%2F05%2F07%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[设计模式的学习，巩固面向对象的开发知识，力求写出更加优雅的代码。 写在前面设计模式的运用个人觉得不用刻意，以解决需求为前提。需求是驱动技术的根本。在业务需求不稳定的时候，设计模式的使用更加需要谨慎衡量。设计模式应用在业务需求的发展十分明确的场景下，比如重构或者写基础框架。设计模式模式是遵循软件设计七大原则的典范，而面向对象的多态、继承与封装是他们实现的基础。 面向对象封装抽象现实模型（现实对象的行为和属性）为类，隐藏用户不关心的属性和方法，暴露用户需要的属性和方法。 继承子类通过继承父类，扩展甚至修改父类的功能，并复用父类的代码。表达的是IS-A关系。 多态 定义：同一父类型的引用在不同运行时的情况下表现出不同的子对象行为形式。 解析：多态发生条件如下： 存在继承 子类重写父类方法 父类引用指向子类对象父类引用指向子类对象，父类引用调用的方法会是其子类对象复写的父类方法。 例子：123List&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add("Hello,World!");//在这里父类引用list指向子类对象ArrayList，父类引用调用add方法，实际上调用的是子类ArrayList的add方法。 软件设计原则开闭原则(Open-Closed Principle, OCP) 定义：软件实体(classes,modules,functions,etc.)应当对扩展开放，对修改关闭。 解析：即在软件设计中，需求变更或者业务的增加，不应修改原有的代码，影响原来的业务逻辑；而是通过增加代码或者配置的形式来解决新需求。 例子：Spring的Bean注入，只需要修改配置文件，便可以注入新的Bean，且不影响原来的Bean。典型地通过工厂模式遵循了开闭原则。这里的依赖注入还遵循了控制翻转的思想，将原本应在代码里实现的对象依赖暴露给配置文件（调用方）控制。 里氏代换原则(Liskov Substitution Principle,LSP) 定义：如果是S是T的子类，则S的实例可以透明地转换为T类型。（多态） 解析：在语言层面，Java早已支持多态。而在代码设计层面，应遵循： 子类尽量不覆盖父类的非抽象方法。（因为此举会破坏父类制定的行为，然而事实上并不严格遵守此要求） 子类方法的返回类型应比父类更为严格。（如果父类返回List，那么子类就返回ArrayList） 例子：List&lt;String&gt; list = new ArrayList&lt;String&gt;(); 依赖倒置原则(Dependence Inversion Principle,DIP) 定义：高层模块不应依赖低层模块。两者都应依赖其抽象。抽象不依赖具体。具体依赖于抽象。 解析：面向接口编程，编程时应高屋建瓴，先制定接口约束实现类的行为规范，再去写具体的实现类。 例子：一般代码里面写服务层，都是写XXXService和XXXServiceImpl，这便是面向接口编程。不过这里的依赖并非指继承，Service之间的依赖通过组合来体现。 接口隔离原则(Interface Segregation Principle,ISP) 定义：客户端不应依赖它们不需要使用的接口，类之间的依赖应尽可能建立在最小的接口上。 解析：程序开发中，接口应该尽量细化，避免冗余的接口实现。 例子：public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable，ArrayList的多接口实现便是接口细化的结果。 合成/聚合复用原则(Composite/Aggregate Reuse Principle,CARP) 定义：通过组合/继承实现代码复用。 解析：尽量通过组合HAS-A实现代码复用，除非有十分明确的继承IS-A语义（代码上则体现为是否需要使用多态），才使用继承实现代码复用。 例子：轮子组合为汽车，轮子是汽车的内部属性（组合），奔驰继承自汽车，汽车可以派生出奔驰、宝马一类的汽车。 迪米特法则(Law of Demeter,LoD) 定义：最少知道原则，调用者尽量少地调用被调用者的接口来实现自己的功能。通俗总结如下： 每个单元应有限度地知道其他单元，其只局限于与当前单元紧密联系的单元 每个单元都应只和熟人说话，不要和陌生人说话 只和你的直系朋友说话 解析：服务端暴露给客户端的接口应该尽可能简单（或者说统一的对外接口），使得服务端修改代码后，而不影响服务端的业务逻辑。 例子：实际开发中，通常通过private、protected之类的关键字控制代码的可见性，目的便是遵循最少知道原则；在提供对外API时，也常常封装统一的对外接口，通过传参来确定内部服务的复杂调用逻辑，而不是将调用逻辑暴露给客户端（有问题优先内部解决）。 单一职责原则(Simple responsibility pinciple,SRP) 定义：每个模块/类只负责单一的业务功能，遵循高内聚的思想。 解析：额，略。 例子：日常开发中，我们早已不自觉的遵循了这个原则，比如服务层和控制器层，会用package去分类。而每个业务功能的Service，通过类再次划分。实现了低耦合和高内聚。 设计模式单例模式]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统相关概念]]></title>
    <url>%2F2017%2F05%2F07%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[分布式系统相关概念 CAPC(Consistency)：A read is guaranteed to return the most recent write for a given client.数据在各个节点是一致的，读操作总能读到最新写入的数据。A(Availability)：A non-failing node will return a reasonable response within a reasonable amount of time (no error or timeout).非故障节点必须在合理有限的时间返回合理的结果（非Error或者Timeout）P(Partition Tolerance)：The system will continue to function when network partitions occur.分布式系统必须在发生网络分区（脑裂，任意子节点集合故障）的情况下也提供服务。对于CAP，在分布式系统中，总是认为网络是不可靠的，则P是必选，所以分布式系统的一致性也就只存在CP和AP两种选择：在发生网络分区时，是选择一致性还是可用性？ PACELC PAC:if there is a partition (P), how does the system trade off availability and consistency (A and C); ELC:else (E), when the system is running normally in the absence of partitions, how does the system trade off latency (L) and consistency (C)?个人理解PACELC是CAP的一个细化描述，其中可用性涉及到时延的描述，PACELC细化了CAP，如下图：分布式系统发生网络分区时，是在A和C之间选择(PA or PC)？还是在L和C之间选择(PL or PC)？意思就是，在分布式系统中，网络分区和一致性是硬性要求。根据不同的情景，牺牲一定的一致性来换取低延迟和可用性。CA:即不考虑网络分区，认为网络是可靠的。（只存在于单节点系统，或系统认为集群是“不可分区的”）CP:牺牲A来获得强一致性。P发生时，为了保证C，必然损失A。例如，写数据最后1个副本时遇上网络延时（分区），此时只能返回Timeout或者Error，可用性丧失。AP：牺牲C来获得A。P发生时，每个节点依然可以提供合理的服务响应。但是在整个分布式系统，各个节点的数据并非一致。一致性丧失，当分区恢复正常，数据逐渐同步，达到最终一致性。 ACID and BASEACID Atomicity:原子性，整个事务中的所有操作，要么全部完成，要么全部不完成，不可能存在中间状态。 Consistency:(强)一致性，事务提交后，所有读操作都可以读到事务提交后的一致的数据。 Isolation:隔离性，并发事务之间相互隔离，互不影响。 Durability:持久性，在事务提交后，该事务对数据库所作的更改便持久的保存在存储介质之中，并且不可变。 BASE(牺牲一致性) Basic Availability:基本可用，分布式系统更加关注系统的可用性。分布式系统在出现不可预知故障时，允许损失部分可用性：性能损失（高延时）和功能损失（降级，参考Spring Cloud熔断器Hystrix） 响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1～2秒。功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 Soft-state:软状态/柔性事务。指在分布式系统中，client和server cluster建立的连接是无状态的(stateless)。可以类比Storm集群，Storm状态信息在Zookeeper。无状态提高了Storm的可用性，但牺牲了一致性（Storm节点的内存消息会丢失）。于我理解，无状态的分布式session是否也属于Soft-state的一种运用？ Eventual consistency:最终一致性 ACID/BASE特点 ACID BASE 强一致性事务优先 高可用可伸缩优先 弱化可用性 弱一致性 悲观锁 乐观锁 rigorous analysis best effort最优性能，充分利用系统资源 复杂机制 简单高效 Quorum NWRQuorom机制，是一种分布式系统中常用的，用来保证数据冗余和最终一致性的投票算法，其主要数学思想来源于鸽巢原理。具体体现是选主投票（过半数投票才会成为Leader）和NWR。 N: 数据副本总数量（可以理解为分布式系统中的节点数，因为通常是一个节点一个副本） W: 写操作需要写入的最少副本数。W越大，写性能越低。 R: 读操作需要读取的最少副本数。R越大，读性能越低。NWR其实也是CAP的另外一种体现。 W+R&gt;N：强一致性。因为读写必然重叠，只要取到版本号最高的数据（读写重叠部分的数据）即为最新数据。无脏读。 W+R&lt;=N：最终一致性。因为读写不重叠，存在发生脏读的可能。 Lease机制Lease，租约。我在用Redis实现分布式锁的时候使用过（利用expired），目的是防止死锁。在Raft算法里面，term也可以理解是一个租约有效期的体现。理解为：跨进程持有的资源使用的是租赁的形式，持有资源的进程如果存在宕机的可能，则租约过期后，资源会被自动释放。 引用Consistency Tradeoffs in Modern Distributed Database System Design15-446 Distributed Systems Spring 2009解惑soft stateCAP和BASE理论]]></content>
      <categories>
        <category>分布式</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM类加载器与自定义类加载器]]></title>
    <url>%2F2017%2F05%2F07%2FJVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E4%B8%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[JVM类加载器与自定义类加载器的目的。 JVM类加载器什么是类的加载类的加载是指将类的.class文件中的二进制数据读入内存中，将其放在运行时数据区的方法区内。JDK 8移除了永久代，并在Native Heap创建了元空间(Metaspace)，类也被加载在元空间。class文件被加载近内存后，JVM会在Java Heap创建1个java.lang.Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 类的生命周期类的生命周期包括： 加载：查找并加载类的二进制数据，在Java Heap中创建一个java.lang.Class对象 连接：连接包含验证、准备、初始化。 验证：文件格式、元数据、字节码、符号引用。 准备：为类的静态变量分配内存，并将其初始化为默认值。 解析：把类中的符号引用转换为直接引用。 初始化：为类的静态变量赋予正确的初始值。 使用：new出对象在程序中被使用。 卸载：执行垃圾回收。如图所示： 类加载器 启动类加载器(Bootstrap ClassLoader)：负责加载存放在JAVA_HOME/jre/lib下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库。 扩展类加载器(Extension ClassLoader)：该加载器由sun.misc.Launcher$ExtClassLoader实现，负载加载存放在JAVA_HOME/jre/lib/ext下，或被java.ext.dirs系统变量指定的路径中的所有类库。 应用程序类加载器(Application ClassLoader)：该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径ClassPath所指定的类。 类加载机制 全盘负责：当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入。 父类委托(双亲委托)：先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类。 缓存机制：缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效。 双亲委托模型JVM规范推荐使用双亲委托的类加载机制，即使是重写类加载器也应遵循此规范（不强制）。类加载器保证了类在JVM的唯一性。符合以下3点的2个类才是同类型的类实例，否则类型强制转换的时候会抛出异常。 两个类来自同一个Class文件 两个类是由同一个虚拟机加载 两个类是由同一个类加载器加载 Tomcat的类加载器 为什么Tomcat需要重写类加载器？ 因为一个Tomcat容器可以部署多个应用，而每个应用引用的jar各不相同，重写自定义加载器可以区分不同的jar引用，防止jar包冲突。类加载器保证了类在JVM的唯一性。 Spring Boot的类加载器 为什么Spring Boot需要重写类加载器？ Spring Boot的FatJar的打包方式打破了Java加载jar包的约定，所以需要重写类加载器去加载class文件以及类库。（详细可以观察Spring Boot工程打包后的目录结构） 引用JVM（8）：JVM知识点总览-高级Java工程师面试必备JVM类加载机制详解（二）类加载器与双亲委派模型]]></content>
      <categories>
        <category>JVM</category>
        <category>资料集</category>
      </categories>
      <tags>
        <tag>类加载器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式一致性算法：Raft]]></title>
    <url>%2F2017%2F05%2F06%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%EF%BC%9ARaft%2F</url>
    <content type="text"><![CDATA[分布式一致性算法：Raft 术语概念节点状态在集群中，每个节点可能具有一下3中状态 Follower：从节点 Candidate：候选人节点，Follower-&gt;Leader的中间态 Leader：主节点，在集群中有且只有1个合法Leader；如果存在多个Leader，则term最大的Leader是合法Leader。 选主 Heartbeat Interval：Leader向Follower发送心跳的间隔 Election Timemout/Heartbeat Timeout：我认为这2个Timeout属于同一个东西。Follower失去了与Leader的心跳(在某个时间内没有收到Leader的心跳消息)，则会随机设置ET=[150,300]ms(随机数可以减少多个Node同时成为Candidate的几率)。election timemout后，该节点会成为Candidate，并开始一轮election term，请求其他节点为其投票。 Election Term：选主有效期，带有版本号，election term版本号最高的Leader为合法Leader。每轮选举每个节点只能投票一次，并重置election timemout。 日志复制 LogEntry/AppendEntry：理解为undo log和redo log，并且认为LogEntry一旦写入磁盘便永不改变（持久化），并且需要保持log的顺序。 其他 term：我理解为租约有效期（参考Lease机制）。 集群启动假设存在1个只有3个节点的集群，分别为A、B、C A、B、C的初始状态均为Follower 由于没有Leader，Follower不能收到Leader的心跳。然后某个Follower会成为Candidate。假设A成为了Candidate，则A会请求B、C投票让A成为Leader（A会给自己投一票，每个节点在一轮投票中只能投票1次）。一个Candidate如果获得了集群大多数节点的投票，则成为Leader。此过程成为选主（Leader Selection）。 所有交互现在通过Leader完成（具体参看日志复制章节的写操作示意图）。 选主(Leader Election)Leader定期(Heartbeat Interval)向Followers发送心跳，如下图，假设Leader和Followers之间发生了网络分区，Followers无法收到Leader的心跳，等待Election Timeout时间后，Followers会发起新的一轮选主(Election Term)。选举开始，Candidate初始化并维护term和Vote Count（投票数量），然后会投票给自己。之后将term发送给Followers，请求他们投票。如果该轮选举未投票，Follower会投票给该Candidate。一旦Candidate获得了集群大多数Followers的投票，则Candidate成为Leader。 如何减少多个Follower成为Candidate？ Election Timeout随机设置 如果同时存在多个Candidate呢？ 投票完毕，如果Candidates票数相等，则重置Election Timeout，进行下一轮选主。 如何保证一轮选举只产生一个Leader？ 每个Follower在同一个Election Term只能投票一次，获得大多数投票的Candidate正式成为Leader 还有疑问：集群如何维护term的全局递增序列？ 无责任猜想：在实际中，通过redis，zookeeper等方式实现，比如redis的incr命令，又或者分布式锁。 日志复制(Log Replication)Raft算法必须认为log一旦写入，便是持久的（即不可改变），这是前提。log也应该是有序的（如何保证有序性不展开讲，不过如果log乱序，一致性没有保证，比如先删后写，和先写后删完全是不一样的最终结果）。以一次写入操作为例： Client发送请求给Leader，设置X=1。Leader写入undo log和redo log(LogEntry/AppendEntry)，但事务并未提交。 Leader将x=1的LogEntry复制至Follower Follower响应leader，告知写入成功（这里遵循NWR规则） Leader响应Client，告知些如此成功。 如果网络分区将集群分为多个子集群，如何保证一致性？ 每个子集群都会有一个Leader，当网络恢复后，在整个集群之中，term最高的Leader为合法Leader。其他Leader产生的log会被回滚（因为无法获得大部分节点的log写入确认，所以所有的log都不能提交），然后同步唯一的合法Leader的log至整个集群。含有大部分节点的子集群依然正常提供服务，但是含有少部分节点的子集群永远无法提交事务，丧失部分可用性。此处就是典型的牺牲可用性来换取一致性。所以Raft是一个CP算法。疑问：根据Raft:Understandable Distributed Consensus介绍，网络分区分为大小集群，分区集群独立选主，但是提交log却按照整个进群的节点进行写入确认。还有如何保证网络分区时如何保证大的子集群的的term一定高于小的子集群？[待研究] Raft的CAP分析Raft是一个CP算法，具体参看日志复制章节介绍。 引用Raft:Understandable Distributed ConsensusIn Search of an Understandable Consensus Algorithm]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性算法</category>
      </categories>
      <tags>
        <tag>分布式一致性算法</tag>
        <tag>Raft</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper基础]]></title>
    <url>%2F2017%2F05%2F06%2FZookeeper%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[简单介绍Zookeeper的知识。 Zookeeper是什么？Zookeeper是一个分布式协调服务器，基于paxos实现了Zab一致性算法。 Zookeeper提供了什么？ 文件系统 对于zk文件系统，在zk文件系统的目录树中，每个目录都被乘坐znode。 znode可有子目录，并且可存放数据，但是EPHEMERAL类（临时znode）znode除外。 znode有版本机制，即znode可以存储多个版本的数据。 znode类型： PERSISTENT(持久化目录节点):client与zk断开连接后，该节点依旧存在 PERSISTENT_SEQUENTIAL(持久化顺序编号目录节点):client与zk断开连接后，该节点依旧存在。且创建新节点时，会被顺序编号 EPHEMERAL(临时目录节点):区别于PERSISTENT，client断开连接后，znode会被删除 EPHEMERAL_SEQUENTIAL(临时顺序编号目录节点):区别于PERSISTENT_SEQUENTIAL，client断开连接后，znode会被删除 发布/订阅通知机制 client可以订阅（watch）指定的znode，当znode发生改变时，zk会发布消息通知client。（Redis也有类似的pub/sub机制）基于这个机制zk和redis都可以实现分布式锁以及服务注册与发现（比如dubbo） Zookeeper可以做什么？ 服务注册与发现：dubbo 配置管理：disconf 集群管理：HBase Storm等 分布式锁 分布式队列 进程屏障Barrier 服务注册与发现作为注册中心，zk大致流程如下，可以实现客户端的负载均衡：我没有看过dubbo源码，但是根据配置文件，可以大致推出以下结论： dubbo:reference：消费者将会监听zookeeper下关于reference下的znode，消费者获得他所有reference的服务注册在zk的信息，维护成一个表。以此实现客户端的负载均衡。 dubbo:service：生产者将service注册到指定znode registry：指定了注册中心的地址，服务注册和发现的address dubbo:protocol：指定了RPC的传输和序列化协议123456789101112&lt;!-- 提供方应用信息，用于计算依赖关系 --&gt;&lt;dubbo:application name="hello-world-app" /&gt;&lt;!-- 使用multicast广播注册中心暴露服务地址 --&gt;&lt;dubbo:registry address="zookeeper://224.5.6.7:1234" /&gt;&lt;!-- 用dubbo协议在20880端口暴露服务 --&gt;&lt;dubbo:protocol name="dubbo" port="20880" /&gt;&lt;!-- 声明需要暴露的服务接口 --&gt;&lt;dubbo:service interface="com.alibaba.dubbo.demo.DemoService" ref="demoService" /&gt;&lt;!-- 声明需要引用的服务接口 --&gt;&lt;dubbo:reference interface="com.foo.BarService" actives="10" /&gt;&lt;!-- 和本地bean一样实现服务 --&gt;&lt;bean id="demoService" class="com.alibaba.dubbo.demo.provider.DemoServiceImpl" /&gt;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat调优、JVM调优与垃圾回收]]></title>
    <url>%2F2017%2F05%2F05%2FTomcat%E8%B0%83%E4%BC%98%E3%80%81JVM%E8%B0%83%E4%BC%98%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[Tomcat调优、JVM调优与垃圾回收，资料主要来源于互联网，整理下来备用。不过感觉描述多有不同，且思且看。 JVM运行时数据区 程序计数器 程序计数器是一块较小的内存空间，它可以看成是当前线程所执行的字节码的行号指示器。程序计数器记录线程当前要执行的下一条字节码指令的地址。由于Java是多线程的，所以为了多线程之间的切换与恢复，每一个线程都需要单独的程序计数器，各线程之间互不影响。这类内存区域被称为“线程私有”的内存区域。由于程序计数器只存储一个字节码指令地址，故此内存区域没有规定任何OutOfMemoryError情况。 虚拟机栈 Java虚拟机栈也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。一个栈帧就代表了一个方法执行的内存模型，虚拟机栈中存储的就是当前执行的所有方法的栈帧（包括正在执行的和等待执行的）。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机中入栈到出栈的过程。我们平时所说的“局部变量存储在栈中”就是指方法中的局部变量存储在代表该方法的栈帧的局部变量表中。而方法的执行正是从局部变量表中获取数据，放至操作数栈上，然后在操作数栈上进行运算，再将运算结果放入局部变量表中，最后将操作数栈顶的数据返回给方法的调用者的过程。虚拟机栈可能出现两种异常：由线程请求的栈深度过大超出虚拟机所允许的深度而引起的StackOverflowError异常；以及由虚拟机栈无法提供足够的内存而引起的OutOfMemoryError异常。 本地方法栈 本地方法栈与虚拟机栈类似，他们的区别在于：本地方法栈用于执行本地方法（Native方法）；虚拟机栈用于执行普通的Java方法。在HotSpot虚拟机中，就将本地方法栈与虚拟机栈做在了一起。本地方法栈可能抛出的异常同虚拟机栈一样。 堆 Java堆是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例：所有的对象实例以及数组都要在堆上分配（The heap is the runtime data area from which memory for all class instances and arrays is allocated）。但Class对象比较特殊，它虽然是对象，但是存放在方法区里。在下面的方法区一节会介绍。Java堆是垃圾收集器（GC）管理的主要区域。现在的收集器基本都采用分代收集算法：新生代和老年代。而对于不同的”代“采用的垃圾回收算法也不一样。一般新生代使用复制算法；老年代使用标记整理算法。对于不同的”代“，一般使用不同的垃圾收集器，新生代垃圾收集器和老年代垃圾收集器配合工作。Java堆可以是物理上不连续的内存空间，只要逻辑上连续即可。Java堆可能抛出OutOfMemoryError异常。 运行时常量池 运行时常量池是方法区的一部分，关于运行时常量池的介绍，参考：String放入运行时常量池的时机与String.intern()方法解惑 方法区 方法区与Java堆一样，是各个线程共享的内存区域。它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。所有的字节码被加载之后，字节码中的信息：类信息、类中的方法信息、常量信息、类中的静态变量等都会存放在方法区。正如其名字一样：方法区中存放的就是类和方法的所有信息。此外，如果一个类被加载了，就会在方法区生成一个代表该类的Class对象（唯一一种不在堆上生成的对象实例）该对象将作为程序访问方法区中该类的信息的外部接口。有了该对象的存在，才有了反射的实现。在Java7之前，HotSpot虚拟机中将GC分代收集扩展到了方法区，使用永久代来实现了方法区。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载。但是在之后的HotSpot虚拟机实现中，逐渐开始将方法区从永久代移除。Java7中已经将运行时常量池从永久代移除，在Java 堆（Heap）中开辟了一块区域存放运行时常量池。而在Java8中，已经彻底没有了永久代，将方法区直接放在一个与堆不相连的本地内存区域，这个区域被叫做元空间。 直接内存/堆外内存 JDK1.4中引用了NIO，并引用了Channel与Buffer，可以使用Native函数库直接分配堆外内存，并通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作。如上文介绍的：Java8以及之后的版本中方法区已经从原来的JVM运行时数据区中被开辟到了一个称作元空间的直接内存区域。 永久代(元空间)数据在Java虚拟机(JVM)内部，class文件中包括类的版本、字段、方法、接口等描述信息，还有运行时常量池，用于存放编译器生成的各种字面量和符号引用。 JVM垃圾回收What/When/HowJVM的GC需要了解的3个问题： 哪些东西是Garbage？ 通过垃圾标识算法：“引用计数算法”/“可达性分析算法”标记的对象便是等待被回收的“垃圾”对象 什么时候触发Collection？ 触发Full GC条件： 调用System.gc()时被建议执行Full GC，但不一定会发生GC。 老年代空间不足，Full GC 方法区空间不足，Full GC Young GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、From Survivor区向To Survivor区复制时，对象大小大于To Survivor可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 触发Young GC条件： 当Eden区满时，Young GC Garbage Collection做了什么？ 释放“垃圾”对象的内存空间。 GC类型 Partial GC：并不收集整个GC堆的模式 Young GC：只收集young gen的GC Old GC：只收集old gen的GC。只有CMS的concurrent collection是这个模式 Mixed GC：收集整个young gen以及部分old gen的GC。只有G1有这个模式 Full GC：收集整个堆，包括young gen、old gen、perm gen（如果存在的话）等所有部分的模式。 最简单的分代式GC策略，按HotSpot VM的serial GC的实现来看，触发条件是： young GC：当young gen中的eden区分配满的时候触发。注意young GC中有部分存活对象会晋升到old gen，所以young GC后old gen的占用量通常会有所升高。 full GC：当准备要触发一次young GC时，如果发现统计数据说之前young GC的平均晋升大小比目前old gen剩余的空间大，则不会触发young GC而是转为触发full GC（因为HotSpot VM的GC里，除了CMS的concurrent collection之外，其它能收集old gen的GC都会同时收集整个GC堆，包括young gen，所以不需要事先触发一次单独的young GC）；或者，如果有perm gen的话，要在perm gen分配空间但已经没有足够空间时，也要触发一次full GC；或者System.gc()、heap dump带GC，默认也是触发full GC。 垃圾标识算法引用计数法Java堆中每个Object都有一个计数器，Object被引用时计数器+1，当Object被置为null或者离开作用域时计数器-1.计数器为0的Object会被标识为“垃圾”对象。但此方法无法解决对象间循环引用的问题。123456789ObjA a = new ObjA();ObjB b = new ObjB();//相互引用（循环引用，构成环）a.prop = b;b.prop = a;//释放对象a = null;b = null;//a，b的对象计数器都不为0 可达性分析算法JVM以一系列GCRoots为根节点，从GCRoot向下搜索，搜索的路径称之为引用链。不在引用链上的对象（无法搜索到的对象）便是“垃圾”对象。 在Java语言中,可作为GC Roots的对象包括下面几种: 虚拟机栈(栈帧中的本地变量表)中引用的对象。 方法区中类静态属性引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI(即一般说的Native方法)引用的对象。可达性分析算法会造成GC停顿，因为此项工作必须在一个一致性的快照中进行，此时所有Java线程停止执行（否则可达性会在分析过程中发生变化）。Sun称此为Stop The World。 Java的引用方式 强引用(StrongReference)：我们常用的复制引用便是强引用，强引用的对象不会被JVM回收。内存不足时，JVM抛出OOM异常。 软引用(SoftReference)：JVM内存不足时会优先回收 弱引用(WeakReference)：JVM无论内存是否充足，在GC的时候都会回收弱引用对象。 虚引用(PhantomReference)：虚引用和软引用、弱引用不同，它并不影响对象的生命周期。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。 理解软引用和弱引用：被软引用关联的对象只有在内存不足时才会被回收，而被弱引用关联的对象在JVM进行垃圾回收时总会被回收。针对上面的特性，软引用适合用来进行缓存，当内存不够时能让JVM回收内存，弱引用能用来在回调函数中防止内存泄露。因为回调函数往往是匿名内部类，隐式保存有对外部类的引用，所以如果回调函数是在另一个线程里面被回调，而这时如果需要回收外部类，那么就会内存泄露，因为匿名内部类保存有对外部类的强引用。 级别 什么时候被垃圾回收 用途 生存时间 强引用 从来不会 对象的一般状态 JVM停止运行时终止 软引用 在内存不足时 对象缓存 内存不足时终止 弱引用 在垃圾回收时 对象缓存 GC运行后终止 虚引用 Unknown Unknown Unknown 垃圾回收算法 标记/清除法Mark-Sweep 标记：标记“垃圾”对象 清除：清除“垃圾”对象，释放内存 优点：GC速度快 缺点：产生大量的内存碎片（即内存空间在物理上不连续）。可能会导致申请大对象时由于没有足够的连续内存空间而触发GC操作。 标记/复制法mark-Copy 标记：标记“垃圾”对象 复制：将内存等量分为2块：A和B。当A的内存消耗完毕，把A的非垃圾对象复制到B，然后清空A所有对象。 优点：不用考虑内存碎片化。 缺点：占用多一倍的内存。 标记/整理法Mark-Compact 标记：标记“垃圾”对象 整理：让非垃圾对象都向一端移动，垃圾对象向另一端移动。最后清除非垃圾对象边界外的所有对象，释放内存空间。 优点：不用考虑内存碎片化。 缺点：GC时间长，因为需要移动对象。 分代回收算法分代回收法是垃圾回收算法的综合应用，根据不同的内存区域执行不同的垃圾回收算法。各种垃圾收集器的差异在此不讨论。 基本概念 理论支持：经验得出——“大部分的对象在生成后马上就变成了垃圾，很少有对象能活得很久”。 分代垃圾回收将刚生成的对象称为新生代，达到一定年龄(进过一次GC即一岁)的对象称为老年代，不同代的对象使用不同回收算法。 新生代对象执行GC称为新生代GC(Young GC)。 新生代对象存活一定次数GC将晋升到老年代，老年代的GC称为老年代GC(Old GC)。GC机制对于用可达性分析法搜索不到的对象，GC并不一定会回收该对象。要完全回收一个对象，至少需要经过两次标记的过程。 第一次标记：对于一个没有其他引用的对象，筛选该对象是否有必要执行finalize()方法，如果没有执行必要，则意味可直接回收。（筛选依据：筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法,或者finalize()方法已经被虚拟机调用过,虚拟机将这两种情况都视为“没有必要执行”，即意味着直接回收。需要注意finalize方法在对象的声明周期内只能被执行一次）。 第二次标记：如果被筛选判定为有必要执行，则会放入FQueue队列，并自动创建一个低优先级的finalize线程来执行释放操作。如果在一个对象释放前被其他对象引用，则该对象会被移出FQueue队列。 堆内存分代 年轻代(Young Generation)：年轻代分为3个区 Eden区：新生对象进入 From Survivor区：上次GC存活的年轻代对象 To Survivor区：Young GC时，复制Eden区，From Survivor区存活的对象至To Survivor区。之后From/To互换。 默认分配比例为：Eden : From Survivor : To Survivor = 8:1:1 老年代(Old Generation)： 长期存活对象，默认Age=15（每GC一次Age+1） 大对象直接进入 Young GC后，Survivor区放不下 PS:持久代(PermanentGeneration)在JDK8已经修改为Metaspace。 分代回收并不一定如此，不同的垃圾收集器可能存在不同的GC算法。 Young GC：标记/复制算法 Old GC：标记/清除算法 垃圾收集器 CMS G1 Parallel Scavenge ParNew Serial Old(MSC) Parallel Old Serial JVM调优参数表 -server/-client： -server:x64默认模式，特点是启动速度比较慢，但运行时性能和内存管理效率很高，适用于生产环境 -client:x86默认模式，特点是启动速度快，但运行时性能和内存管理效率不高，通常用于客户端应用程序或开发调试 -Xms：表示 Java 初始化堆的大小，-Xms 与-Xmx 设成一样的值，避免 JVM 反复重新申请内存，导致性能大起大落，默认值为物理内存的 1/64，默认（MinHeapFreeRatio参数可以调整）空余堆内存小于 40% 时，JVM 就会增大堆直到 -Xmx 的最大限制。 -Xmx：表示最大 Java 堆大小，当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃，因此一般建议堆的最大值设置为可用内存的最大值的80%。如何知道我的 JVM 能够使用最大值，使用 java -Xmx512M -version 命令来进行测试，然后逐渐的增大 512 的值,如果执行正常就表示指定的内存大小可用，否则会打印错误信息，默认值为物理内存的 1/4，默认（MinHeapFreeRatio参数可以调整）空余堆内存大于 70% 时，JVM 会减少堆直到-Xms 的最小限制。 -Xmn：新生代的内存空间大小，注意：此处的大小是（eden+ 2 survivor space)。与 jmap -heap 中显示的 New gen 是不同的。整个堆大小 = 新生代大小 + 老生代大小 + 永久代大小。在保证堆大小不变的情况下，增大新生代后，将会减小老生代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的 3/8。 -Xss：表示每个 Java 线程堆栈大小，JDK 5.0 以后每个线程堆栈大小为 1M，以前每个线程堆栈大小为 256K。根据应用的线程所需内存大小进行调整，在相同物理内存下，减小这个值能生成更多的线程，但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000 左右。一般小的应用， 如果栈不是很深， 应该是128k 够用的，大的应用建议使用 256k 或 512K，一般不易设置超过 1M，要不然容易出现out ofmemory。这个选项对性能影响比较大，需要严格的测试。 -XX:NewSize：设置新生代内存大小。 -XX:MaxNewSize：设置最大新生代新生代内存大小 -XX:PermSize：设置持久代内存大小 -XX:MaxPermSize：设置最大值持久代内存大小，永久代不属于堆内存，堆内存只包含新生代和老年代。注：JDK8已经将持久代更名为元空间Metaspace JDK 8:Metaspace默认为无限制，并且会自动调节Metaspace的大小。当然也可以通过-XX:MaxMetaspaceSize来指定大小。JDK7还将常量池从永久代（位于方法区）移出到堆。 -XX:+AggressiveOpts：作用如其名（aggressive），启用这个参数，则每当 JDK 版本升级时，你的 JVM 都会使用最新加入的优化技术（如果有的话）。 -XX:+UseBiasedLocking：启用一个优化了的线程锁，我们知道在我们的appserver，每个http请求就是一个线程，有的请求短有的请求长，就会有请求排队的现象，甚至还会出现线程阻塞，这个优化了的线程锁使得你的appserver内对线程处理自动进行最优调配。 -XX:+DisableExplicitGC：在程序代码中不允许有显示的调用“System.gc()”。每次在到操作结束时手动调用 System.gc() 一下，付出的代价就是系统响应时间严重降低，就和关于 Xms，Xmx 里的解释的原理一样，这样去调用 GC 导致系统的 JVM 大起大落。 -XX:+UseConcMarkSweepGC：设置年老代为并发收集，即 CMS gc，这一特性只有 jdk1.5后续版本才具有的功能，它使用的是 gc 估算触发和 heap 占用触发。我们知道频频繁的 GC 会造面 JVM的大起大落从而影响到系统的效率，因此使用了 CMS GC 后可以在 GC 次数增多的情况下，每次 GC 的响应时间却很短，比如说使用了 CMS GC 后经过 jprofiler 的观察，GC 被触发次数非常多，而每次 GC 耗时仅为几毫秒。 -XX:+UseParNewGC：对新生代采用多线程并行回收，这样收得快，注意最新的 JVM 版本，当使用 -XX:+UseConcMarkSweepGC 时，-XX:UseParNewGC 会自动开启。因此，如果年轻代的并行 GC 不想开启，可以通过设置 -XX：-UseParNewGC 来关掉。 -XX:MaxTenuringThreshold：设置垃圾最大年龄。如果设置为0的话，则新生代对象不经过 Survivor 区，直接进入老年代。对于老年代比较多的应用（需要大量常驻内存的应用），可以提高效率。如果将此值设置为一 个较大值，则新生代对象会在 Survivor 区进行多次复制，这样可以增加对象在新生代的存活时间，增加在新生代即被回收的概率，减少Full GC的频率，这样做可以在某种程度上提高服务稳定性。该参数只有在串行 GC 时才有效，这个值的设置是根据本地的 jprofiler 监控后得到的一个理想的值，不能一概而论原搬照抄。 -XX:+CMSParallelRemarkEnabled：在使用 UseParNewGC 的情况下，尽量减少 mark 的时间。 -XX:+UseCMSCompactAtFullCollection：在使用 concurrent gc 的情况下，防止 memoryfragmention，对 live object 进行整理，使 memory 碎片减少。 -XX:LargePageSizeInBytes：指定 Java heap 的分页页面大小，内存页的大小不可设置过大， 会影响 Perm 的大小。 -XX:+UseFastAccessorMethods：使用 get，set 方法转成本地代码，原始类型的快速优化。 -XX:+UseCMSInitiatingOccupancyOnly：只有在 oldgeneration 在使用了初始化的比例后 concurrent collector 启动收集。 -Duser.timezone=Asia/Shanghai：设置用户所在时区。 -Djava.awt.headless=true：这个参数一般我们都是放在最后使用的，这全参数的作用是这样的，有时我们会在我们的 J2EE 工程中使用一些图表工具如：jfreechart，用于在 web 网页输出 GIF/JPG 等流，在 winodws 环境下，一般我们的 app server 在输出图形时不会碰到什么问题，但是在linux/unix 环境下经常会碰到一个 exception 导致你在 winodws 开发环境下图片显示的好好可是在 linux/unix 下却显示不出来，因此加上这个参数以免避这样的情况出现。 -XX:CMSInitiatingOccupancyFraction：当堆满之后，并行收集器便开始进行垃圾收集，例如，当没有足够的空间来容纳新分配或提升的对象。对于 CMS 收集器，长时间等待是不可取的，因为在并发垃圾收集期间应用持续在运行（并且分配对象）。因此，为了在应用程序使用完内存之前完成垃圾收集周期，CMS 收集器要比并行收集器更先启动。因为不同的应用会有不同对象分配模式，JVM 会收集实际的对象分配（和释放）的运行时数据，并且分析这些数据，来决定什么时候启动一次 CMS 垃圾收集周期。这个参数设置有很大技巧，基本上满足(Xmx-Xmn)(100-CMSInitiatingOccupancyFraction)/100 &gt;= Xmn 就不会出现 promotion failed。例如在应用中 Xmx 是6000，Xmn 是 512，那么 Xmx-Xmn 是 5488M，也就是老年代有 5488M，CMSInitiatingOccupancyFraction=90 说明老年代到 90% 满的时候开始执行对老年代的并发垃圾回收（CMS），这时还 剩 10% 的空间是 548810% = 548M，所以即使 Xmn（也就是新生代共512M）里所有对象都搬到老年代里，548M 的空间也足够了，所以只要满足上面的公式，就不会出现垃圾回收时的 promotion failed，因此这个参数的设置必须与 Xmn 关联在一起。 -XX:+CMSIncrementalMode：该标志将开启 CMS 收集器的增量模式。增量模式经常暂停 CMS 过程，以便对应用程序线程作出完全的让步。因此，收集器将花更长的时间完成整个收集周期。因此，只有通过测试后发现正常 CMS 周期对应用程序线程干扰太大时，才应该使用增量模式。由于现代服务器有足够的处理器来适应并发的垃圾收集，所以这种情况发生得很少，用于但 CPU情况。 -XX:NewRatio：年轻代（包括 Eden 和两个 Survivor 区）与年老代的比值（除去持久代），-XX:NewRatio=4 表示年轻代与年老代所占比值为 1:4，年轻代占整个堆栈的 1/5，Xms=Xmx 并且设置了 Xmn 的情况下，该参数不需要进行设置。 -XX:SurvivorRatio：Eden 区与 Survivor 区的大小比值，设置为 8，表示 2 个 Survivor 区（JVM 堆内存年轻代中默认有 2 个大小相等的 Survivor 区）与 1 个 Eden 区的比值为 2:8，即 1 个 Survivor 区占整个年轻代大小的 1/10。 -XX:+UseSerialGC：设置串行收集器。 -XX:+UseParallelGC：设置为并行收集器。此配置仅对年轻代有效。即年轻代使用并行收集，而年老代仍使用串行收集。 -XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集，JDK6.0 开始支持对年老代并行收集。 -XX:ConcGCThreads：早期 JVM 版本也叫-XX:ParallelCMSThreads，定义并发 CMS 过程运行时的线程数。比如 value=4 意味着 CMS 周期的所有阶段都以 4 个线程来执行。尽管更多的线程会加快并发 CMS 过程，但其也会带来额外的同步开销。因此，对于特定的应用程序，应该通过测试来判断增加 CMS 线程数是否真的能够带来性能的提升。如果还标志未设置，JVM 会根据并行收集器中的 -XX:ParallelGCThreads 参数的值来计算出默认的并行 CMS 线程数。 -XX:ParallelGCThreads：配置并行收集器的线程数，即：同时有多少个线程一起进行垃圾回收，此值建议配置与 CPU 数目相等。 -XX:OldSize：设置 JVM 启动分配的老年代内存大小，类似于新生代内存的初始大小 -XX:NewSize。 其他更多的参数 jstack/jmap/jstat/jps/jhat现实企业级Java开发中，有时候我们会碰到下面这些问题：OutOfMemoryError，内存不足，内存泄露，线程死锁，锁争用（Lock Contention）Java进程消耗CPU过高等。此时我们需要利用一些命令来分析问题所在。 jstack: jstack主要用来查看某个Java进程内的线程堆栈信息。 jstack [option] pid jstack [option] executable core jstack [option] [server-id@]remote-hostname-or-ip -l : long listings，会打印出额外的锁信息，在发生死锁时可以用jstack -l pid来观察锁持有情况 -m : mixed mode，不仅会输出Java堆栈信息，还会输出C/C++堆栈信息（比如Native方法） jmap(Memory Map)/jhat(Java Heap Analysis Tool):jmap用来查看堆内存使用状况，一般结合jhat使用。如果运行在64位JVM上，可能需要指定-J-d64命令选项参数。 jmap [option] pid jmap [option] executable core jmap [option] [server-id@]remote-hostname-or-ip jstat:JVM统计监测工具 jstat [ generalOption | outputOptions vmid [interval[s|ms] [count]] ] vmid是虚拟机ID，在Linux/Unix系统上一般就是进程ID。interval是采样时间间隔。count是采样数目。比如下面输出的是GC信息，采样时间间隔为250ms，采样数为4：jstat -gc 21711 250 4 jps(Java Virtual Machine Process Status Tool):jps主要用来输出JVM中运行的进程状态信息。 jps [options] [hostid] hostid:如果不指定hostid就默认为当前主机或服务器。 options: -q : 不输出类名、Jar名和传入main方法的参数 -m : 输出传入main方法的参数 -l : 输出main类或Jar的全限名 -v : 输出传入JVM的参数 VisualVM这是一个Java程序性能分析工具 Tomcat调优参数 maxThreads :Tomcat 使用线程来处理接收的每个请求，这个值表示 Tomcat 可创建的最大的线程数，默认值是 200 minSpareThreads：最小空闲线程数，Tomcat 启动时的初始化的线程数，表示即使没有人使用也开这么多空线程等待，默认值是 10。 maxSpareThreads：最大备用线程数，一旦创建的线程超过这个值，Tomcat 就会关闭不再需要的 socket 线程。 上边配置的参数，最大线程 500（一般服务器足以），要根据自己的实际情况合理设置，设置越大会耗费内存和 CPU，因为 CPU 疲于线程上下文切换，没有精力提供请求服务了，最小空闲线程数 20，线程最大空闲时间 60 秒，当然允许的最大线程连接数还受制于操作系统的内核参数设置，设置多大要根据自己的需求与环境。当然线程可以配置在“tomcatThreadPool”中，也可以直接配置在“Connector”中，但不可以重复配置。 URIEncoding：指定 Tomcat 容器的 URL 编码格式，语言编码格式这块倒不如其它 WEB 服务器软件配置方便，需要分别指定。 connnectionTimeout： 网络连接超时，单位：毫秒，设置为 0 表示永不超时，这样设置有隐患的。通常可设置为 30000 毫秒，可根据检测实际情况，适当修改。 enableLookups： 是否反查域名，以返回远程主机的主机名，取值为：true 或 false，如果设置为false，则直接返回IP地址，为了提高处理能力，应设置为 false。 disableUploadTimeout：上传时是否使用超时机制。 connectionUploadTimeout：上传超时时间，毕竟文件上传可能需要消耗更多的时间，这个根据你自己的业务需要自己调，以使Servlet有较长的时间来完成它的执行，需要与上一个参数一起配合使用才会生效。 acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可传入连接请求的最大队列长度，超过这个数的请求将不予处理，默认为100个。 keepAliveTimeout：长连接最大保持时间（毫秒），表示在下次请求过来之前，Tomcat 保持该连接多久，默认是使用 connectionTimeout 时间，-1 为不限制超时。 maxKeepAliveRequests：表示在服务器关闭之前，该连接最大支持的请求数。超过该请求数的连接也将被关闭，1表示禁用，-1表示不限制个数，默认100个，一般设置在100~200之间。 compression：是否对响应的数据进行 GZIP 压缩，off：表示禁止压缩；on：表示允许压缩（文本将被压缩）、force：表示所有情况下都进行压缩，默认值为off，压缩数据后可以有效的减少页面的大小，一般可以减小1/3左右，节省带宽。 compressionMinSize：表示压缩响应的最小值，只有当响应报文大小大于这个值的时候才会对报文进行压缩，如果开启了压缩功能，默认值就是2048。 compressableMimeType：压缩类型，指定对哪些类型的文件进行数据压缩。 noCompressionUserAgents=”gozilla, traviata”： 对于gozilla, traviata浏览器，不启用压缩。 如果已经对代码进行了动静分离，静态页面和图片等数据就不需要 Tomcat 处理了，那么也就不需要配置在 Tomcat 中配置压缩了。以上是一些常用的配置参数属性，当然还有好多其它的参数设置，还可以继续深入的优化，HTTP Connector 与 AJP Connector 的参数属性值，可以参考官方文档的详细说明：Tomcat7:HTTP ConnectorTomcat7:AJP Connector 引用Tomcat 调优及 JVM 参数优化Java 8: 从永久代（PermGen）到元空间（Metaspace）《垃圾回收的算法与实现》——分代垃圾回收GC详解及Minor GC和Full GC触发条件总结GC是如何判断一个对象为”垃圾”的？被GC判断为”垃圾”的对象一定会被回收吗？RednaxelaFX关于GC的回答深入理解 Java G1 垃圾收集器Java垃圾回收机制引用计数算法标记-清除算法标记-压缩算法半区复制算法Java垃圾回收机制JVM性能调优监控工具jps、jstack、jmap、jhat、jstat使用详解JVM内存区域划分（JDK6/7/8中的变化）Java中的四种引用方式的区别Java 7之基础 - 强引用、弱引用、软引用、虚引用]]></content>
      <categories>
        <category>JVM</category>
        <category>资料集</category>
      </categories>
      <tags>
        <tag>Tomcat调优</tag>
        <tag>JVM调优</tag>
        <tag>垃圾回收</tag>
        <tag>资料集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka基础：概念与原理]]></title>
    <url>%2F2017%2F05%2F05%2FKafka%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[简单介绍Kafka里面的一些基础知识和原理实现。 基本概念 BrokerKafka集群包含一个或多个服务器，这种服务器被称为broker Topic每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处） PartitionParition是物理上的概念，每个Topic包含一个或多个Partition. Producer负责发布消息到Kafka broker Consumer消息消费者，向Kafka broker读取消息的客户端。 Consumer Group每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。]]></content>
      <categories>
        <category>分布式</category>
        <category>队列</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程与锁]]></title>
    <url>%2F2017%2F05%2F05%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E9%94%81%2F</url>
    <content type="text"><![CDATA[多线程与锁，介绍JVM内存模型，多线程以及线程同步（锁），还有线程池等知识。 内存模型内存模型概念模型类别 顺序一致性模型 要求对某处理机缩写的值立即进行传播，确保该值已被所有处理机接受后才能继续其他指令的执行。 释放一致性模型 允许将某处理机所写的值延迟到释放锁时进行传播。 内存模型功能 Describes execution trace of a program Describes possible behaviors of a program Determines what values can be read at every point in the program 内存模型特征 Visibility 可见性：多核、多线程间数据的共享 Ordering 有序性：对内存的操作应该是有序的 JVM内存模型理论基础 定义了Java线程和内存交互的规则 保证多线程程序结果的可预测，语义一致性 Heap Memory：用来在线程间共享内存 instance fields static fileds array elements 线程本地变量（局部变量，方法参数等）在堆栈中，不受JMM(Java Memory Model)影响 JMM如何体现可见性？ 在JMM中，通过并发线程修改变量值，必须将线程变量同步回竹村后，其他线程才能访问到。 JMM怎么体现有序性？ 通过Java提供的同步机制或者volatile关键字，来保证内存的访问顺序。 有序性、可见性 程序顺序 程序声明的顺序 执行顺序 JMM不保证线程对变量操作发生的顺序和被其他线程看到的是同样的顺序。JMM容许线程以写入变量时所不相同的次序把变量存入主存。 线程内部本身遵循程序顺序，从线程外看到的是执行顺序 编译器和处理器可能会为了性能优化，进行指令重排序 程序执行为了优化也可能重新排序 Happens-Before Memory Model 类似释放一致性模型 Partial odering(Happens-Before) 如果B能够看到A的动作产生的结果，我们说A Happens-Before B，IMM定义了一些这样的规则： Program order rule:Each action in a thread Happens-Before every action in that thread that comes later in the program order. Monitor lock rule:An unlock on a monitor lock Happens-Before every subsequent lock on that same monitor lock. Volatile variable rule:A write to a volatile field Happens-Before every subsequent read of that same filed. ……The rules for Happens-Before 带锁的线程和内存交互行为 获取对象监视器的锁(lock) 清空工作内存数据，从主内存复制变量到当前工作内存，即同步数据(read and load) 执行代码，改变共享变量的值(use and assign) 将工作内存数据刷回竹村(store and write) 释放对象监视器的锁(unlock) JMM相关术语 Thread working copy memory(工作内存) 在JVM规范中这是一个抽象的概念，对应的可能会是寄存器，CPU缓存，编译以及执行优化等。 一个新产生的Thread有一个空的working memory。 线程间无法相互直接访问，变量传递均需要通过主存来完成。 The main memory(主存) Java堆内存 Thread’s execution engine 保证线程的正确执行顺序 JVM定义了6个原子操作来读写工作内存和主内存的数据以及lock、unlock实现更大范围的原子性： lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中如果要把一个变量从主内存中复制到工作内存，就需要按顺寻地执行read和load操作，如果把变量从工作内存中同步回主内存中，就要按顺序地执行store和write操作。Java内存模型只要求上述操作必须按顺序执行，而没有保证必须是连续执行。也就是read和load之间，store和write之间是可以插入其他指令的，如对主内存中的变量a、b进行访问时，可能的顺序是read a，read b，load b， load a。Java内存模型还规定了在执行上述八种基本操作时，必须满足如下规则： 不允许read和load、store和write操作之一单独出现 不允许一个线程丢弃它的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中。 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中。 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。 一个变量在同一时刻只允许一条线程对其进行lock操作，lock和unlock必须成对出现 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值 如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）。 线程状态从线程创建到线程死亡的生命周期，一共6个状态。具体可参考java.lang.Thread.State. 新建状态NEW：新建线程，并且未调用start()方法之前。 就绪（可运行）状态RUNNABLE：NEW调用start(),RUNNING调用yield()后，进入就绪状态，等待CPU资源调度。 运行状态RUNNING(JDK没有定义该状态)：线程正在执行 阻塞状态BLOCKED:这个状态下, 是在多个线程有同步操作的场景, 比如正在等待另一个线程的synchronized 块的执行释放, 或者可重入的 synchronized块里别人调用wait() 方法, 也就是这里是线程在等待进入临界区。 无限等待状态WAITING:调用了Object.wait(),join(),LockSupport.park()并且没有设置timeout时间，进入无限等待状态。Object.wait()后只能被Object.notify(),Object.notifyAll()唤醒。join()则需要等待指定的线程进入TERMINATED状态。 限时等待TIMED_WAITING：调用Thread.sleep(long),Object.wait(long),Thread.join(long),LockSupport.parkNanos,LockSupport.parkUntil方法，指定了timeout时间，则进入限时等待状态。 死亡状态TERMINATED：线程结束，正常运行结束或者抛出异常。此老外的图描述有误：threadX.join(long)是进入TIMED_WAITING状态的 顺序性/可见性/原子性顺序性如果在本线程内观察，所有操作都是有序的；如果在一个线程中观察另一个线程，所有操作都是无序的。前半句是指“线程内表现为串行语义”，后半句是指“指令重排序”现象和“工作内存中主内存同步延迟”现象。 可见性可见性就是指当一个线程修改了线程共享变量的值，其它线程能够立即得知这个修改 原子性原子性是指在一个操作中就是cpu不可以在中途暂停然后再调度，既不被中断操作，要不执行完成，要不就不执行。 线程上下文切换什么是线程上下文切换？CPU给每个线程分配一定的CPU时间，这个时间片很短，一般是几十ms。CPU通过不停切换线程执行程序。CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再次加载这个任务的状态，从任务保存到再加载的过程就是一次上下文切换。 如何减少线程上下文切换？上下文切换又分为2种： 让步式上下文切换 执行线程主动释放CPU，与锁竞争严重程度成正比，可通过减少锁竞争来避免. 无锁并发编程/降低锁粒度：多线程竞争时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash取模分段，不同的线程处理不同段的数据。 CAS算法(or其他锁无关算法)：Java的Atomic包使用CAS算法来更新数据，而不需要加锁。java.util.concurrent下的并发集合大多使用了CAS。 抢占式上下文切换 后者是指线程因分配的时间片用尽而被迫放弃CPU或者被其他优先级更高的线程所抢占，一般由于线程数大于CPU可用核心数引起，可通过调整线程数，适当减少线程数来避免。 使用最少线程：避免创建不需要的线程，防止大量线程处于等待状态。 协程(coroutine)：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 JDK线程调度相关类FutureTask12345678910ExecutorService executor = Executors.newSingleThreadExecutor();Future future = executor.submit(new Callable() &#123; //Future是FutureTask的父接口，此处返回的是FutureTask @Override public String call() throws Exception &#123; System.out.println("call back when the thread ends!"); return "call back result"; &#125;&#125;);System.out.println(future.get()); //future.get()方法是阻塞的executor.shutdown(); CyclicBarrier/CountDownLatch/join/Phaser这几个类主要用于线程之间协同调度。 CyclicBarrier1234CyclicBarrier cb = new CyclicBarrier(int parties);CyclicBarrier cb = new CyclicBarrier(int parties,Runnable barrierAction);cb.await() //parties计数-1cb.await(long timeout, TimeUnit unit) //等待timeout时间直至触发Barrier。如果超时则自行执行下去。 参数解析： parties: parties意思是必须有“parties”个线程执行了await方法，所有线程才会执行await之后的代码。每await一次，count计数-1。循环迭代的时候count计数会被重置为parties。 barrierAction: the command to execute when the barrier is tripped.唤醒所有线程前执行的屏障动作。过程和代码如图：12345678910111213141516171819202122232425262728293031323334353637383940414243444546final CyclicBarrier cyclicBarrier = new CyclicBarrier(3);Thread a = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("aStart"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println("aEnd"); &#125;&#125;);Thread b = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("bStart"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println("bEnd"); &#125;&#125;);Thread c = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("cStart"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println("cEnd"); &#125;&#125;);a.start();b.start();c.start(); 注意：如果在屏障等待的某个线程抛出InterruptedException，则其他所有线程都会抛出BrokenBarrierException。此时Cyclicbarrier处于Broken状态（被损坏）。cb.isBroken()能知道CyclicBarrier是否已被损坏。cb.reset()可以重置CyclicBarrier状态。 CountDownLatch可用于控制线程的执行顺序。1234CountDownLatch cdh = CountDownLatch(int count)cdl.countDown() //计数器-1，但**不会阻塞线程**，这区别于await()方法cdh.await() //一直等到count计数为0，除非被其他线程打断。cdh.await(long timeout, TimeUnit unit) //等待timeout时间直至count计数为0则返回true；如果超时则返回false。 参数解析： count:count表示必须有“count”个线程执行了cdl.countDown()方法，使得计数器为0，主线程往下执行。CountDownLatch类似于join，但需要知道线程数量（用于倒计数）：123456789101112131415161718192021222324252627final CountDownLatch countDownLatch = new CountDownLatch(3);Thread a = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("aEnd"); countDownLatch.countDown(); &#125;&#125;);Thread b = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("bEnd"); countDownLatch.countDown(); &#125;&#125;);Thread c = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("cEnd"); countDownLatch.countDown(); &#125;&#125;);a.start();b.start();c.start();countDownLatch.await(); //此处阻塞，等待countDown为0System.out.println("Main Thread goes on!"); join1thread.join(long millis) //将thread join进主线程，并且主线程只等待millis毫秒。超时后抛出IllegalArgumentException异常。 类似与上面countDownLatch，但是不需要知道线程数量，也不需要计数器。12345678910111213141516171819202122232425Thread a = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("aEnd"); &#125;&#125;);Thread b = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("bEnd"); &#125;&#125;);Thread c = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("cEnd"); &#125;&#125;);a.start();b.start();c.start();a.join();b.join();c.join();System.out.println("Main Thread goes on!"); CountDownLatch和join还有一个明显的区别是：join进主线程的子线程必须执行完毕，主线程才能继续执行。而CountDownLatch则可以灵活地通过调用countDown()方法，让计数器为0边执行主线程，且countDown()方法无阻塞，子线程也可以继续执行。 Semaphore(信号量)123456789101112Semaphore(int permits表示许可数目，即同时允许) //permits表示许可数目，即同时允许permits个线程获取许可进行并发运行。Semaphore(int permits, boolean fair) //fair表示是否公平，true表示公平竞争，即等待时间越久的线程越先获得许可。semaphore.acquire(); //获取许可semaphore.release(); //释放许可semaphore.acquire(int permits); //获取permits个许可semaphore.release(int permits); //释放permits个许可semaphore.acquireUninterruptibly(int permits);//类似acquire()，但是被打断时，线程会继续申请许可，当申请到许可后，会标记线程状态为中断。semaphore.acquireUninterruptibly();//semaphore.tryAcquire();//尝试获取许可，非阻塞semaphore.tryAcquire(int permits);//尝试获取permits个许可，非阻塞semaphore.tryAcquire(long timeout, TimeUnit unit);//timeout时间内获取1个许可semaphore.tryAcquire(int permits, long timeout, TimeUnit unit);//timeout时间内获取permits个许可 Semaphore比较简单，看代码一目了然，主要用于控制线程并发数量。12345678910111213141516171819202122232425262728293031323334353637383940414243final Semaphore semaphore = new Semaphore(2);Thread a = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); System.out.println("aEnd"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; semaphore.release(); &#125; &#125;&#125;);Thread b = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); System.out.println("bEnd"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; semaphore.release(); &#125; &#125;&#125;);Thread c = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; semaphore.acquire(); System.out.println("cEnd"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; semaphore.release(); &#125; &#125;&#125;);a.start();b.start();c.start(); PhaserSleep/waitThread.sleep(long millis)睡眠，让出CPU时间，但是不释放锁。millis时间后自动执行Object.wait(long timeout)挂起，让出CPU时间，并且释放锁。timeout时间后抛出异常；timeout设置为0永不超时，需要其他线程使用notify/notifyAll唤醒。 线程池JDK线程池Executors.newCachedThreadPool适用于大量短生命周期的线程。 可配置参数corePoolSize： 0 线程池里面保持的线程，即使是空闲线程。通过参数allowCoreThreadTimeOut控制过期时间。maximumPoolSize：Integer.MAX_VALUE 线程池最大的线程数量限制keepAliveTime：60 如果线程池线程数大于corePoolSize，超过keepAliveTime的空闲线程将会被释放unit： TimeUnit.SECONDS keepAliveTime的时间单位workQueue： [同步队列]new SynchronousQueue() 保存未执行的任务。任务：仅限于被方法execute提交的Runnable Task。Executors.newFixedThreadPool线程数固定的，且线程关闭只能是因为异常或者调用了shutdown方法。有线程被关闭时，被阻塞的线程会加入线程池。 可配置参数 corePoolSize：自定义，线程数 maximumPoolSize：同corePoolSize keepAliveTime：0 unit：TimeUnit.MILLISECONDS workQueue ：new LinkedBlockingQueue() Executors.newSingleThreadExecutor 特殊的newFixedThreadPool ，因为线程数是1 可配置参数corePoolSize：1maximumPoolSize：1keepAliveTime：0unit：TimeUnit.MILLISECONDSworkQueue ：new LinkedBlockingQueue() Executors.newScheduledThreadPoolExecutors.newSingleThreadScheduledExecutorSpring线程池(TaskExecutor实现类)org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor它不支持任何对java.util.concurrent包的替换或者下行移植。Doug Lea和Dawid Kurzyniec对java.util.concurrent的实现都采用了不同的包结构，导致它们无法正确运行。 这个实现只能在Java 5环境中使用，但是却是这个环境中最常用的。它暴露的bean properties可以用来配置一个java.util.concurrent.ThreadPoolExecutor，把它包装到一个TaskExecutor中。如果你需要更加先进的类，比如ScheduledThreadPoolExecutor,我们建议你使用ConcurrentTaskExecutor来替代。 可配置参数 corePoolSize：核心线程数，线程池维护线程的最少数量。默认1，运行时可修改，比如通过JMX。 keepAliveSeconds：线程池维护线程所允许的空闲时间。默认60 maxPoolSize：线程池(ThreadPoolExecutor)维护线程的最大数量 queueCapacity：线程池所使用的缓冲队列BlockingQueue大小，默认是Integer.MAX_VALUE。配置为正数，使用LinkedBlockingQueue；负数则使用SynchronousQueue allowCoreThreadTimeOut：Specify whether to allow core threads to time out. This enables dynamic growing and shrinking even in combination with a non-zero queue (since the max pool size will only grow once the queue is full).默认false。 rejectedExecutionHandler：拒绝多线程任务的策略 execute(Runnable)方法 执行过程 如果此时线程池中的数量小于corePoolSize，即使线程池中的线程都处于空闲状态，也要创建新的线程来处理被添加的任务。 如果此时线程池中的数量等于corePoolSize，但是缓冲队列workQueue未满，那么任务被放入缓冲队列。 如果此时线程池中的数量大于corePoolSize，缓冲队列workQueue满，并且线程池中的数量小于maximumPoolSize，建新的线程来处理被添加的任务。 如果此时线程池中的数量大于corePoolSize，缓冲队列workQueue满，并且线程池中的数量等于maximumPoolSize，那么通过 handler所指定的策略来处理此任务。也就是：处理任务的优先级为：核心线程corePoolSize、任务队列workQueue、最大线程 maximumPoolSize，如果三者都满了，使用handler处理被拒绝的任务。 当线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止。这样，线程池可以动态的调整池中的线程数。 Reject策略预定义 ThreadPoolExecutor.AbortPolicy策略，是默认的策略,处理程序遭到拒绝将抛出运行时 RejectedExecutionException。 ThreadPoolExecutor.CallerRunsPolicy策略 ,调用者的线程会执行该任务,如果执行器已关闭,则丢弃. ThreadPoolExecutor.DiscardPolicy策略，不能执行的任务将被丢弃. ThreadPoolExecutor.DiscardOldestPolicy策略，如果执行程序尚未关闭，则位于工作队列头部的任务将被删除，然后重试执行程序（如果再次失败，则重复此过程）. org.springframework.core.task.SimpleAsyncTaskExecutor这个实现不重用任何线程，或者说它每次调用都启动一个新线程。但是，它还是支持对并发总数设限，当超过线程并发总数限制时，阻塞新的调用，直到有位置被释放。本质上不能算是一个池。 org.springframework.core.task.SyncTaskExecutor这个实现不会异步执行。相反，每次调用都在发起调用的线程中执行。它的主要用处是在不需要多线程的时候，比如简单的test case。 org.springframework.scheduling.concurrent.ConcurrentTaskExecutor这个实现是对Java 5 java.util.concurrent.Executor类的包装。有另一个备选, ThreadPoolTaskExecutor类，它暴露了Executor的配置参数作为bean属性。很少需要使用ConcurrentTaskExecutor, 但是如果ThreadPoolTaskExecutor不敷所需，ConcurrentTaskExecutor是另外一个备选。 SimpleThreadPoolTaskExecutor这个实现实际上是Quartz的SimpleThreadPool类的子类，它会监听Spring的生命周期回调。当你有线程池，需要在Quartz和非Quartz组件中共用时，这是它的典型用处。 TimerTaskExecutor这个实现使用一个TimerTask作为其背后的实现。它和SyncTaskExecutor的不同在于，方法调用是在一个独立的线程中进行的，虽然在那个线程中是同步的。 WorkManagerTaskExecutor这个实现使用了CommonJ WorkManager作为其底层实现，是在Spring context中配置CommonJ WorkManager应用的最重要的类。和SimpleThreadPoolTaskExecutor类似，这个类实现了WorkManager接口，因此可以直接作为WorkManager使用。 线程池优化线程池优化关键点 尽量减少线程切换和管理的开支 要求线程数尽量少，这样可以减少线程切换和管理的开支 最大化利用CPU 要求尽量多的线程，以保证CPU资源最大化的利用。 线程池优化实践 高并发，但执行耗时短的任务 耗时短的任务应线程尽量少，降低线程上下文切换。 低并发，但执行耗时长的任务 耗时长的任务应分为cpu密集型任务或者IO密集型任务 cpu密集型任务：减少并发线程数，减少cpu上下文切换带来的影响。 IO密集型任务:增加并发线程数，让CPU去切换其他线程充分忙起来 高并发，且任务耗时长的任务 锁死锁与活锁 死锁 互斥条件：一个资源每次只能被一个线程使用 请求与保持条件：一个线程因请求资源而阻塞时，保持已获得的资源不释放 不剥夺条件：线程获得的资源不能被强行剥夺 循环等待条件：若干线程(&gt;=2)形成一种头尾相接的循环的循环等待资源关系。 活锁活锁指的是任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，所谓的“活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。 阻塞锁阻塞时不占用CPU时间，通过notify等方法唤醒。线程竞争锁时需要切换线程上下文。 自旋锁线程循环竞争锁，线程状态无须改变，所以少量线程竞争时响应快。但是如果存在大量线程自旋，则会占用大量的CPU资源。 公平自旋锁：所有线程并发自旋。 非公平自旋锁：线程入队列排队，顺序出队竞争自旋锁。 阻塞锁和自旋锁相当于APP通知的“推”和“拉”。 乐观锁每次去读取数据都会乐观地认为其他线程不会修改数据，所以读时不上锁，只有在修改数据时会判断此期间是否有线程修改了该数据。这里可以用版本号等机制实现。如果写冲突可以返回错误给用户处理。乐观锁适用于多读数据，提高吞吐量。举例：MySQL的版本号机制乐观锁（类似于SVN） 悲观锁每次去读取数据都会悲观地认为其他线程会修改数据，所以每次读数据都会上锁。传统的RMDB默认是悲观锁：行锁，表锁，读锁，写锁等。 可重入锁（递归锁） Java的实现是ReentrantLock和synchronized 可重入锁可避免同一线程的死锁。 可重入锁同一线程（一般在递归函数），可多次获得锁。锁内部都有1个状态计数器，每次重入状态计数+1；退出则-1。 锁无关算法(CAS)CAS概述CAS即Compare And Swap，是一种锁无关算法。具体例子可以看java.util.concurrent下的并发集合以及原子类。 wait-free 等待无关 lock-free 锁无关：没有线程阻塞的情况下实现变量同步。 lock-based 基于锁 CAS必须是一个原子操作，用伪代码来表示：12345678template&lt;class T&gt;bool CAS(T* addr,T expected,T value)&#123; if(*addr==expected)&#123; *addr = value; return true; &#125; return false;&#125; CAS优点 CAS突出的是“无阻塞”，避免死锁，但是可能会造成活锁。 阻塞/非阻塞：强调的是资源问题，阻塞当前线程会挂起，竞争锁还会造成线程的上下文切换。非阻塞表示不影响当前线程的执行，并且没有线程的上下文切换，即使正在自旋CAS的线程挂了，也不会阻塞其它线程自旋CAS。 同步/异步：强调的是等待时间，同步表示必须等候方法处理完成并且返回；异步则是立刻返回结果，无须等待。 CAS缺点 ABA问题：因为CAS需要在操作前检查下值有没有发生变化，如果没有则更新。但是如果一个值开始的时候是A，变成了B，又变成了A，那么使用CAS进行检查的时候会发现它的值没有发生变化，但是事实却不是如此。ABA问题的解决思路是使用版本号(乐观锁)，如A-B-A变成1A-2B-3A 循环时间长开销大：自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。比如CAS实现的并发队列在队列为空时出队，结果队列长时间为空，则这段时间每个线程都在CAS自旋。 只能保证一个共享变量的原子操作：对一个共享变成可以使用CAS进行原子操作，但是多个共享变量的原子操作就无法使用CAS，这个时候只能使用锁。 以JDK的并发类举个例子（基于JDK 7） 原子操作的++i代码：1234567891011121314151617//利用AtomicInteger实现++iAtomicInteger atomicInteger = new AtomicInteger(0);atomicInteger.incrementAndGet();//++i内部实现public final int getAndIncrement() &#123; for (;;) &#123; //for循环CAS自旋 int current = get(); //获取当前值 int next = current + 1; //当前值+1，next为CAS应该Set进去的值 if (compareAndSet(current, next)) //CAS操作：如果当前值等于期望值，则设置当前值为next，返回当前值。否则循环获取当前值做CAS操作。 return current; &#125;&#125;//CAS内部实现public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);//原子CAS，操作系统orCPU指令直接支持&#125; ConcurrentLinkedQueue的出队和入队12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455//入队public boolean offer(E e) &#123; checkNotNull(e); final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); for (Node&lt;E&gt; t = tail, p = t;;) &#123;//t,p都是tail节点 Node&lt;E&gt; q = p.next;//获取tail节点的next q if (q == null) &#123;//单线程下tail节点的next肯定是null多线程需要判断。 // p is last node 如果为null，证明此时此刻q的确是最后一个节点（null节点） if (p.casNext(null, newNode)) &#123; //通过CAS（这里是compareAndSwapObject，不是Set）去设置null为newNode（申请入队元素）。如果失败，证明其他线程先一步入队了 // Successful CAS is the linearization point // for e to become an element of this queue, // and for newNode to become "live". if (p != t) // hop two nodes at a time 如果此时tail节点不相等了，则更新tail节点 casTail(t, newNode); // Failure is OK. compareAndSwapObject失败也无所谓，证明有其他线程在之前也casNext成功，并更新了tail节点。 return true; &#125; // Lost CAS race to another thread; re-read next &#125; else if (p == q) //p指向tail或者head // We have fallen off list. If tail is unchanged, it // will also be off-list, in which case we need to // jump to head, from which all live nodes are always // reachable. Else the new tail is a better bet. p = (t != (t = tail)) ? t : head; //TODO:这一陀表达式再研究 else //p指向tail或者q(tail.next) // Check for tail updates after two hops. p = (p != t &amp;&amp; t != (t = tail)) ? t : q;//TODO:这一陀表达式再研究 &#125;&#125;//出队public E poll() &#123; restartFromHead: for (;;) &#123; for (Node&lt;E&gt; h = head, p = h, q;;) &#123; E item = p.item; if (item != null &amp;&amp; p.casItem(item, null)) &#123; //compareAndSwapObject如果队列不为空且CAS出队首元素，并将队首设置为null。如果不成功，证明队首元素被其他线程出队了。 // Successful CAS is the linearization point // for item to be removed from this queue. if (p != h) // hop two nodes at a time updateHead(h, ((q = p.next) != null) ? q : p); return item; &#125; else if ((q = p.next) == null) &#123; updateHead(h, p); return null; &#125; else if (p == q) continue restartFromHead; else p = q; &#125; &#125;&#125; 分布式锁 层次锁：具体参看本人blog：分布式令牌桶设计实现（流控） synchronized作用 Low level locking 每个对象都有一个相关的lock对象(监视器) Java语言没有提供分离的lock和unlock操作，但是在JVM提供了2个单独的指令monitorenter和monitorext来实现 特性 Atomicity(原子性):Locking to obtain mutual exclusion Visibility(可见性):Ensuring that changes to object fields made in one thread are seen in other threads(memory) Ordering(顺序性):Ensuring that you aren’t surprised by the order in which statements are executed Blocking:Cant’t interruptsynchronized原理synchronized的实现依赖于lock-free队列，基本思想是先自旋后阻塞。先自旋可以减少线程的上下文切换，获得更改的吞吐量，但代价是造成不公平锁。JVM中，锁有个名字叫“对象监视器”。 Contention List：所有请求锁的线程将被首先放置到该竞争队列 Entry List：Contention List中那些有资格成为候选人的线程被移到Entry List Wait Set：那些调用wait方法被阻塞的线程被放置到Wait Set OnDeck：任何时刻最多只能有一个线程正在竞争锁，该线程称为OnDeck Owner：获得锁的线程称为Owner !Owner：释放锁的线程在1会先自旋，无法获得锁才会进入ContentionList。ContentionList与EntryList同属等待队列。但是EntryList优先级较高。 EntryList优先级高，WaitSet的线程被notify唤醒会直接进入EntryList。 ContentionList是LIFO队列，EntryLis的存在减轻了队首竞争。（疑问？） synchronized内存模型语义分析 synchronized内存操作 通过对象引用找到同步对象，然后获取对象上的监视器锁 当线程进入synchronized块之后： a.清洗thread’s working memory b.变量复制：对块内的变量执行assign原子操作 c.变量复制：对use变量执行read-&gt;load原子操作 当线程退出synchronized块之前，对它在iworkingmemory中所有的assigned values执行store-&gt;write原子操作，写回mian memory synchronized不足与发展 不能跨越多个对象 当在等待锁对象的时候不能中途放弃，知道成功 等待没有超时限制 不能中断阻塞 JDK5 提供了更灵活的锁禁止：Lock和Condition synchronized优化技术 锁省略：锁对象的引用时，线程本地对象（线程的堆栈内的对象）。意思就是操作线程内局部变量不需要加锁。 锁粗化：锁粗化就是把使用同一锁对象的相邻同步块合并的过程（减少线程上下文的切换） 自适应锁优化技术：实现阻塞有2种技术，即让操作系统暂挂线程，直到线程被唤醒，或使用旋转（spin）锁。（指的分别是阻塞锁和自旋锁） Hotspot可以对持有时间短的锁使用自旋锁，而对持有时间长的锁使用阻塞锁 ConditionCondition是一个接口，位于java.util.concurrent.locks.Condition123456789ReentrantLock reentrantLock = new ReentrantLock(true);Condition condition = reentrantLock.newCondition();condition.await(); //阻塞释放CPU资源和对象锁condition.awaitUninterruptibly();//无视中断阻塞，但中断后进入此方法，线程状态会被设置为`interrupted status`condition.awaitNanos(long nanosTimeout);//阻塞，返回`超时时间-等待时间`。超时时间为nanosTimeout，超时后抛出InterruptedException。condition.await(long time, TimeUnit unit);//同上，区别是时间单位condition.awaitUntil(Date deadline);//阻塞到某时刻condition.signal();//唤醒被该Condition对象阻塞的线程中的1个condition.signalAll();//唤醒所有被该Condition对象阻塞的线程 ReentrantLocksynchronized与ReentrantLock都是可重入锁，Synchronized在编译期，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。（进入和退出对象监视器，分别代表lock和unclock）。synchronized是非公平锁，在无法获取到锁的时候，会先进行自旋才会进入竞争锁队列。且synchronized不可中断（没有相关API）ReentrantLock则可提供公平锁和非公平锁的选择，且可被中断（指持有锁的线程不释放锁时，正在等待的线程可以选择放弃等待），灵活控制锁的使用，减少死锁。synchronized的作用是某个代码块，单个类，单个对象；而ReentrantLock则可同时锁定多个Condition对象。 ReentrantReadWriteLockReentrantReadWriteLock 读锁之间不互斥，读写锁互斥，写锁之间互斥。用于读多少写的并发资源访问情况。 CopyOnWrite类集合：读不加锁，写时复制。让读性能更高。优于ReentrantReadWriteLock，但是消耗更多资源。 volatilevolatile变量的内存模型分析 旧的内存模型 保证读写volatile都直接发生在main memory中，线程的working memory不进行缓存（保证了可见性，但仍参与指令重排） 新的内存模型 如果当线程A写入volatile变量V而线程B读取V时，那么在写入V时，A可见的所有变量值现在都可以保证对B是可见的。结果就是作用更大的volatile语义，代价是访问volatile字段时会对性能产生一点点的影响。(A volatile var write Happens-Before read of the var)(保证了可见性和有序性) 注意volatile变量不能保证原子性 使用java.util.concurrent.atomic部分情景可以取代volatile，atomic的底层实现是CAS，而且保证了运算原子性。 引用Java多线程中提到的原子性和可见性、有序性Java内存模型RE：关于JMM模型中工作内存、主内存和几个操作的理解java 线程的几种状态java内存模型与并发技术-阿里yangjs多线程上下文切换自己对多线程的一点思考SPRING中的线程池ThreadPoolTaskExecutorSpring中线程池的应用Java多线程系列–“JUC集合”10之 ConcurrentLinkedQueuejava多线程-概念&amp;创建启动&amp;中断&amp;守护线程&amp;优先级&amp;线程状态(多线程编程之一)Java多线程编程：Callable、Future和FutureTask浅析（多线程编程之四）JVM底层又是如何实现synchronized的]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>资料集</tag>
        <tag>锁</tag>
        <tag>多线程</tag>
        <tag>线程池</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase基础：概念与原理]]></title>
    <url>%2F2017%2F05%2F04%2FHBase%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[简单介绍HBase里面的一些基础知识和原理实现。 HBase特点HBase是一个分布式、列式存储、稀疏（HBase的列值为null可以不占用空间）的key-value数据库。 线性、模块化的可扩展性 读写强一致性（HBase是CP系统） 表可自动或手动配置分片 RegionServers之间的自动故障切换支持 HBase Tables可以很好地支持MR作业 Java API Client简单易用 Block cache 和 Bloom Filters用于实时查询 Thrift网关和支持XML，Protobuf和二进制数据编码的RESTful Webservice 可扩展的 jruby-based (JIRB) shell Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX HBase Shell比起HBase shell我个人更加愿意用Web写GUI来替代Shell。 基本概念 rowkey：rowkey便是HBase Table的索引。 散列：常用时间倒序，加盐散列等 预分区：相当于手工设定负载均衡 rowkey一般根据业务设定，唯一性，字典ASCII排序 Column Family：存储在HDFS上的一个单独文件中，列的空值不会被保存。 Column Version Number：类型为Long，默认值是系统时间戳，可由用户自定义 Value(Cell)：Byte array Region：Region是RegionServers负载均衡的最小单元，表按行分片得到Region。Region可自动分裂，也可以预分区。 Table 在一个HBase Table中： 表是行的集合 行是列族的集合 列族是列的集合 列是键值对的集合下图可以是一个稀疏矩阵，且为null的列不会占用空间。注意：HBase的列不属于建表的元数据，这才使得列为null可以不占用存储空间，但代价是每个Cell都需要包含列的具体信息，还有版本号之类的其他信息。 HBase架构HBase架构图HBase是Master-Slave集群架构。HBase由HMaster节点、HRegionServer节点、ZooKeeper集群节点组成，而底层存储则基于HDFS。HBase Client通过RPC方式和HMaster、HRegionServer通信；一个HRegionServer可以存放X个HRegion；底层Table数据存储于HDFS中，而HRegion所处理的数据尽量和数据所在的DataNode在一起，实现数据的本地化；数据本地化并不是总能实现，比如在HRegion移动(如因Split)时，需要等下一次Compact才能继续回到本地化。 HMaster 管理HRegionServer，实现其负载均衡。 管理和分配HRegion，比如在HRegion split时分配新的HRegion；在HRegionServer退出时迁移其内的HRegion到其他HRegionServer上。 实现DDL操作（Data Definition Language，namespace和table的增删改，column familiy的增删改等）。 管理namespace和table的元数据（实际存储在HDFS上）。 权限控制（ACL）。 HRegionServer 存放和管理本地HRegion。 读写HDFS，管理Table中的数据。 Client直接通过HRegionServer读写数据（从HMaster中获取元数据，找到RowKey所在的HRegion/HRegionServer后）。 ZooKeeper集群是协调系统 存放整个 HBase集群的元数据以及集群的状态信息。 实现HMaster主从节点的failover。(主从故障自动切换) HRegionServer(RegionServer)HRegion(即Region)HBase使用RowKey将表水平切割成多个HRegion，从HMaster的角度，每个HRegion都纪录了它的StartKey和EndKey（第一个HRegion的StartKey为空，最后一个HRegion的EndKey为空），由于RowKey是排序的，因而Client可以通过HMaster快速的定位每个RowKey在哪个HRegion中（BloomFilter）。HRegion由HMaster分配到相应的HRegionServer中，然后由HRegionServer负责HRegion的启动和管理，和Client的通信，负责数据的读(使用HDFS)。每个HRegionServer可以同时管理1000个左右的HRegion（这个数字怎么来的？没有从代码中看到限制，难道是出于经验？超过1000个会引起性能问题？来回答这个问题：感觉这个1000的数字是从BigTable的论文中来的（5 Implementation节）：Each tablet server manages a set of tablets(typically we have somewhere between ten to a thousand tablets per tablet server)）。 RegionServers负载RegionHBase的Table按行分片（分区）为Region，每个Region属于一个RegionServers。每个RegionServers管理着多个Region。RegionServer是HBase的数据服务进程。负责处理用户数据的读写请求。由这个图就可以了解到rowkey为何要散列，因为散列后rowkey落在不同的Region，才能在不同的RegionServer负载均衡。并且Region可以在RegionServer之间发生转移。再看此图：Table被水平（按行）切分为Region，其依据便是Rowkey。属于一种RangePartitioner。每个Region都有一个StartRowkey和EndRowkey。 元数据Region(Meta Region)Meta Region记录了每一个UserRegion的路由信息。Client读写Region数据的路由： 找寻Meta Region地址。 再由Meta Region找寻UserRegion地址。 用户Region(User Region) HMasterHMaster没有单点故障问题，可以启动多个HMaster，通过ZooKeeper的Master Election机制保证同时只有一个HMaster出于Active状态，其他的HMaster则处于热备份状态。一般情况下会启动两个HMaster，非Active的HMaster会定期的和Active HMaster通信以获取其最新状态，从而保证它是实时更新的，因而如果启动了多个HMaster反而增加了Active HMaster的负担。前文已经介绍过了HMaster的主要用于HRegion的分配和管理，DDL(Data Definition Language，既Table的新建、删除、修改等)的实现等，既它主要有两方面的职责： 协调HRegionServer 启动时HRegion的分配，以及负载均衡和修复时HRegion的重新分配。 监控集群中所有HRegionServer的状态(通过Heartbeat和监听ZooKeeper中的状态)。 Admin职能 创建、删除、修改Table的定义。 ZooKeeper：协调者ZooKeeper为HBase集群提供协调服务，它管理着HMaster和HRegionServer的状态(available/alive等)，并且会在它们宕机时通知给HMaster，从而HMaster可以实现HMaster之间的failover，或对宕机的HRegionServer中的HRegion集合的修复(将它们分配给其他的HRegionServer)。ZooKeeper集群本身使用分布式一致性协议(PAXOS协议)保证每个节点状态的一致性。 How The Components Work TogetherRegionServers和Active HMaster通过Zookeeper的session相互联系。Zookeeper通过心跳维持临时ZNode(Ephemeral Node)的活动session。每个RegionServer都会创建一个临时ZNode，HMaster监视这些ZNodes来发现可用的RegionServer，同时也监控这些ZNode来判断RegionServer的健康状态。HMasters也会竞争创建Ephemeral Node。Zookeeper选择第一个创建的HMaster作为Active HMaster。Active HMaster会发送心跳给Zookeeper，而Inactive HMaster(非活动HMaster)会随时监听Active HMaster的故障。 如果Active HMaster或者Region Server无法发送心跳，则Zookeeper维护的session就会过期，相应的Ephemeral Node也会被删除。Znode被删除会通知订阅者Active HMaster/Inactive HMaster。Region Server故障：Active HMaster会尝试去恢复Region Server；Active HMaster故障：Inactive HMaster会尝试成为Active HMaster。 Minor Compaction/Major CompactionHBase第一次读/写流程HBase有一个特别的元数据表.META.。META Table存储了HBase集群的所有Regions的位置。而Zookeeper则保存了META Table的位置。所以HBase第一次读流程如下： Client通过Zookeeper获取META Table的Region Servers Client查询META Table，根据rowkey定位到相应的Region Server Client从Region Server读取到/写入数据而对于之后的读操作，Client会通过缓存定位META Table，和先前缓存的rowkey。随着时间推移，Client不需要再查询META Table，除非由于Region移动导致缓存无法命中。此时便会重新查询并更新缓存。 HBase META Table(.META.) META Table是一个HBase的表，存储了HBase集群的Regions元数据 META Table类似与B-Tree（B-Tree又叫平衡多路查找树，常用于数据库索引） META Table结构如下： Key:region start key，region id Values:RegionServer Region Server的组件RegionServer运行在HDFS的DataNode上。包含以下组件： WAL：Write Ahead Log(HLog)也是分布式文件系统的一个文件，每个RegionServer共享一个HLog。类似与MySQL的binlog，用于存储HBase的数据变更操作记录。用于HBase故障时的数据恢复。 BlockCache：读缓存。HBase在内存中存储了频繁读取的数据，缓存淘汰算法是 Least Recently Used,LRU. MemStore：写缓存，用于缓存那些写入了WAL但是还没写入磁盘的数据。 HBase Table横向切分为Region，纵向切分为列族。HBase是列式数据库，所以列族是已一个HStore存储在HDFS上。还有一点，假设列族存在数据倾斜，每个列族包含的数据量差异巨大。当Region根据rowkey横向分裂，造成列族数据不均匀分布。这就要求我们在设计HBase的表时，具有相同IO特性的列应该归于同一个列族，避免由于跨列族访问而导致的跨物理存储访问数据。HStore包含1个MemStore和多个StoreFile(HFile)。HBase每个列族的每个Region的每个Store包含一个MemStore和多个StoreFile(HFile)，详细可看架构图。具体我总结出一个HBase Table的物理存储和抽象映射图，具体如下： HFiles：基于HDFS，在磁盘上存储了有序的KeyValues。 HBase MemStore HBase读写流程图中读流程，缺少BlockCache的描述。 rowkey检索流程 访问Zookeeper定位META Table的Region位置 访问META Table定位rowkey对应的Region位置 PS:之后会通过缓存提高定位效率。写流程 检索rowkey Client的Put请求，首先写WAL： Edits会被追加在WAL的末尾（写入磁盘） 当HBase故障时，WAL用于恢复未持久化的数据 Client写入WAL后，会写入MemStore，之后直接告诉客户端写入成功（这样保证了高效的写性能） 读流程 检索rowkey 读BlockCache 读MemStore 读HFile PS：在任意地方(BlockCache,MemStore,HFile)读到都返回结果，这里通过ReadPoint、WriteNumber实现读写一致性的事务。 HBase Region FlushMemStore存储达到阈值，那么位于MemStore的有序数据集就会写入一个新的HFile。这个过程称之为Flush。每个列族会有多个HFiles，HFile基于HDFS，存放着HBase的实际数据。这里引出一个问题：HBase的列族数量为什么不应过多？MapR解析：There is one MemStore per CF; (和上面的There is one MemStore per column family per region.岂不是矛盾，或者表达的是one of the MemStore of CF？)when one is full, they all flush. It also saves the last written sequence number so the system knows what was persisted so far.意思就是1个MemStore的flush会触发其所在Region内（等价于所在Table）的所有MemStore的flush，所以列族越多，就会有越多的flush，频繁的IO便会影响性能。flush过程包括:(其实和HDFS的NameNode的edit log的flush(checkpoint)流程是非常相似的，就是swap的思路) 触发时机：某个Region内的一个MemStore达到阈值，触发整个Region内的所有MemStore的flush操作。 prepare(基于MemStore做snapshot) 遍历Region的所有MemStore，将MemStore的数据保存为snapshot，然后新建一个MemStore.NEW，新的写入操作会将数据写入MemStore.NEW。flush时，读请求会先从MemStore和MemStore.NEW读取操作，缓存位命中，才会去访问HFile。就在生成快照的时候，会上updateLock，阻塞写请求。 flushcache(基于snapshot生成临时文件) 遍历所有snapshot，将snapshot持久化为临时文件，目录是.tmp。这里涉及到磁盘IO，耗时操作。 commit(确认flush操作完成，rename临时文件为正式文件名称，清除mem中的snapshot)遍历所有的Memstore，将flush阶段生成的临时文件移到指定的ColumnFamily目录下，针对HFile生成对应的storefile和Reader，把storefile添加到HStore的storefiles列表中，最后再清空prepare阶段生成的snapshot。 HBase的ACIDACID是指原子性(Atomicity)，一致性(Consistency)，隔离性(Isolation)和持久性(Durability) 原子性(Atomicity)：整个事务中的所有操作，要么全部完成，要么全部不完成，不可能存在中间状态。 一致性(Consistency)：事务提交后，所有读操作都可以读到事务提交后的一致的数据。 隔离性(Isolation)：并发事务之间相互隔离，互不影响。 持久性(Durability)：在事务提交后，该事务对数据库所作的更改便持久的保存在存储介质之中，并且不可变。在HBase里面，提供有限度的ACID特性： 原子性：行级锁，保证行更改时的原子行，对某一行的更改操作要么全部成功，要么全部失败。 一致性：1行数据存在于Region，而Region属于1个RegionServer。该行数据更新后，所有读操作都会定位到此Region读取唯一的最新的数据。所以数据一致。 隔离性：行与行的事务互不影响，同行事务由于“写行”操作的原子性，那么并发写必然是串行的，也必然是隔离的。 持久性：事务一旦提交，WAL数据便认为不可破坏。即使宕机导致MemStore数据丢失，从WAL也能恢复数据。 HBase写时流程： 锁行，相同行无法并发写操作 获取WriteNumber（相当于事务版本号） 写WAL 写MemStore 更新ReadPoint为最新的WriteNumber 释放锁 HBase读时流程： 打开Scanner 获取最新ReadPoint 顺序读取BlockCache、MemStore、HFile，直至取得数据 关闭Scanner HBase的锁[TODO:存疑]行锁RowLock用于实现“写行”时的原子性。RowLock有Lease租约的机制，超时会自动释放行锁。（即使是超时释放行锁，整个操作也是要么全部成功，要么全部失败，保持原子性） MVCC:Multi-Version Concurrency Control 多版本并发控制参考HBase读写流程的WriteNumber和ReadPoint，读写版本分离。 MemStore锁对Store的写操作会调用Memstore的相关操作，在对memstore做snapshot以及清除snapshot的时候会阻塞其他操作(如add、delete、getNextRow)。 Region锁在做更新操作时，需要判断资源是否满足要求，如果到达临界点，则申请进行flush操作，等待直到资源满足要求（参见Region中的checkResource）Region update更新锁(updatesLock)，在internalFlushCache时加写锁，导致在做Put、delete、increment操作时候阻塞(这些操作加的是读锁)。Region close保护锁(lock)，在Region close或者split操作的时(加写锁)，阻塞对region的其他操作(加读锁)，比如compact、flush、scan和其他写操作。 StoreFile锁其中在flush过程的commit阶段，compact过程的completeCompaction阶段(rename临时compact文件名、清理旧的文件)，close store(关闭store)，bulkLoadHFile，会阻塞对store的写操作。 协处理器CoprocessorObserver(类似触发器)EndPoint(类似与存储过程)二级索引Coprocessor Observer实时同步构建写队列异步构建ETL(MapReduce/Spark/DataX等)批量离线构建引用深入HBase架构解析（一）深入HBase架构解析（二）An In-Depth Look at the HBase ArchitectureHbase原理、基本概念、基本架构Apache HBase Internals: Locking and Multiversion Concurrency Control总结一下HBase各种级别的锁以及对读写的阻塞Configuring HBase Memstore: What You Should KnowHBase – Memstore Flush深度解析]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS基础：概念与原理]]></title>
    <url>%2F2017%2F05%2F03%2FHDFS%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[简单介绍HDFS里面的一些基础知识和原理实现。 HDFS特点 支持： 处理超大文件(Very large files)：处理几百MB、TB甚至PB级别的数据。 流式地访问数据(Streaming data access)：HDFS设计模式基于“一次写入，多次读取”(write-once, read-many-times)。数据集由数据源生成或复制至HDFS，然后被用于分析。一般来说，这种数据分析应该基于整个数据集。所以对于HDFS来说，设计上应该考虑读取整个数据集的性能而不是读取其中一条数据的性能。 商用硬件(Commodity hardware)：可以运行于廉价的商用硬件集群上。 不支持： 低延迟数据访问(Low-latency data access)：HDFS不适用于低延迟的数据访问，比如数百ms的响应时间。HDFS强调的是吞吐量，如果需要实时的数据访问，基于HDFS的HBase才是合适的技术。 海量小文件(Lots of small files)：HDFS会将文件系统的元数据加载到NameNode的内存。NameNode的内存限制了文件存储的数量。每个文件的元数据大约占用150 bytes空间。PS：在HDFS 2.x，已经支持NameNode分片扩展(NameNode Federation)，类似于Redis Cluster。 多用户写入，文件任意修改(Multiple writers, arbitrary file modifications)：在HDFS文件只能被一个用户写入，并且只能是文件末尾追加。不支持多用户写入以及通过偏移量修改文件任意位置。 基本概念block块block是HDFS存储（读写）文件的最小单位（类似于windows文件系统NFS的簇，或Linux文件系统的块）。block的默认大小在Hadoop 1.x是64M，在Hadoop 2.x是128M。HDFS将文件分割存储在各个block中，而block则分布在各个DataNode中。HDFS默认是3个副本。HDFS中文件大小如果小于block，并不会占用整个block的空间。HDFS将集群当做一个整体，每个DataNode划分了很多的block（类比Windows将磁盘看做一个整体，在磁盘上划分了很多的簇），数据便存储在这些block里面。注意：NFS的簇是Windows操作系统的逻辑概念，HDFS的块是Hadoop应用的逻辑概念。二者所属层次不同，NFS是操作系统提供支持，HDFS是应用提供支持。不过二者硬件底层都是磁盘提供物理存储支持。 为什么HDFS的block单位如此大？ 文件被切分时，块的数量少，HDFS访问定位（相当于“磁盘寻道”）、聚合（从各个block读取数据）都会较快。 bolck的大小设置由磁盘的传输速率决定。假设磁盘寻道时间是10ms，传输速度是100 MB/s。让寻道时间为传输时间的1%，那么传输时间是1s。所以块大小应为：传输速率传输时间，即100MB/s1s = 100M NameNodeHDFS是典型的Master-Slave(Worker)模式，NameNode便是Master。NameNode管理这文件系统的命名空间，维护整个文件系统的文件目录树以及这些文件的索引目录（概括来说是维护HDFS的元数据）。NameNode的元数据包括： 文件存储路径 NameNode描述 副本数量 数据块数量 数据块存储的物理位置（主机名） CRC校验判断文件块是否损坏 HDFS元数据数据以2种形式持久化在本地磁盘中： 文件系统镜像(FileSystem Image)：整个文件系统的元数据快照。 编辑日志(Edit Log)：NameNode启动后，client对文件系统的改动日志。 NameNode扩展(NameNode Federation)NameNode受限与内存，那么【分片】就很有必要了。单NameNode架构： Namespace：有且只有一个Namespace/NameNode Block Storage Service： Block Management： 通过处理注册和心跳维护DataNodes状态 处理block reports和维护block位置 支持block的增删改查 管理副本放置、块复制以及删除多余副本 Storage：DataNodes提供把blocks存储在本地磁盘并给予读写权限的功能。 NameNode Federation架构： Namespace Volume：Namespace和它的Block Pool合称为Namespace Volume Namespace：Namespace由一个或多个NameNode组成 Block Pool：每个NameNode都有一个pool，每个pool都维护这一些列属于该NameNode的blocks。一个Namespace下的所有pool组成Block Pool。Block Pool维护了整个Namespace的blocks信息。 ClusterID：用来标识集群中的所有节点。在格式化NameNode的时候，需要指定ClusterID，否则会自动生成。ClusterID会被用于将其他NameNode格式化进集群。 ViewFs：用于管理HDFS的Namespaces（或者说Namespace Volumes）。 NameNode Federation是如何进行分片横向扩展的？是否类似于Redis Cluster的Hash分片？显然，由于划分了Namespace，通过Hash(filePath)的方式来分片，那么具有同样业务IO特性的文件有可能跨Namespace，性能降低。HDFS使用的方法是ViewFs,类似于Unix/Linux系统的client-side mount table。指定Namespace的Pathname Pattern（将Pathname Pattern挂载在某个Namespace），然后将具有相同Pathname Pattern的文件将会定位到指定的Namespace。下图表示将/data、/project、/user、/tmp 一共4个路径挂载到4个不同的命名空间。 DataNodeDataNode是具体读写任务的执行节点：存储文件块，被客户端和NameNode调用。同时通过心跳周期性地向NameNode报告文件块的信息。 SecondaryNameNode负责定期合并edit logs到系统文件镜像(namespace image or called FileSystem Image)，以减少edit log的空间。SecondaryNameNode进行checkpoint备份的数据必然是滞后于NameNode的，如果NameNode完全挂掉，包括磁盘（即丢掉了最新的edit logs）。那么还是会丢掉一部分数据。Hadoop 2.x通过JournalNodes实现了热备。PS：SecondaryNameNode并不能实现高可用，NameNode依然是单点的。NameNode故障重启加载元数据（冷启动）也要消耗大量时间。 HDFS副本策略（机架感知rack-aware） 第一个副本在本地机器 第二个副本在远端机架 第三个副本在本地机架的不同节点 其他副本随机放置 HDFS就近读取策略HDFS会就近读取文件的副本，而所谓的远近由拓扑距离定义。同节点，距离为0；同一机架不同节点，距离为2；相同数据中心不同机架，距离为4；不同数据中心，距离为6. HDFS的安全模式NameNode启动后会进入安全模式。处于安全模式的NameNode不会进行数据块的“写”操作，只能进行“读”操作。NameNode从所有的DataNode接收心跳信号和块状态报告，状态报告包括了某个DataNode所有的block的信息。每个block都有一个指定的最小副本数，当NameNode确认某个block的副本数量达到最小副本数时，则认为该block是副本安全的。NameNode确认一定百分比（可配置）的block是副本安全后，NameNode会退出安全模式。之后，NameNode对于副本数量不足的block会补充合适的副本。 HDFS High Availability JournalNodes一组用于NameNode Active和NameNode Standby通信的进程。通过JournalNodes实现HA，也就实现了HDFS的高可用。注意JournalNodes必须大于3，且最好是奇数，因为NameNode Active的editlog必须写入超过一半的JournalNodes才算写入成功（分布式一致性算法）。 高可用满足： edit logs共享，NameNode Standby可以读取NameNode Active全部的edit logs，之后与NameNode Active保持edit logs同步。 DataNodes必须向NameNode Standby和NameNode Active同时发送block reports，因为block的映射信息存储于NameNode的内存而不是磁盘。 客户端操作HDFS时，故障切换NameNode对于用户透明。 SecondaryNameNode的checkpoint工作由NameNode Standby取代，NameNode Standby会周期性地checkpoint NameNode Active的namespace。 Block Caching通常DataNode从磁盘读取block，但对于频繁访问的文件块，可以显式缓存在DataNode的堆外内存中。默认一个block只缓存在1个DataNode的内存中，不过这个参数可以基于文件进行配置。比如，需要将一个小表用于join操作，那么用户可以指定NameNode缓存指定的文件（并且指定缓存过期时间），NameNode就会生成一个cache加入到cache pool（落在物理上就是DataNode将block写入堆外内存）。cache pool用于管理缓存的权限和资源使用。 HDFS的checkpointcheckpoint指的是SecondaryNameNode合并edit log的过程。 1.SecondaryNameNode请求回滚NameNode正在写入的editlogs，并将new editlogs写到一个新文件中去（图中edits_inprogress_20）。NameNode更新它所有的seen_txid文件。 seentxid是存放transactionId的文件，format之后是0，它代表的是NameNode里面的edits*文件的尾数，NameNode重启的时候，会按照seen_txid的数字， 顺序从头跑edits_0000001~到seen_txid的数字 2.SecondaryNameNode通过HTTP GET请求获取最新的fsimage and edits files（图中fsimage_0和dits_1-19）。 3.SecondaryNameNode加载fsimage至内存，合并editlog。得到一个新的fsimage file（图中fsimage_19.ckpt）。 4.SecondaryNameNode通过HTTP PUT将新的fsimage传输到NameNode，保存为一个临时.ckpt文件。 5.NameNode重命名临时.ckpt文件使其生效。checkpoint的最终结果是：NameNode得到一个最新的fsimage和一个较小的新的editlog文件（checkpoint过程客户端对HDFS的写操作日志） HDFS读流程 1.Client调用FileSystem.open(filePath) 2.DistributedFileSystem用RPC去NameNode获取文件块的位置 3.open方法返回得到FSDataInputStream，用于流式读取 4.Client调用FSDataInputStream.read读取拓扑距离最近的DataNode 5.直至FSDataInputStream读取到文件最后的一个block 6.读取完毕，关闭输入流 HDFS写流程 1.Client调用FileSystem.create(filePath) 2.DistributedFileSystem通过RPC在NameNode创建一个新的空文件（即没有blocks的文件） 3.create方法返回得到FSDataOutputStream，用于流式写入。FSDataOutputStream会将Client写入的数据分割成packets，组成一个data queue。data queue被DataStreamer消费，并请求NameNode通过副本策略选择合适的节点来存放数据副本。这些节点组成一个管道pipeline（图中展示的是3个副本的写入操作）。 4.DataStreamer将packets写入第一个节点，第一个节点将数据复制到第二个节点，依次类推。 5.DFSOutputStream还维护了一个ack queue，ack queue里面存放的是未被ack的packets。1个packet被所有节点ack后，就会从ack queue里面移除。 6.写入完毕，关闭输出流。 TODO:关于HDFS读写流程就图只能简单解读，如果需要更加深入的了解，可以通过HDFS原生的Client结合HDFS源码来细细研读。还可以结合SpringHadoop研究一下优秀的代码封装(TextFileReader和TextFileWriter)。HDFS相关jar是：12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-hadoop-store&lt;/artifactId&gt; &lt;version&gt;2.4.0.RELEASE&lt;/version&gt;&lt;/dependency&gt; HDFS命令 User Commands Administration Commands Debug Commands]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark作业资源分配]]></title>
    <url>%2F2017%2F04%2F27%2FSpark%E4%BD%9C%E4%B8%9A%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%2F</url>
    <content type="text"><![CDATA[Spark作业计算时，如何调度资源？]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm拓扑资源分配]]></title>
    <url>%2F2017%2F04%2F27%2FStorm%E6%8B%93%E6%89%91%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%2F</url>
    <content type="text"><![CDATA[Storm运行拓扑时，如何调度资源？]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm之启动脚本解析]]></title>
    <url>%2F2017%2F04%2F27%2FStorm%E4%B9%8B%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[好奇了一下storm的命令启动过程，结果发现是通过bash调用python脚本。 storm1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#!/usr/bin/env bash## Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# "License"); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.## Resolve links - $0 may be a softlinkPRG="$&#123;0&#125;" #源程序名# 这个while循环最终回到一个结果 相对路径的PRG=./storm 或者绝对路径的 PRG=/opt/stormwhile [ -h "$&#123;PRG&#125;" ]; do # -h表示判断$&#123;PRG&#125;文件是否存在并且是一个symbolic link (also symlink or soft link)。可以理解为快捷方式。为什么使用while？因为防止嵌套的快捷方式 ls=`ls -ld "$&#123;PRG&#125;"` # 返回$&#123;PRG&#125;的详细参数。结果类似于：lrwxrwxrwx. 1 holmes holmes 12 Apr 26 11:06 storm -&gt; /opt/storm/bin/storm # ls -ld -l 参数 以详细格式列表 -d 参数 仅列指定文件。 link=`expr "$ls" : '.*-&gt; \(.*\)$'` #通过expr命令处理$ls变量。 # expr value : expression 通过正则提取symlink -&gt; 后面的hard link if expr "$link" : '/.*' &gt; /dev/null; then # 如果是绝对路径，表示是 / 开头 。 表达式的结果输出到/dev/null（垃圾回收站） PRG="$link" else PRG=`dirname "$&#123;PRG&#125;"`/"$link" # dirname：获取父路径，如果是/和.则直接返回。 如果是相对路径，在前面加上 ./ 或者绝对路径（取决于真正运行bash的时候使用的是./storm 或者是/opt/storm/...） fidone# check for versionPYTHON="/usr/bin/env python" #python环境变量# （1）$PYTHON -V 输出python版本：Python 2.6.6# （2）$PYTHON -V 2&gt;&amp;1 1指的是标准输出 2指的是错误输出 2&gt;&amp;1指的是将错误输出流重定向到标准输出流。（这里个人觉得可以不加上，如果命令错误了不如直接报错）# （3）echo 'Python 2.6.6' | awk '&#123;print $2&#125;' 对输出的结果Python 2.6.6以空格为分隔符，打印第二个参数。这里会打印2.6.6# （4）echo '2.6.6'| cut -d'.' -f1` 对输出结果2.6.6以.为分隔符。-d'.' 定义分隔符号为. -f1 表示取第一个分割到的字符串majversion=`$PYTHON -V 2&gt;&amp;1 | awk '&#123;print $2&#125;' | cut -d'.' -f1` #输出python大版本，比如2 minversion=`$PYTHON -V 2&gt;&amp;1 | awk '&#123;print $2&#125;' | cut -d'.' -f2` #输出python小版本，比如6numversion=$(( 10 * $majversion + $minversion)) # python 2.6版本会输出：2*10+6 = 26if (( $numversion &lt; 26 )); then echo "Need python version &gt; 2.6" exit 1fiSTORM_BIN_DIR=`dirname $&#123;PRG&#125;` #获取父路径 即/opt/storm/bin/export STORM_BASE_DIR=`cd $&#123;STORM_BIN_DIR&#125;/..;pwd` # 返回上一级，即/opt/storm/，pwd输出当前目录路径#check to see if the conf dir or file is given as an optional argumentif [ $# -gt 1 ]; then # 如果参数个数大于1。 $#表示此脚本传入的参数个数 if [ "--config" = "$1" ]; then # 如果第一个参数是--config则取第二个参数（--config接着的参数）作为conf_file conf_file=$2 if [ -d "$conf_file" ]; then # 如果conf_file目录存在，conf_file设置为$conf_file/storm.yaml conf_file=$conf_file/storm.yaml fi if [ ! -f "$conf_file" ]; then # 如果storm.yaml不存在，则抛出异常 echo "Error: Cannot find configuration file: $conf_file" exit 1 fi STORM_CONF_FILE=$conf_file #storm.yaml目录 即/opt/storm/conf/storm.yaml STORM_CONF_DIR=`dirname $conf_file` #获取配置文件目录 即/opt/storm/conf/ fifi# export 的变量被所有的子shell共享（父子shell的变量传递）# :- 例子：x=$&#123;1:-y&#125;表示 如果$&#123;1&#125;存在且不空则x=$&#123;1&#125;，否则x=yexport STORM_CONF_DIR="$&#123;STORM_CONF_DIR:-$STORM_BASE_DIR/conf&#125;"export STORM_CONF_FILE="$&#123;STORM_CONF_FILE:-$STORM_BASE_DIR/conf/storm.yaml&#125;"if [ -f "$&#123;STORM_CONF_DIR&#125;/storm-env.sh" ]; then # 如果storm-env.sh存在，则行改文件。 . "$&#123;STORM_CONF_DIR&#125;/storm-env.sh"fiexec "$&#123;STORM_BIN_DIR&#125;/storm.py" "$@" #调用storm.py的Python脚本 exec执行脚本 $@表示：storm脚本传入的所有参数。例子：storm a b c 转为storm.py "a" "b" "c" storm.py1# 待解析]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Storm</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python脚本：遍历文件夹]]></title>
    <url>%2F2017%2F04%2F27%2FPython%E8%84%9A%E6%9C%AC%EF%BC%9A%E9%81%8D%E5%8E%86%E6%96%87%E4%BB%B6%E5%A4%B9%2F</url>
    <content type="text"><![CDATA[如题，实现文件夹的先序遍历和层次遍历（多叉树）。 通过命令：python xx.py /xx/xx 运行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!/usr/bin/env python# -*- coding:utf-8 -*-import sysimport osimport time# 先序遍历def preorder_traversal(path, recursion): if not os.path.isdir(path) and not os.path.isfile(path): # 如果既不是文件夹也不是文件，直接返回 return if os.path.isfile(path): return upper_dir = path for fileOrDir in os.listdir(path): abs_path = os.path.join(upper_dir, fileOrDir) if os.path.isfile(abs_path): # 打印文件 print fileOrDir elif recursion and os.path.isdir(abs_path): print abs_path # 打印子文件夹 preorder_traversal(abs_path, recursion)# 层次遍历def level_traversal(path): if not os.path.isdir(path) and not os.path.isfile(path): # 如果既不是文件夹也不是文件，直接返回 return level = 1 for root, sub_dirs, files in os.walk(path): print ("============ The Level is %d ============" % level) level += 1 for file_path in files: # 打印文件 print file_path for sub in sub_dirs: # 打印子文件夹 print os.path.join(root, sub)pyName = sys.argv[0]path = sys.argv[1]print ("The running python script is : [%s] and the first argv is : [%s]" % (pyName, path))# path = 'D:\\Work\\trytry\\github'preorder_traversal_start_time = time.time()preorder_traversal(path, True)print ( "============ preorderTraversal cost Time : %s seconds============" % (time.time() - preorder_traversal_start_time))level_traversal_start_time = time.time()level_traversal(path)print ("============ levelTraversal cost Time : %s seconds============" % (time.time() - level_traversal_start_time))]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm基础：概念与原理]]></title>
    <url>%2F2017%2F04%2F27%2FStorm%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[简单介绍Storm里面的一些基础知识和原理实现。 Storm基本原理与概念Component组件，Spout和Bolt都是组件，从代码上看Spout和Bolt都是实现了IComponent接口的。 Topology 拓扑Storm实时流式计算的逻辑封装便是封装在拓扑中，在分布式环境中组成拓扑的成员便是Component，即Spout和Bolt。Storm和MapReduce很是相似，MR总会执行完毕，而Storm会一直在后台运行，监听数据源的输入。个人认为拓扑也应是一个有向无环图（DAG）。拓扑图： Stream 数据流数据流Stream是Storm数据流动的一个抽象概念。数据流是一个无界Tuple序列，Stream是分布式、并行地被处理和创建的。数据流在创建元组Tuple的时候，通过定义Schema来声明的。在declareOutputFields使用declare.declareStream方法定义Stream。定义Schema主要是确定streamId，如果不声明streamId，那么数据流默认的streamId是default。streamId的作用是：在定义拓扑的时候，分组策略可以通过指定streamId来限定Storm算子只处理来自指定数据流的数据。且每个Bolt只支持订阅1个Stream。 Component(Spout/Bolt)可以发射多个数据流stream，通过多次调用declareStream声明多个streamId。 Spout Spout通常负责从外部读取数据，并处理成Tuple分发到拓扑中。 Spout可以是可靠的，也可以是不可靠的： 可靠Spout：如果Tuple处理Tuple失败，Spout可以重新处理； 不可靠Spout：Spout一旦emit了元组，便不管该元组是否能被成功处理。 Spout中的主要方法是： nextTuple：nextTuple负责向拓扑发射元组，如果没有元组，则需要直接返回，有时为了避免消耗过度的CPU资源，没有元组时可能sleep较短时间，比如1ms。需要特别注意，nextTuple是绝对不能是一个阻塞方法，因为Storm会调用同一个线程的所有spout的实现，如果阻塞了某个Spout线程，会影响Spout的并行处理速度。 ack和fail：在可靠Spout中，当emit Tuple失败或者成功时，会调用他们来告诉storm处理结果。下游Component处理上游Component emit 过来的Tuple，调用OutputCollector.ack(input)或者OutputCollector.fail(input) nextTuple、ack和fail会在同一个线程被调用。PS：听说JStorm已经是异步调用ack和fail了。 Bolt几乎所有的Storm计算都是通过Bolt来完成的，包括：filtering过滤, functions函数, aggregations聚合, joins连接, talking to databases操作数据库等等。 通过使用提供的TopologyContext或通过跟踪OutputCollector中的emit方法的输出（返回发送元组的任务ID），消费者的任务ID。 Bolt中的主要方法是： execute：execute负责接收上游组件发射来的Tuple，然后将输入的Tuple转换成新的Tuple发射出去。调用方法：OutputCollector.emit(newTuple)。 OutputCollector是线程安全的，所以在 execute中可以使用多线程来异步处理Tuple来进行emit操作。BasicBoltExecutor implements IRichBolt使用装饰者模式（他们共同的接口是IComponent），对于实现了IBasicBolt的bolt会自动执行ack操作。 Stream grouping 流分组策略下游Bolt通过调用相关的Grouping方法来订阅上游Component的输出数据(Tuple)来作为自己的输入。 Shuffle grouping(随机分组)：元组随机分布在Bolt的Tasks中，每个Bolt都能得到一样数量的Tuples。 Fields grouping(域分组)：上游数据流由Fileds进行分区，下游Bolt通过订阅上游Component指定的Fileds获取数据，每个Bolt只能接收它订阅了的Fields。 Partial grouping(部分关键字分组)：和Fields grouping类似，在数据倾斜的情况下，能更好地实现负载均衡。原理见此处 All grouping(完全分组)：广播分组，每个Tuple都会emit到每一个Bolt中去 Global grouping(全局分组)：全部Tuple都会emit到一个Bolt的一个Task中去 None grouping(非分组)：不关心数据流是否被分组，目前相当于Shuffle grouping，不过Storm会把使用None grouping的这个bolt放到它订阅的Spout/Bolt所在的同一个线程里面去执行（如果可能的话）。 Direct grouping(直接分组)：上游Component指定下游Bolt的Task来接收Tuple。Direct grouping只能用于direct streams，通过调用emitDirect将Tuples发射到direct streams，并指定taskId。 一个上游Bolt（生产者）如何获得下游Bolt（消费者）的taskIds呢？通过TopologyContext或者追踪OutputCollector.emit方法，Tuple发射成功后，会返回Tuples发送到的taskIds。 Local or shuffle grouping(本地或随机分组)：如果目标Bolt在同一个worker进程（指和生产者在同一个进程）有一个或多个Tasks，Tuples会被优先随机分发到进程内的Tasks（Local grouping）。否则边是shuffle grouping。 CustomStreamGrouping：通过实现CustomStreamGrouping接口来自定义分组策略。 数据流分组图： Partial grouping(部分关键字分组) processing elemen(PE)：流处理应用程序组成DAG，DAG顶点就叫PE（算子） streams：DAG的边和运算符表示流 processing element instance (PEI)：为了可伸缩，streams会被分区为sub-streams，就会交由PEI进行并行处理 key splitting：power of two choices(PoTC)当key做Hash或者其他分区方法进行分区，每个key可以被2个PEI处理（在Storm里应该就是2个Bolt），这个过程就叫key splitting。在Storm里面，会根据2个Bolt的负载进行均衡。 Local Load Estimation：那如何进行负载的判断？好吧，看论文和源码去吧~ 可靠性Storm保证每个Spout Tuple会被完全处理。Storm通过追踪由每个Spout元组触发的元组树，并确定元组树何时完全完成。 元组树：第一个元组从Spout发出，之后在拓扑中流转分裂变形成更多的元组，这些元组构成了一棵树。第一个Spout Tuple便是树的根节点。 每个拓扑都有一个”message timeout（消息超时）”，如果Storm在超时时间内无法检测到Tuple是否已经被完全处理则会认为处理失败，并且稍后的时间释放该元组（这儿的稍后是啥时候？如果此段时间存在大量Timeout Message会不会造成OOM？会不会在内存满时触发清除超时元组？）。元组树的“边”在被创建（Anchoring）和元组被处理完成（ack/fail）的时候，通过OutputCollector来通知Storm。Anchoring通过OutputCollector.emit完成，ack/fail通过OutputCollector.ack/OutputCollector.fail或BasicOutputCollector.emit(底层调用了OutputCollector.ack(input))完成。详见此处：Guaranteeing Message Processing 如何去除可靠性 Spout发射Tuples的时候不带上messageId，因为Storm的ack机制是通过msgId进行追踪的。 设置Config.TOPOLOGY_ACKERS为0（即Config.setNumAckers(0)），这样子Spout会在发射一个Tuple后自己立刻调用ack方法。 使用Unanchor方式发射元组：比如使用OutputCollector.emit发射元组，而BasicOutPutCollector.emit则是anchor的方式。 BasicOutPutCollector.emit底层会自动构建Tuple，并通过调用OutputCollector.emit发射元组，所以是anchor的方式。而直接调用OutputCollector.emit，底层不会构建Tuple，所以是Unanchor方式。他们最底层的方法接口都是：IOutputCollector.emit(String streamId,Collection&lt;Tuple&gt; anchors,List&lt;Object&gt; tuple) Task每个Task对应的是一个线程，数据流分组决定了上游Tasks的元组如何流动到下游Tasks中。 Worker指的是Worker进程，在这个JVM进程中，执行了一组Spout/Bolt的Tasks，每个Worker只能对应一个拓扑。 Executor每个Worker进程中运行了一个或多个Executor线程。每个Executor可以执行一个或多个Task（不过一般设置为1Executor:1Task），每个Executor只对应一个Component(Spout/Bolt) PS：以上三者可以通过下图来描述： parallelismHint(并行度) Worker并行度：Config.TOPOLOGY_WORKERS，通过Config.setNumWorkers设定，表示Storm在集群中会启动多少个Worker进程来处理此拓扑。 Component并行度：在调用TopologyBuilder.setSpout和TopologyBuilder.setBolt的时候，可传入并行度参数，并行度参数确定了一个Component(Spout/Bolt)的初始 executor （线程）数量。 Task并行度： Config.TOPOLOGY_MEX_TASK_PARALLELISM，通过Config.setMaxTaskParaellelism设定，表示一个Component在拓扑中的最大并行任务（此参数常用来限制在本地模式中的最大线程数）。 通过TopologyBuilder.setSpout.setNumTasks和TopologyBuilder.setBolt.setNumTasks可以设置一个Component的Executor中运行的Task数量 这里列举的都是通过代码配置参数，配置优先级：defaults.yaml &lt; storm.yaml &lt; topology-specific configuration &lt; internal component-specific configuration &lt; external component-specific configuration NimbusStorm也是Master-Slave模式，和Spark，HDFS，HBase等Hadoop组件的设计是一样的。且整个Storm Cluster是无状态的，所有状态都维护在zookeeper中。Nimbus便是Storm在Master节点的守护进程，Storm Client将拓扑提交到Nimbus，Nimbus将拓扑相关代码分发到负责Slave节点（工作节点），Slave节点的Supervisor守护进程会生成Worker执行Spout/Bolt的Task。Nimbus还会监控Slave的状态。因为Supervisor的状态会记录在zookeeper，Nimbus通过zk得知Supervisor是否健康。如果Supervisor挂了，Nimbus会将它的Task分配给其他节点。 SupervisorSupervisor是Storm在Slave节点（工作节点）的守护进程，负责监听Slave节点上由Master分配的Task，并启动合适数量的Worker。Supervisor也会监控Worker进程的状态，如果Worker挂了，Supervisor会重新启动新的工作进程（Worker Process）。 Nimbus、Supervisor与zookeeper关系如图： Nimbus HA (Since Storm 1.x)Tuple 元组元组默认支持integers, longs, shorts, bytes, strings, doubles, floats, booleans, and byte arrays.用户可以通过实现自己的序列化器使得Tuple可以支持自定义对象类型。 Field通过OutputFieldsDeclarer.declare来定义Fields，new Values(col1,col2)与new Fields(field1,field2)顺序对应。 BasicBoltExecutor与BaseBasicBolt自动调用ack，一开始以为是模板方法。后来看了BasicBoltExecutor发现是装饰者，这儿有点意思。对于可靠的消息处理，若希望自动ack，理应继承BaseBasicBolt而不是BasicRichBolt。BasicBoltExecutor对于Tuple会自动ack，并且ack失败会自动调用fail方法重发。 Storm与MapReduce我写Spark的时候，MR类思想总感觉可以对应上Storm，遂搜寻有下表：来源 MapReduce Storm 系统角色 JobTracker Nimbus 系统角色 TaskTracker Supervisor 系统角色 Child Worker 应用名称 Job Topology 组件接口 Mapper/Reducer Spout/Bolt Storm消息保证等级对于消息在集群中传递，比如Kafka，JMS等消息机制，都会有类似的保证机制等级。 At Most Once: 最多一次，消息只发送一次，消息不重复、可丢失。即Tuple不进行Track。 At Least Once: 至少一次，消息至少发送一次，消息不丢失、可重复。即Tuple被Track，失败了会进行重发，直至成功，所以可能会造成重复计算。 Exactly Once: 恰好一次，消息只发送一次，消息不重复、不丢失。提供TridentAPI，实现事务Tuple。 Bolt的线程安全问题No, they do not need to be thread-safe. Each task has their own instance of the bolt or spout object they’re executing,and for any given task Storm calls all spout/bolt methods on a single thread.Storm主程nathanmarz回答既然如此，这里便有一个疑问：Storm的Bolt的最小任务执行单元是Task，并且是Task是多实例的？那么对于一些共享的资源，比如连接池，只能在在Task间共享？而不能在每个Executor甚至每个Worker共享？如果是这样子，岂不是作用域很小而导致资源浪费？有人如果看到这个，希望可以解惑。看到org.apache.storm.jdbc.bolt.AbstractJdbcBolt(JdbcInsertBolt)的源码，他的ConnectionProvider(HikariCPConnectionProvider)提供了连接池。也不过是Bolt上的一个实例域而已。]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark RDD API]]></title>
    <url>%2F2017%2F04%2F23%2FSpark_RDD_API%2F</url>
    <content type="text"><![CDATA[介绍Spark RDD API的含义与使用。 Spark官方网站的描述 写在前面Spark基于RDD的编程，个人理解是和SQL、存储过程编程是一样的，面向数据集编程。RDD就像数据库里面的一个个表，SQL的计算便是RDD的API。比如where对应filter什么的。所以在编写Spark程序的时候，倒不需要死记RDD的API，而是想象如果用SQL如何实现，然后再来查询相关的API，或者使用SparkSQL。写得多了，便是唯手熟尔。 如何练习API开发的时候，想看看某个API的结果是否复合预期，毕竟Scala语法糖甜死人，会常常有所疑问。此时就可以通过spark-shell调用API进行测试。比如测试leftOuterJoin，通过以下代码可以很直观地看到改API的实际操作结果：1234var rdd1 = sc.makeRDD(Array(("A","1"),("B","2"),("C","3")),2)var rdd2 = sc.makeRDD(Array(("A","a"),("A","c"),("D","d")),2)rdd1.leftOuterJoin(rdd2).collect# 输出：Array[(String, (String, Option[String]))] = Array((B,(2,None)), (A,(1,Some(a))),(A,(1,Some(c))), (C,(3,None))) Spark主要类 SparkContext：是Spark对外接口，负责向调用该类的scala应用提供Spark的各种功能，如连接Spark集群、创建RDD等。 SparkConf：Spark应用配置类，如配置应用名称，执行模式，executor内存等。 RDD（Resilient Distributed Dataset）：用于Spark应用程序中定义RDD的类，该类提供数据集的操作方法，如map，filter。 PairRDDFunctions：为key-value对的RDD数据提供运算操作，如groupByKey。 Broadcast：广播变量类，广播变量允许保留一个只读的变量，缓存在每一台机器上，而非每个任务保存一份拷贝。 StorageLevel：数据存储级别，有内存（MEMORY_ONLY），磁盘（DISK_ONLY），内存+磁盘（MEMORY_AND_DISK）等。RDD支持2中类型的操作：transformation和action。transformation实质是一个逻辑的action，记录了RDD的演变过程。transformation采用的是懒策略。只有action被提交时才会触发transformation的计算动作。 Each RDD has 2 sets of parallel operations: transformation and action.(1)Transformation:Return a MappedRDD[U] by applying function f to each element(2)Action:return T by reducing the elements using specified commutative and associative binary operator Operations which can cause a shuffle include repartition operations like repartition and coalesce, ‘ByKey operations (except for counting) like groupByKey and reduceByKey, and join operations like cogroup and join. Transformations 方法 说明 宽依赖or窄依赖 map(func) 对调用map的RDD数据集中的每个element都使用func方法，生成新的RDD。 窄依赖 filter(func) 对RDD中所有元素调用func方法，生成将满足条件数据集以RDD形式返回。 窄依赖 flatMap(func) 对RDD中所有元素调用func方法，然后将结果扁平化，生成新的RDD。理解为降维 窄依赖 mapPartitions(func) 类似于Map，不过Map作用的对象是每个元素，而mapPartitions作用的对象是分区。由于分区必然不跨节点，所以通过mapPartitions来实现一些资源在分区内共享，比如数据库连接等 窄依赖 mapPartitionsWithIndex(func) 类似于mapPartitions，提供多1个index参数，表示分区的索引 窄依赖 sample(withReplacement, fraction, seed) 抽样，返回RDD一个子集。withReplacement同一个元素是否可以重复抽样，fraction样本大小占总样本大小的百分比 窄依赖 union(otherDataset) 返回一个新的RDD，包含源RDD和给定RDD的元素的集合。求并集，且不去除重复集合。 窄依赖 intersection(otherDataset) 求交集，去除重复元素。 未知 distinct([numTasks])) 去除重复元素，生成新的RDD。 窄依赖 groupByKey([numTasks]) 返回(K,Iterable[V])，将key相同的value组成一个集合。 宽依赖 reduceByKey(func, [numTasks]) 对key相同的value调用func。按key聚合。 宽依赖 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 暂时略，通过例子说明。 宽依赖 combineByKey(createCombiner, mergeValue, mergeCombiners) 暂时略，通过例子说明。 宽依赖 foldByKey(zeroValue)(func) 暂时略，通过例子说明。 宽依赖 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 暂时略，通过例子说明。 宽依赖 sortByKey([ascending], [numTasks]) 按照key来进行排序，Key需实现Ordered接口。ascending升序还是降序，使用的是RangePartitioner。 宽依赖 join(otherDataset, [numTasks]) 当有两个KV的dataset(K,V)和(K,W)，返回的是(K,(V,W))的dataset,numPartitions为并发的任务数。 Hash分区窄依赖，Range分区宽依赖 cogroup(otherDataset, [numTasks]) 将当有两个key-value对的dataset(K,V)和(K,W)，返回的是(K, (Iterable[V], Iterable[W]))的dataset,numPartitions为并发的任务数。 宽依赖 cartesian(otherDataset) 返回该RDD与其它RDD的笛卡尔积。 宽依赖 pipe(command, [envVars]) 以管道的方式对分区执行脚本命令，处理当前进程的标准输出流，比如perl、bash。返回结果是RDD[String] 窄依赖 coalesce(numPartitions) 合并分区为numPartitions个分区，在RDD结果多次计算后数据减少可以通过合并分区提高效率 宽依赖 repartition(numPartitions) 重分区，随机Reshuffle RDD中的数据以增加/减少分区，并在其间平衡。分区数据的Shuffle通过网络完成。 宽依赖 repartitionAndSortWithinPartitions(partitioner) 根据给定的分区器partitioner重新分区RDD，并且在每个生成的分区中，通过它们的键对记录进行排序。这比调用repartition，然后在每个分区中排序更有效，因为it can push the sorting down into the shuffle machinery。 宽依赖 说一下Map与flatMap，flatMap是Map之后再flat，即是将((a,b),(c),(d,e))转化为(a,b,c,d,e) Actions 方法 说明 reduce(func) 对RDD中的元素调用f，f必须是1个可交换和可关联的函数，以便Spark可以进行并行计算。即聚合所有RDD的元素为1个元素 collect() 返回包含RDD中所有元素的一个数组 count() 返回dataset中element的个数 first() 返回dataset中的第一个元素 take(n) 返回前n个elements takeSample(withReplacement, num, [seed]) 对dataset随机抽样，返回有num个元素组成的数组。withReplacement同一个元素是否可以重复抽样，num表示抽样个数 takeOrdered(n, [ordering]) 使用自然顺序或自定义比较器返回RDD的前n个元素。 saveAsTextFile(path) 把dataset写到一个text file中，或者hdfs，或者hdfs支持的文件系统中，spark把没条记录都转换为一行记录，然后写到file中。 saveAsSequenceFile(path) 只能用在key-value对上，然后生成SequenceFile写到本地或者hadoop文件系统。 saveAsObjectFile(path) 生成ObjectFile写到本地或者hadoop文件系统。 countByKey() 对每个key出现的次数做统计，返回一个Map。 foreach(func) 在数据集的每一个元素上，运行函数func。 countByValue()(implicitord: Ordering[T] = null):Map[T, Long] 对RDD中每个元素出现的次数进行统计。]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark基础：概念与原理]]></title>
    <url>%2F2017%2F04%2F23%2FSpark%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[简单介绍Spark里面的一些基础知识和原理实现。 总体架构Spark集群总体架构图如下： Cluster Manager(集群管理器)： Standalone – Spark自己的集群管理器 Apache Mesos – 基于Mesos Hadoop YARN – 基于Yarn这些集群管理器可以在应用间分配资源。SparkContext与Cluster Manager一旦连接，Spark需要在集群上的线程池子节点，也就是那些执行计算和存储应用数据的工作进程。然后，它将把你的应用代码（以JAR或者Python定义的文件并传送到SparkContext）发送到线程池。最后，SparkContext发送任务让线程池运行。 Application指编写的Spark程序。 DriverSpark作业的主进程，负责作业的解析，并且负责向yarn申请资源，并且调度作业的executor。提交Spark Application到集群的时候，对于Spark来说，就是生成一个Driver进程来执行Spark App的main函数，并且初始化SparkContext。DAGScheduler和TaskScheduler都是属于Driver的。 Executor：执行器，里面包含了N个core，每个core包含了1个Task。 DAGScheduler：负责生成Stage（DAG）和TaskSet，每个Stage就有一组TaskSet。 TaskScheduler：接收DAGScheduler的TaskSet并发送给Executor。 疑问：为什么是有向无环图？（1）有向图表示RDD可追溯，失败可以重运行。（2）无环图表示RDD不可变，如果存在环，是否意味着RDD可变？ Job可以认为在Spark App中，每一个Spark的Action API就是一个Job。而Spark App可以有多个job，而Job可以划分成多个Stage，Stage是一组Task（即TaskSet）：1234567Spark App ———— Job |———— Stage | |————Task | |————Task |———— Stage |————Task |————Task StageStage由Task组成，每个Stage内的Task是并行执行的，而Stage之间是串行的。Stage的划分：每个Shuffle Dependency（即Wide Dependency）之前的所有RDD操作。 TaskTask有2类：ShuffleMapTask和ResultTask，类似与MapReduce的Map和Reduce。Task的划分：Stage的最后1个RDD的分区数 = Task数量。Task是并行的，合理设置分区可以提高资源利用率。 这些概念在Spark官网介绍如下 Term Meaning Application User program built on Spark. Consists of a driver program and executors on the cluster. Application jar A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies. The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime. Driver program The process running the main() function of the application and creating the SparkContext Cluster manager An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN) Deploy mode Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster. Worker node Any node that can run application code in the cluster Executor A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. Task A unit of work that will be sent to one executor Job A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs. Stage Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs. RDDRDD是Spark并行计算的基础。RDD是一个只读的并行、可伸缩的分布式数据结构。RDD由Partition组成。多个Partition可能被分配在不同的节点，但是1个Partition只能在一个节点（即对于一个分区来说，分区是不跨节点的） 分区是一个很关键的概念，具体可参考JDK的HashMap(哈希表的每个hash位置可以理解为1个分区)以及ConcurrentHashMap（每个桶理解为1个分区）源码，类似的还有kafka的topic和Redis Cluster的slot概念。分区/分片，是弹性扩展数据结构的一个手段。 在Spark的RDD设计上，所有的窄依赖在分区上进行管道式的计算，以达到并行计算的目的。如图： 宽依赖与窄依赖父RDD与子RDD的依赖关系：（1）宽依赖：每个父RDD的任意Partition会被子RDD的多个Partition所依赖。发生了Shuffle的算子会产生Shuffle依赖（2）窄依赖：每个父RDD的任意Partition只被子RDD的一个Partition所依赖。 Spark为什么要区分宽依赖与窄依赖呢？（1）窄依赖支持在同一个节点中，以pipeline（管道）的形式执行多条命令。窄依赖计算是并行的、分区独立的（分区作为管道）。而宽依赖在计算过程中会发生Shuffle，是跨分区的计算。（2）从失败恢复的角度（容错机制）看：窄依赖只需要重新计算丢失分区的父分区，而且不同节点之间可以并行计算。宽依赖则需要重新计算子分区所依赖的所有父分区，并且产生冗余计算。如下图：假设Partition1’丢失，则需要重新计算Partition1和Partition2，从而冗余计算了Partition2’。 宽依赖发生Shuffle的性能影响 （1）Shuffle由于产生磁盘IO/网络IO/数据的序列化与反序列化会严重影响RDD算子的性能，Spark的Map Tasks生产Shuffle的数据，Reduce Tasks读取Shuffle数据来进行聚合操作。注：Map Tasks和Reduce Tasks的命名法来自MapReduce，并不直接与Spark的Map和Reduce操作有关。 （2）在Spark计算内部处理过程中，部分Map Tasks的计算结果会被缓存在内存里，直到内存不足才会被清理。然后，这部分数据会根据目标分区进行排序并写入单个文件。而Reduce Task会去读取相关的排序块（sorted blocks）。 （3）某些Shuffle操作会消耗大量的堆内存，因为它们在tranffer数据之前或之后会通过基于内存的数据结构来处理记录。具体来说，reduceByKey和aggregateByKey在Map端上创建这些基于内存的数据结构，“ByKey”操作会在reduce端生成in-memory data structures。在内存不足的时候，Spark会将这些数据表溢写到磁盘，从而导致磁盘IO的额外开销和增加的垃圾回收。 （4）Shuffle操作还会在磁盘上生成大量的中间文件。从Spark 1.3开始，这些文件将被保留，直到相应的RDD不再使用并被垃圾回收。这样的作用是如果需要重新计算Shuffled RDD，则不需要重新执行Shuffle过程。如果Spark App保留对这些RDD的引用或不频繁触发GC，那么垃圾收集可能会在很长一段时间之后发生。这意味着Spark作业的产生的中间文件会长时间占用大量的磁盘。在配置Spark上下文时，由spark.local.dir配置指定临时存储目录。更多的Shuffle配置 RDD的持久化 Storage Level Meaning MEMORY_ONLY 默认模式，将RDD反序列化成Java对象存储在JVM内存，如果内存不足，某些分区不会被缓存，而是每次使用都重新计算。 MEMORY_AND_DISK 区别于MEMORY_ONLY，内存不足会写入磁盘，每次使用都从磁盘读取。 MEMORY_ONLY_SER 类似于MEMORY_ONLY，但存储对象的格式不一样，是将RDD序列化为字节数组（每个分区一个字节数组）。这通常比反序列化对象更具空间效率，特别是在使用快速序列化器如KryoSerializer的情况下，但却转化成CPU密集型作业（时间换空间）。 MEMORY_AND_DISK_SER 类似于MEMORY_ONLY_SER，内存不足会写入磁盘，每次使用都从磁盘读取。 DISK_ONLY RDD的partitions只被写入磁盘 MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. 和上面的设置类似，不过会复制每个分区到2个集群节点上。 OFF_HEAP (experimental) 类似于MEMORY_ONLY_SER，但将数据存储在堆外内存中。这需要启用堆外内存。 RDD持久化选择 如果内存足够，使用默认存储级别（MEMORY_ONLY），速度最快并充分利用了CPU（会消耗大量内存）。 如果内存不足，使用MEMORY_ONLY_SER，通过消耗CPU将RDD序列化以节省空间（减少内存消耗）。 不要溢出到磁盘，除非并行计算资源成本过高，或者需要过滤大量的数据。否则，重新计算分区可能与从磁盘读取分区一样快。 如果要快速故障恢复，请使用MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.（例如，使用Spark来提供来自Web应用程序的请求）。所有RDD持久化策略通过重新计算丢失的数据来提供完整的容错能力，但复制的数据可让您继续在RDD上运行任务，而无需重新计算丢失的分区。 缓存清除Spark会自动监视每个节点的缓存使用情况，并以近期最少使用算法（LRU,Least Recently Used）方式丢弃旧的数据分区。 调用RDD.unpersist()可以手动清除缓存释放空间。 共享变量通常，当提交到Spark操作（例如map或reduce）的函数在远程集群节点上执行时，在函数中使用的所有变量会生成副本到每个节点上，并且远程节点上变量的更新不会传播回Driver。而在提供Task间共享的read-write shared variables是低效的。 不过Spark依然提供了两种有限类型的共享变量：广播变量broadcast variables和累加器accumulators。 广播变量广播变量允许程序员在每个节点上缓存只读变量，而不是为每个Task生成变量副本。使用情景举例：通过广播变量为每个节点提供很大的输入数据集（高效）。 Spark还尝试使用高效的广播算法分发广播变量，以降低通信成本。Spark的Actions可以分解为一系列的Stages（Stage通过Shuffle操作划分）Spark自动广播每个Stage的Task所需的common data。采用此方式广播的数据序列化之后缓存,然后在Task运行之前执行反序列化。这意味着，显式创建广播变量仅在跨多个Stage的Task需要相同数据或者需要以反序列化格式缓存数据时才有用。广播变量通过调用SparkContext.broadcast(v)创建广播变量v。广播变量是围绕v的包装器，其值可以通过调用value方法来访问。代码如下：123val broadcastVar = sc.broadcast(Array(1, 2, 3))broadcastVar.value//broadcastVar.value结果是：Array[Int] = Array(1, 2, 3) 累加器Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel.它们可以用于实现计数器（类似MapReduce一样）或者求和，Spark原生支持数字类型的Accumulators，当然程序员也可以实现自定义的Accumulators。用户可以创建实名/匿名的accumulators，实名accumulators在web UI，修改该accumulators的stage处可以看到accumulators变量的变化。具体如图：Tracking accumulators in the UI can be useful for understanding the progress of running stages. 程序员可以通过调用SparkContext.longAccumulator()或SparkContext.doubleAccumulator()来分别创建Long或Double类型的数字累加器。然后集群运行这的Task可以调用add方法添加accumulators。 不过Task不能获取accumulators的值。 只有Driver可以使用value方法读取accumulators的值。12345val accum = sc.longAccumulator("My Accumulator")// accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))accum.value//res2: Long = 10 而实现自定义的accumulators，需要继承AccumulatorV2。代码如下：123456789101112131415161718class VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVector] &#123; private val myVector: MyVector = MyVector.createZeroVector def reset(): Unit = &#123; myVector.reset() &#125; def add(v: MyVector): Unit = &#123; myVector.add(v) &#125; ...&#125;// Then, create an Accumulator of this type:val myVectorAcc = new VectorAccumulatorV2// Then, register it into spark context:sc.register(myVectorAcc, "MyVectorAcc1") 注意：For accumulator updates performed inside actions only,即accumulator的更新需要Actions来触发计算。123val accum = sc.longAccumulatordata.map &#123; x =&gt; accum.add(x); x &#125;// Here, accum is still 0 because no actions have caused the map operation to be computed. 提交任务到集群# Run application locally on 8 cores ./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master local[8] \ /path/to/examples.jar \ 100 # Run on a Spark standalone cluster in client deploy mode ./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://207.184.161.138:7077 \ --executor-memory 20G \ --total-executor-cores 100 \ /path/to/examples.jar \ 1000 # Run on a Spark standalone cluster in cluster deploy mode with supervise ./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ /path/to/examples.jar \ 1000 # Run on a YARN cluster export HADOOP_CONF_DIR=XXX ./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode cluster \ # can be client for client mode --executor-memory 20G \ --num-executors 50 \ /path/to/examples.jar \ 1000 # Run a Python application on a Spark standalone cluster ./bin/spark-submit \ --master spark://207.184.161.138:7077 \ examples/src/main/python/pi.py \ 1000 # Run on a Mesos cluster in cluster deploy mode with supervise ./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master mesos://207.184.161.138:7077 \ --deploy-mode cluster \ --supervise \ --executor-memory 20G \ --total-executor-cores 100 \ http://path/to/examples.jar \ 1000 通过Java/Scala启动Spark作业The org.apache.spark.launcher package provides classes for launching Spark jobs as child processes using a simple Java API. 单元测试Spark is friendly to unit testing with any popular unit test framework. Simply create a SparkContext in your test with the master URL set to local, run your operations, and then call SparkContext.stop() to tear it down. Make sure you stop the context within a finally block or the test framework’s tearDown method, as Spark does not support two contexts running concurrently in the same program. Spark官方示例]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase ORM的实现]]></title>
    <url>%2F2016%2F12%2F30%2FHbase-ORM%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Hbase ORM的设计与实现，参考了一个开源项目的设计思路。 有空会补完文章具体见Github:hd-client]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式并发控制（连接池）]]></title>
    <url>%2F2016%2F12%2F30%2F%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%EF%BC%88%E8%BF%9E%E6%8E%A5%E6%B1%A0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[基于连接池思想的分布式并发控制设计实现 有空会补完文章]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>并发控制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式令牌桶设计实现（流控）]]></title>
    <url>%2F2016%2F12%2F28%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%A4%E7%89%8C%E6%A1%B6%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0%EF%BC%88%E6%B5%81%E6%8E%A7%EF%BC%89%2F</url>
    <content type="text"><![CDATA[分布式环境下的流量控制，通过redis实现令牌桶算法。记录在设计实现过程中的思维变迁。 流程图（分布式锁实现） get last refill time:这里需要加入分布式锁，每次只能有1个请求获得lastRefillTime。令牌桶配置为k-v形式，lastRefillTimeLock的key和Token Bucket的key类似（比如通过前缀区分）。释放锁的时间为refill token bucket或者nowTime-lastRefillTime&gt;refillInterval为no，更新lastRefillTime之后释放锁。 现在假设令牌桶的设置key是：tokenBucketKey，对应的分布式锁key是：lockKey。 简单说下此处分布式锁的实现： set(lockKey, value, &quot;NX&quot;, &quot;PX&quot;, time)value为任意值，而time是毫秒数。 每次获取lastRefillTime之前都需要通过该命令判断返回值是否为OK，OK表示获得锁。并且设置过期时间，过期表示锁自动释放。PS:此处还有一种基于redis的订阅/发布（sub/pub）模式的分布式锁。 线程安全 线程安全:每个JVM（进程）内的所有线程，对于获得锁应getLock到释放锁releaseLock之间的代码应加关键字synchronized，锁住这块代码避免了一个节点内的所有并发都去竞争分布式锁。（可避免分布式锁被过度竞争，将锁竞争的压力分散到各个节点中去，甚至在竞争分布式锁的时候增加竞争间隔来减轻压力）这有点像加个滤网，先内部竞争，优秀的人才可以去外部竞争。（后来我才知道这叫层次锁Hierarchical Lock，学至深处，万物皆然） 第一次改进（去除分布式锁）在获取时间锁，到填充令牌的过程使用lua实现，这里利用了redis执行lua的原子性。lua代码如下：12345678910local nowTime = redis.call('time')[1]local lastRefilledTime = redis.call('hget',KEYS[1],'TOKEN_BUCKET_LAST_REFILLED_TIME_OPP#10016#CMB00002')local refilledInterval = redis.call('hget',KEYS[1],'TOKEN_BUCKET_REFILLED_INTERVAL_OPP#10016#CMB00002')if(tonumber(nowTime) - tonumber(lastRefilledTime) &gt; tonumber(refilledInterval))then local refilledNum = redis.call('hget',KEYS[1],'TOKEN_BUCKET_REFILLED_NUM_OPP#10016#CMB00002') redis.call('hset',KEYS[1],'TOKEN_BUCKET_NUM_OPP#10016#CMB00002',refilledNum) local newRefilledTime = redis.call('time')[1] redis.call('hset',KEYS[1],'TOKEN_BUCKET_LAST_REFILLED_TIME_OPP#10016#CMB00002' ,newRefilledTime)end Redis执行lua注意要点：如上代码会报错：1redis.clients.jedis.exceptions.JedisDataException: ERR Error running script (call to f_f8e67ef88ffbbf7d8837a1fecb1b97f62133fea2): @user_script:7: @user_script: 7: Write commands not allowed after non deterministic commands 解析见stackoverflowRedis tries to protect itself against such cases by blocking any write command (such as DEL) if it is executed after a random command (e.g. SCAN but also TIME, SRANDMEMBER and similar). 像TIME这种带有随机性的命令，在lua中，得到的结果不能作用于写命令。 第二次改进（将时间由外部传入）外部传入的时间同时代表nowTime和newRefilledTime，那么newRefilledTime会比实际的newRefilledTime少。Redis正常的情况下，误差目测为2ms以内。 第三次改进（利用redis的expire）以上设计都不完美，再次修改lua脚本：12345678910111213141516171819202122232425private synchronized boolean refillToken(JedisCluster jedisCluster, String tokenKey, String refilledInterval, String refilledNum) &#123; StringBuffer luaScript = new StringBuffer(); luaScript.append("local nowTokenNum = redis.call('get', KEYS[1])\n") .append("if not nowTokenNum then\n") .append(" redis.call('set',KEYS[1],ARGV[2],'EX',ARGV[1])\n") .append(" nowTokenNum=ARGV[2]\n") .append("end\n") .append("if(tonumber(nowTokenNum) &lt; 0) then\n") .append(" return 0\n") .append("else\n") .append(" nowTokenNum = redis.call('decr', KEYS[1])\n") .append(" if(tonumber(nowTokenNum)&lt;0) then\n") .append(" return 0\n") .append(" else\n") .append(" return 1\n") .append(" end\n") .append("end\n"); List&lt;String&gt; keys = new ArrayList&lt;&gt;(); keys.add(tokenKey); List&lt;String&gt; args = new ArrayList&lt;&gt;(); args.add(refilledInterval);//1 args.add(refilledNum);//2 Object eval = jedisCluster.eval(luaScript.toString(), keys, args); return eval.toString().equals("1");//eval为null抛异常 &#125; 很明显，如此设计去掉了时间参数的限制，老实说，我是受github上一个单机令牌桶算法的影响，跳不出【上次更新令牌时间】这个桎梏，忽略了redis的expire的特性。大约流程为： Redis集群的注意要点对于一个令牌桶，涉及到的配置有： 令牌桶令牌数 令牌补充间隔 上次补充令牌时间[利用了expire可以省掉此配置]。 如果3个配置都需要一个key，那么在redis集群对于一次补充令牌操作，便不能进行批量操作（即利用lua脚本进行令牌补充）了。此处可设计为一个Hashmap，1个key然后3个field，便可以在集群里面将补充令牌封装成lua。有个问题是：对于令牌数放Map里面执行decr可以使用HINCRBY -1或者封装成lua操作。redis执行lua是原子操作，可以避免使用分布式锁。注意：redis调用lua应该是简单的redis操作，避免阻塞其他的redis操作过久。（因为redis执行lua的原子性） 猜测一下Redis Cluster执行涉及多个key的原子操作的设计原理 首先理解slot，slot是redis通过crc16(key) mod nodeNum来确定key应该在哪个节点，在redis中slot不存在物理概念，只是用来确定key所属的节点【分片】。 为什么跨节点不能执行原子事务？猜测：redis集群是多个节点都部署了redis，节点间redis是多进程，虽然是redis是单线程，但是每个进程至少有一个线程。如果没有分布式锁，或者类似的机制，已经不能保证跨节点的原子事务。 为什么必须同一个slot的key才允许原子操作？猜测：同一个节点的确可以保证原子性，但是如果2个key落在同一个节点的不同的slot，如果以后扩充或者减少redis集群的节点，都会导致节点拥有的slot的改变，从而导致原来同一个节点的key分片到不同节点。这样一来，坑就大了。集群节点数量改变会导致原来的代码挂掉。 写在最后强烈建议使用Redis单实例来做，依赖可以避免Redis的事务问题；二来Redis单实例效率比集群高不少（毕竟少了一次Hash定位）。]]></content>
      <categories>
        <category>分布式</category>
        <category>流量控制</category>
      </categories>
      <tags>
        <tag>令牌桶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker基础：概念与原理]]></title>
    <url>%2F2016%2F10%2F11%2FDocker%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Docker基础：概念与原理，该blog记录于2016-10。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello,World!]]></title>
    <url>%2F1970%2F01%2F01%2FHello_World%2F</url>
    <content type="text"><![CDATA[Hello,I love this World!]]></content>
      <categories>
        <category>Hello World</category>
      </categories>
      <tags>
        <tag>Hello World</tag>
      </tags>
  </entry>
</search>